{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ECG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HclOK4e4Rmij"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "#pio.renderers.default = \"colab\"\n",
    "import neurokit2 as nk\n",
    "#pio.renderers.default = \"browser\"  # Abre o gráfico no navegador\n",
    "# ou\n",
    "#pio.renderers.default = \"notebook\"  # Usa o modo compatível com notebooks\n",
    "# Configurar o tamanho da figura globalmente\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 6)  # Ajuste a largura e altura aqui\n",
    "\n",
    "# Definir o número máximo de linhas a serem exibidas\n",
    "pd.set_option('display.max_rows', None)  # Exibe todas as linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import glob\n",
    "\n",
    "# # Define o padrão para os arquivos CSV\n",
    "\n",
    "# path = r'D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ECG\\High-resolution_ECG\\P0*'\n",
    "# path_holter = r'D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ECG\\Holter_ECG\\P0*'\n",
    "\n",
    "# #path = r'E:/Repositorio_Git/zzz-projeto_final/dados/ca-*.csv'\n",
    "\n",
    "\n",
    "# # Usando glob para pegar todos os arquivos que seguem o padrão\n",
    "# arquivos = glob.glob(path)\n",
    "# arquivos_path_holter = glob.glob(path_holter)\n",
    "# arquivos = [os.path.splitext(arquivo)[0] for arquivo in arquivos] # tirando as a extensões\n",
    "\n",
    "# arquivos_path_holter = [os.path.splitext(arquivos_path_holter)[0] for arquivos_path_holter in arquivos_path_holter] # tirando as a extensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define o padrão para os arquivos CSV\n",
    "\n",
    "#path = r'D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ECG\\High-resolution_ECG\\P0*'\n",
    "path = r'D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ECG\\Holter_ECG\\P0*'\n",
    "\n",
    "#path = r'E:/Repositorio_Git/zzz-projeto_final/dados/ca-*.csv'\n",
    "\n",
    "\n",
    "# Usando glob para pegar todos os arquivos que seguem o padrão\n",
    "arquivos = glob.glob(path)\n",
    "#arquivos_path_holter = glob.glob(path_holter)\n",
    "arquivos = [os.path.splitext(arquivo)[0] for arquivo in arquivos] # tirando as a extensões\n",
    "\n",
    "#arquivos_path_holter = [os.path.splitext(arquivos_path_holter)[0] for arquivos_path_holter in arquivos_path_holter] # tirando as a extensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #arquivos_path_holter\n",
    "\n",
    "# record = wfdb.rdrecord(arquivos_path_holter[0])\n",
    "\n",
    "\n",
    "# # Exibir as informações do arquivo\n",
    "# print(record.__dict__)\n",
    "#arquivos_path_holter\n",
    "\n",
    "lista_sem_duplicatas = pd.Series(arquivos).unique().tolist() # removando duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Projeto_Tese_mestrado\\\\02_Dataset\\\\dados_ECG\\\\Holter_ECG\\\\P0005'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_sem_duplicatas[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = wfdb.rdrecord(lista_sem_duplicatas[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ],\n",
       "       ...,\n",
       "       [-0.47, -0.47, -0.44],\n",
       "       [-0.41, -0.36, -0.35],\n",
       "       [-0.34, -0.29, -0.28]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_signal = record.p_signal\n",
    "p_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinal_x = p_signal[:,0]\n",
    "sinal_y = p_signal[:,1]\n",
    "sinal_z = p_signal[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16460600"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sinal_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ecg_cleaned = nk.ecg_clean(sinal_x, sampling_rate=record.fs, method=\"neurokit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ecg_cleaned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quality = nk.ecg_quality(ecg_cleaned, sampling_rate=record.fs)\n",
    "#nk.signal_plot([ecg_cleaned, quality], standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exg_dataframe = pd.DataFrame(ecg_cleaned, columns=['ECG_Clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exg_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#px.line(exg_dataframe[1000:2000], x=exg_dataframe.index[0:1000], y = 'ECG_Clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amostra = p_signal[0:300000,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# para dados de ecg holder demora muito porque é muito grande\n",
    "#ecg_x, _ = nk.ecg_process(amostra, sampling_rate= record.fs , method='neurokit') # Dividindo a tupla em DataFrame e Dicionário\n",
    "#ecg_y, dict_info_y = nk.ecg_process(sinal_y, sampling_rate= record.fs , method='neurokit')\n",
    "#ecg_z, dict_info_z = nk.ecg_process(sinal_z, sampling_rate= record.fs , method='neurokit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if (ecg_x['ECG_Clean'] < 0).any():\n",
    "#    print(\"A coluna ECG_Clean contém valores negativos.\")\n",
    "#else:\n",
    "#    print(\"A coluna ECG_Clean não contém valores negativos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 'ECG_R_Peaks'\n",
    "- Descrição: Refere-se aos picos R do ECG, que são a parte mais alta da onda QRS.\n",
    "Importância: O pico R é um dos eventos mais importantes no ECG, pois indica a despolarização dos ventrículos. É amplamente usado para calcular a frequência cardíaca e identificar batimentos.\n",
    "2. 'ECG_P_Peaks'\n",
    "- Descrição: Refere-se aos picos P, que estão associados à despolarização atrial.\n",
    "Importância: O pico P representa a contração dos átrios do coração. Ele marca o início de cada batimento cardíaco.\n",
    "3. 'ECG_P_Onsets'\n",
    "- Descrição: Marca o início da onda P no ECG, ou seja, o começo da despolarização atrial.\n",
    "Importância: Ajuda a identificar a transição do atividade atrial para o início da contração ventricular.\n",
    "4. 'ECG_P_Offsets'\n",
    "- Descrição: Marca o final da onda P no ECG, ou seja, o término da despolarização atrial.\n",
    "Importância: Indica quando a atividade atrial termina, permitindo a análise do ciclo atrial e ventricular.\n",
    "5. 'ECG_Q_Peaks'\n",
    "- Descrição: Refere-se aos picos Q, que são uma parte da onda QRS.\n",
    "Importância: O pico Q representa a despolarização do septo interventricular. Ele pode ser pequeno ou até ausente em algumas pessoas.\n",
    "6. 'ECG_R_Onsets'\n",
    "- Descrição: Marca o início do pico R, ou seja, o início da despolarização ventricular.\n",
    "Importância: Esse valor ajuda a identificar o momento exato do começo da contração ventricular.\n",
    "7. 'ECG_R_Offsets'\n",
    "- Descrição: Marca o fim do pico R, ou seja, quando a despolarização ventricular está terminando.\n",
    "Importância: Indica o ponto de transição entre a despolarização e a repolarização ventricular.\n",
    "8. 'ECG_S_Peaks'\n",
    "- Descrição: Refere-se aos picos S, que estão associados ao final da onda QRS.\n",
    "Importância: O pico S representa a parte final da despolarização ventricular, quando a células cardíacas estão quase totalmente despolarizadas.\n",
    "9. 'ECG_T_Peaks'\n",
    "- Descrição: Refere-se ao pico T, que está associado à repolarização dos ventrículos.\n",
    "Importância: O pico T indica a recuperação das células ventriculares após a contração, preparando o coração para o próximo batimento.\n",
    "10. 'ECG_T_Onsets'\n",
    "- Descrição: Marca o início da onda T, ou seja, o início da repolarização ventricular.\n",
    "Importância: Esse ponto é importante para calcular a duração da repolarização ventricular, que pode ser usado para avaliar a saúde cardíaca.\n",
    "11. 'ECG_T_Offsets'\n",
    "- Descrição: Marca o final da onda T, indicando quando a repolarização ventricular está concluída.\n",
    "Importância: Esse ponto é útil para analisar a duração da onda T, o que pode ser importante para detectar arritmias.\n",
    "12. 'ECG_Phase_Atrial'\n",
    "- Descrição: Refere-se a uma fase atrial do ECG, provavelmente a fase de despolarização ou repolarização dos átrios.\n",
    "Importância: Pode indicar um marcador da atividade elétrica atrial, o que é útil para análise de arritmias atriais.\n",
    "13. 'ECG_Phase_Completion_Atrial'\n",
    "- Descrição: Marca o término da fase atrial, provavelmente indicando quando a atividade elétrica atrial termina.\n",
    "Importância: Esse evento pode ser útil para identificar a transição entre a atividade atrial e ventricular.\n",
    "14. 'ECG_Phase_Ventricular'\n",
    "- Descrição: Refere-se à fase ventricular do ECG, provavelmente relacionada à despolarização ou repolarização dos ventrículos.\n",
    "Importância: Essencial para a análise da atividade elétrica ventricular e para detectar possíveis problemas cardíacos.\n",
    "15. 'ECG_Phase_Completion_Ventricular'\n",
    "- Descrição: Marca o término da fase ventricular, ou seja, o fim da atividade elétrica ventricular.\n",
    "Importância: Indica a transição entre a repolarização ventricular e o início de um novo ciclo de contração atrial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irei testar Redes neurais com valores binarios do Neurokit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_csv = r'D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_csv_info_definitions\\ubject-info_limpo.csv'\n",
    "\n",
    "dados = pd.read_csv(link_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Follow-up period from enrollment (days)</th>\n",
       "      <th>days_4years</th>\n",
       "      <th>Exit of the study</th>\n",
       "      <th>Cause of death</th>\n",
       "      <th>SCD_4years SinusRhythm</th>\n",
       "      <th>HF_4years SinusRhythm</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender (male=1)</th>\n",
       "      <th>Weight (kg)</th>\n",
       "      <th>...</th>\n",
       "      <th>Angiotensin-II receptor blocker (yes=1)</th>\n",
       "      <th>Anticoagulants/antitrombotics  (yes=1)</th>\n",
       "      <th>Betablockers (yes=1)</th>\n",
       "      <th>Digoxin (yes=1)</th>\n",
       "      <th>Loop diuretics (yes=1)</th>\n",
       "      <th>Spironolactone (yes=1)</th>\n",
       "      <th>Statins (yes=1)</th>\n",
       "      <th>Hidralazina (yes=1)</th>\n",
       "      <th>ACE inhibitor (yes=1)</th>\n",
       "      <th>Nitrovasodilator (yes=1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0001</td>\n",
       "      <td>2065</td>\n",
       "      <td>1460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0002</td>\n",
       "      <td>2045</td>\n",
       "      <td>1460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0003</td>\n",
       "      <td>2044</td>\n",
       "      <td>1460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P0004</td>\n",
       "      <td>2044</td>\n",
       "      <td>1460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P0005</td>\n",
       "      <td>2043</td>\n",
       "      <td>1460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Patient ID  Follow-up period from enrollment (days)  days_4years  \\\n",
       "0      P0001                                     2065         1460   \n",
       "1      P0002                                     2045         1460   \n",
       "2      P0003                                     2044         1460   \n",
       "3      P0004                                     2044         1460   \n",
       "4      P0005                                     2043         1460   \n",
       "\n",
       "   Exit of the study  Cause of death  SCD_4years SinusRhythm  \\\n",
       "0                0.0               0                       0   \n",
       "1                0.0               0                       0   \n",
       "2                0.0               0                       0   \n",
       "3                0.0               0                       0   \n",
       "4                0.0               0                       0   \n",
       "\n",
       "   HF_4years SinusRhythm  Age  Gender (male=1)  Weight (kg)  ...  \\\n",
       "0                      0   58                1           83  ...   \n",
       "1                      0   58                1           74  ...   \n",
       "2                      0   69                1           83  ...   \n",
       "3                      0   56                0           84  ...   \n",
       "4                      0   70                1           97  ...   \n",
       "\n",
       "   Angiotensin-II receptor blocker (yes=1)  \\\n",
       "0                                        0   \n",
       "1                                        1   \n",
       "2                                        1   \n",
       "3                                        1   \n",
       "4                                        0   \n",
       "\n",
       "   Anticoagulants/antitrombotics  (yes=1)  Betablockers (yes=1)  \\\n",
       "0                                       1                     1   \n",
       "1                                       1                     1   \n",
       "2                                       1                     1   \n",
       "3                                       1                     1   \n",
       "4                                       1                     1   \n",
       "\n",
       "   Digoxin (yes=1)  Loop diuretics (yes=1)  Spironolactone (yes=1)  \\\n",
       "0                1                       1                       0   \n",
       "1                0                       0                       0   \n",
       "2                1                       1                       0   \n",
       "3                0                       1                       1   \n",
       "4                0                       1                       0   \n",
       "\n",
       "   Statins (yes=1)  Hidralazina (yes=1)  ACE inhibitor (yes=1)  \\\n",
       "0                0                    0                      1   \n",
       "1                1                    0                      0   \n",
       "2                0                    0                      0   \n",
       "3                0                    0                      0   \n",
       "4                1                    0                      1   \n",
       "\n",
       "   Nitrovasodilator (yes=1)  \n",
       "0                         0  \n",
       "1                         0  \n",
       "2                         0  \n",
       "3                         0  \n",
       "4                         1  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe 0: 726 -> 73.19%\n",
      "Classe 6: 100 -> 10.08%\n",
      "Classe 3: 94 -> 9.48%\n",
      "Classe 1: 61 -> 6.15%\n",
      "Classe 7: 11 -> 1.11%\n"
     ]
    }
   ],
   "source": [
    "# Contagem de classes no conjunto de dados\n",
    "class_counts = dados['Cause of death'].value_counts()\n",
    "\n",
    "\n",
    "# Contagem de classes no conjunto de dados\n",
    "for class_label, count in class_counts.items():\n",
    "    print(f'Classe {class_label}: {count} -> {count/dados.shape[0]*100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de colunas: 105\n",
      "Quantidade de linhas: 992\n"
     ]
    }
   ],
   "source": [
    "print(f'Quantidade de colunas: {dados.shape[1]}')\n",
    "print(f'Quantidade de linhas: {dados.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 3., 1., 2.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(dados['Exit of the study'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arquivos_path_holter\n",
    "\n",
    "lista_sem_duplicatas = pd.Series(arquivos).unique().tolist() # removando duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista_sem_duplicatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sem o filtro de tipo de mortes (911, 3)\n",
      "Com o filtro de tipo de mortes (854, 3)\n",
      "Classe 0: 662 -> 77.52%\n",
      "Classe 6: 95 -> 11.12%\n",
      "Classe 3: 86 -> 10.07%\n",
      "Classe 7: 11 -> 1.29%\n"
     ]
    }
   ],
   "source": [
    "# Criando um DataFrame dos endereços\n",
    "df_enderecos = pd.DataFrame({\n",
    "    \"Endereço\": lista_sem_duplicatas\n",
    "})\n",
    "\n",
    "# Extraindo o Patient ID do final do endereço\n",
    "df_enderecos[\"Patient ID\"] = df_enderecos[\"Endereço\"].apply(lambda x: x.split(\"\\\\\")[-1])\n",
    "\n",
    "\n",
    "# Removendo o sufixo '_H' da coluna 'Patient ID'\n",
    "df_enderecos['Patient ID'] = df_enderecos['Patient ID'].str.replace('_H', '', regex=False)\n",
    "\n",
    "# Aqui contem dados de duas colunas a primeira coluna vou usar como chave id para conectar os pacientes de cada scg e a segunda o tipo de morte.\n",
    "novos_dados = dados[['Patient ID','Cause of death']]\n",
    "\n",
    "# Fazendo a fusão (merge) dos DataFrames com base no \"Patient ID\"\n",
    "df_final = df_enderecos.merge(novos_dados, on=\"Patient ID\", how=\"left\")\n",
    "print(f'Sem o filtro de tipo de mortes {df_final.shape}')\n",
    "\n",
    "df_final = df_final[~df_final['Cause of death'].isin([1])] # tirando valores que tem 1 pois significa mortes não identificada ou seja ruídos,\n",
    "\n",
    "print(f'Com o filtro de tipo de mortes {df_final.shape}')\n",
    "\n",
    "# Contagem de classes no conjunto de dados\n",
    "class_counts = df_final['Cause of death'].value_counts()\n",
    "\n",
    "\n",
    "# Contagem de classes no conjunto de dados\n",
    "for class_label, count in class_counts.items():\n",
    "    print(f'Classe {class_label}: {count} -> {count/df_final.shape[0]*100:.2f}%')\n",
    "    \n",
    "    \n",
    "# Resetando o índice para garantir que o loop use os índices corretos\n",
    "df_final.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lista para armazenar informações de cada paciente\n",
    "# dados_pacientes = []\n",
    "\n",
    "# # Loop para processar cada arquivo e pegar o tamanho do sinal\n",
    "# for idx, arquivo in enumerate(arquivos):\n",
    "#     try:\n",
    "#         # Lendo o arquivo do ECG\n",
    "#         record = wfdb.rdrecord(arquivo)\n",
    "#         p_signal = record.p_signal\n",
    "#         sinal_x = p_signal[:, 0]\n",
    "        \n",
    "#         # Armazenando as informações no dicionário\n",
    "#         dados_pacientes.append({\n",
    "#             \"Paciente\": arquivo.split(\"\\\\\")[-1],  # Extrair o nome do paciente\n",
    "#             \"Quantidade de amostras\": len(sinal_x)\n",
    "#         })\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro ao processar {arquivo}: {e}\")\n",
    "\n",
    "# # Convertendo a lista para um DataFrame\n",
    "# df_pacientes = pd.DataFrame(dados_pacientes)\n",
    "\n",
    "# # Exibindo as primeiras linhas do DataFrame\n",
    "# print(df_pacientes)\n",
    "\n",
    "# df_pacientes['Quantidade de amostras'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrv_time = nk.hrv_time(peaks, sampling_rate=100, show=True)\n",
    "# hrv_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculos para diferentes durações de sinal ECG\n",
    "\n",
    "### 1. Para 5 minutos de sinal:\n",
    "1. **Número de segundos em 5 minutos**:  \n",
    "   5 minutos = 5 * 60 = 300 segundos.\n",
    "\n",
    "2. **Número de amostras em 5 minutos**:  \n",
    "   Como há 1.000 amostras por segundo, em 300 segundos teremos:\n",
    "   \\[\n",
    "   \\text{Amostras} = 1.000 \\times 300 = 300.000 \\, \\text{amostras}.\n",
    "   \\]\n",
    "\n",
    "Portanto, **5 minutos de sinal** corresponderiam a **300.000 amostras**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Para 10 minutos de sinal:\n",
    "1. **Número de segundos em 10 minutos**:  \n",
    "   10 minutos = 10 * 60 = 600 segundos.\n",
    "\n",
    "2. **Número de amostras em 10 minutos**:  \n",
    "   Como há 1.000 amostras por segundo, em 600 segundos teremos:\n",
    "   \\[\n",
    "   \\text{Amostras} = 1.000 \\times 600 = 600.000 \\, \\text{amostras}.\n",
    "   \\]\n",
    "\n",
    "Portanto, **10 minutos de sinal** corresponderiam a **600.000 amostras**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Para 15 minutos de sinal:\n",
    "1. **Número de segundos em 15 minutos**:  \n",
    "   15 minutos = 15 * 60 = 900 segundos.\n",
    "\n",
    "2. **Número de amostras em 15 minutos**:  \n",
    "   Como há 1.000 amostras por segundo, em 900 segundos teremos:\n",
    "   \\[\n",
    "   \\text{Amostras} = 1.000 \\times 900 = 900.000 \\, \\text{amostras}.\n",
    "   \\]\n",
    "\n",
    "Portanto, **15 minutos de sinal** corresponderiam a **900.000 amostras**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Para 20 minutos de sinal:\n",
    "1. **Número de segundos em 20 minutos**:  \n",
    "   20 minutos = 20 * 60 = 1.200 segundos.\n",
    "\n",
    "2. **Número de amostras em 20 minutos**:  \n",
    "   Como há 1.000 amostras por segundo, em 1.200 segundos teremos:\n",
    "   \\[\n",
    "   \\text{Amostras} = 1.000 \\times 1.200 = 1.200.000 \\, \\text{amostras}.\n",
    "   \\]\n",
    "\n",
    "Portanto, **20 minutos de sinal** corresponderiam a **1.200.000 amostras**.\n",
    "\n",
    "---\n",
    "Vou desconsiderar por enquanto primeiro vou usar o ECG com resolução otima, ela tem 20 minutos no total\n",
    "### 5. Para 25 minutos de sinal:\n",
    "1. **Número de segundos em 25 minutos**:  \n",
    "   25 minutos = 25 * 60 = 1.500 segundos.\n",
    "\n",
    "2. **Número de amostras em 25 minutos**:  \n",
    "   Como há 1.000 amostras por segundo, em 1.500 segundos teremos:\n",
    "   \\[\n",
    "   \\text{Amostras} = 1.000 \\times 1.500 = 1.500.000 \\, \\text{amostras}.\n",
    "   \\]\n",
    "\n",
    "Portanto, **25 minutos de sinal** corresponderiam a **1.500.000 amostras**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agora vou testar usando o X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13457b009a9d4aed99c9b9fc51890a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processando paciente 1/817: P0001\n",
      "✅ Paciente P0001 processado com sucesso!\n",
      "✅ Processando paciente 2/817: P0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_fixpeaks.py:307: RuntimeWarning: divide by zero encountered in divide\n",
      "  mrrs /= th2\n",
      "c:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_fixpeaks.py:307: RuntimeWarning: invalid value encountered in divide\n",
      "  mrrs /= th2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Paciente P0002 processado com sucesso!\n",
      "✅ Processando paciente 3/817: P0003\n",
      "✅ Paciente P0003 processado com sucesso!\n",
      "✅ Processando paciente 4/817: P0004\n",
      "✅ Paciente P0004 processado com sucesso!\n",
      "✅ Processando paciente 5/817: P0005\n",
      "✅ Paciente P0005 processado com sucesso!\n",
      "✅ Processando paciente 6/817: P0006\n",
      "✅ Paciente P0006 processado com sucesso!\n",
      "✅ Processando paciente 7/817: P0007\n",
      "✅ Paciente P0007 processado com sucesso!\n",
      "✅ Processando paciente 8/817: P0008\n",
      "✅ Paciente P0008 processado com sucesso!\n",
      "✅ Processando paciente 9/817: P0009\n",
      "✅ Paciente P0009 processado com sucesso!\n",
      "✅ Processando paciente 10/817: P0010\n",
      "✅ Paciente P0010 processado com sucesso!\n",
      "✅ Processando paciente 11/817: P0011\n",
      "✅ Paciente P0011 processado com sucesso!\n",
      "✅ Processando paciente 12/817: P0012\n",
      "✅ Paciente P0012 processado com sucesso!\n",
      "✅ Processando paciente 13/817: P0013\n",
      "✅ Paciente P0013 processado com sucesso!\n",
      "✅ Processando paciente 14/817: P0014\n",
      "✅ Paciente P0014 processado com sucesso!\n",
      "✅ Processando paciente 15/817: P0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_fixpeaks.py:307: RuntimeWarning: divide by zero encountered in divide\n",
      "  mrrs /= th2\n",
      "c:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_fixpeaks.py:307: RuntimeWarning: invalid value encountered in divide\n",
      "  mrrs /= th2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Paciente P0015 processado com sucesso!\n",
      "✅ Processando paciente 16/817: P0016\n",
      "✅ Paciente P0016 processado com sucesso!\n",
      "✅ Processando paciente 17/817: P0017\n",
      "✅ Paciente P0017 processado com sucesso!\n",
      "✅ Processando paciente 18/817: P0018\n",
      "✅ Paciente P0018 processado com sucesso!\n",
      "✅ Processando paciente 19/817: P0019\n",
      "✅ Paciente P0019 processado com sucesso!\n",
      "✅ Processando paciente 20/817: P0020\n",
      "✅ Paciente P0020 processado com sucesso!\n",
      "✅ Processando paciente 21/817: P0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_fixpeaks.py:307: RuntimeWarning: divide by zero encountered in divide\n",
      "  mrrs /= th2\n",
      "c:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_fixpeaks.py:307: RuntimeWarning: invalid value encountered in divide\n",
      "  mrrs /= th2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Paciente P0021 processado com sucesso!\n",
      "✅ Processando paciente 22/817: P0022\n",
      "✅ Paciente P0022 processado com sucesso!\n",
      "✅ Processando paciente 23/817: P0023\n",
      "✅ Paciente P0023 processado com sucesso!\n",
      "✅ Processando paciente 24/817: P0024\n",
      "✅ Paciente P0024 processado com sucesso!\n",
      "✅ Processando paciente 25/817: P0027\n",
      "✅ Paciente P0027 processado com sucesso!\n",
      "✅ Processando paciente 26/817: P0028\n",
      "✅ Paciente P0028 processado com sucesso!\n",
      "✅ Processando paciente 27/817: P0029\n",
      "✅ Paciente P0029 processado com sucesso!\n",
      "✅ Processando paciente 28/817: P0030\n",
      "✅ Paciente P0030 processado com sucesso!\n",
      "✅ Processando paciente 29/817: P0031\n",
      "✅ Paciente P0031 processado com sucesso!\n",
      "✅ Processando paciente 30/817: P0032\n",
      "✅ Paciente P0032 processado com sucesso!\n",
      "✅ Processando paciente 31/817: P0033\n",
      "✅ Paciente P0033 processado com sucesso!\n",
      "✅ Processando paciente 32/817: P0034\n",
      "✅ Paciente P0034 processado com sucesso!\n",
      "✅ Processando paciente 33/817: P0035\n",
      "✅ Paciente P0035 processado com sucesso!\n",
      "✅ Processando paciente 34/817: P0036\n",
      "✅ Paciente P0036 processado com sucesso!\n",
      "✅ Processando paciente 35/817: P0037\n",
      "✅ Paciente P0037 processado com sucesso!\n",
      "✅ Processando paciente 36/817: P0038\n",
      "✅ Paciente P0038 processado com sucesso!\n",
      "✅ Processando paciente 37/817: P0039\n",
      "✅ Paciente P0039 processado com sucesso!\n",
      "✅ Processando paciente 38/817: P0040\n",
      "✅ Paciente P0040 processado com sucesso!\n",
      "✅ Processando paciente 39/817: P0041\n",
      "✅ Paciente P0041 processado com sucesso!\n",
      "✅ Processando paciente 40/817: P0042\n",
      "✅ Paciente P0042 processado com sucesso!\n",
      "✅ Processando paciente 41/817: P0043\n",
      "✅ Paciente P0043 processado com sucesso!\n",
      "✅ Processando paciente 42/817: P0044\n",
      "✅ Paciente P0044 processado com sucesso!\n",
      "✅ Processando paciente 43/817: P0045\n",
      "✅ Paciente P0045 processado com sucesso!\n",
      "✅ Processando paciente 44/817: P0046\n",
      "✅ Paciente P0046 processado com sucesso!\n",
      "✅ Processando paciente 45/817: P0048\n",
      "✅ Paciente P0048 processado com sucesso!\n",
      "✅ Processando paciente 46/817: P0049\n",
      "✅ Paciente P0049 processado com sucesso!\n",
      "✅ Processando paciente 47/817: P0050\n",
      "✅ Paciente P0050 processado com sucesso!\n",
      "✅ Processando paciente 48/817: P0051\n",
      "✅ Paciente P0051 processado com sucesso!\n",
      "✅ Processando paciente 49/817: P0052\n",
      "✅ Paciente P0052 processado com sucesso!\n",
      "✅ Processando paciente 50/817: P0053\n",
      "✅ Paciente P0053 processado com sucesso!\n",
      "✅ Processando paciente 51/817: P0054\n",
      "✅ Paciente P0054 processado com sucesso!\n",
      "✅ Processando paciente 52/817: P0055\n",
      "✅ Paciente P0055 processado com sucesso!\n",
      "✅ Processando paciente 53/817: P0056\n",
      "✅ Paciente P0056 processado com sucesso!\n",
      "✅ Processando paciente 54/817: P0057\n",
      "✅ Paciente P0057 processado com sucesso!\n",
      "✅ Processando paciente 55/817: P0059\n",
      "✅ Paciente P0059 processado com sucesso!\n",
      "✅ Processando paciente 56/817: P0060\n",
      "✅ Paciente P0060 processado com sucesso!\n",
      "✅ Processando paciente 57/817: P0061\n",
      "✅ Paciente P0061 processado com sucesso!\n",
      "✅ Processando paciente 58/817: P0062\n",
      "✅ Paciente P0062 processado com sucesso!\n",
      "✅ Processando paciente 59/817: P0063\n",
      "✅ Paciente P0063 processado com sucesso!\n",
      "✅ Processando paciente 60/817: P0064\n",
      "✅ Paciente P0064 processado com sucesso!\n",
      "✅ Processando paciente 61/817: P0065\n",
      "✅ Paciente P0065 processado com sucesso!\n",
      "✅ Processando paciente 62/817: P0066\n",
      "✅ Paciente P0066 processado com sucesso!\n",
      "✅ Processando paciente 63/817: P0067\n",
      "✅ Paciente P0067 processado com sucesso!\n",
      "✅ Processando paciente 64/817: P0068\n",
      "✅ Paciente P0068 processado com sucesso!\n",
      "✅ Processando paciente 65/817: P0071\n",
      "✅ Paciente P0071 processado com sucesso!\n",
      "✅ Processando paciente 66/817: P0072\n",
      "✅ Paciente P0072 processado com sucesso!\n",
      "✅ Processando paciente 67/817: P0073\n",
      "✅ Paciente P0073 processado com sucesso!\n",
      "✅ Processando paciente 68/817: P0074\n",
      "✅ Paciente P0074 processado com sucesso!\n",
      "✅ Processando paciente 69/817: P0075\n",
      "✅ Paciente P0075 processado com sucesso!\n",
      "✅ Processando paciente 70/817: P0076\n",
      "✅ Paciente P0076 processado com sucesso!\n",
      "✅ Processando paciente 71/817: P0077\n",
      "✅ Paciente P0077 processado com sucesso!\n",
      "✅ Processando paciente 72/817: P0078\n",
      "✅ Paciente P0078 processado com sucesso!\n",
      "✅ Processando paciente 73/817: P0079\n",
      "✅ Paciente P0079 processado com sucesso!\n",
      "✅ Processando paciente 74/817: P0080\n",
      "✅ Paciente P0080 processado com sucesso!\n",
      "✅ Processando paciente 75/817: P0082\n",
      "✅ Paciente P0082 processado com sucesso!\n",
      "✅ Processando paciente 76/817: P0083\n",
      "✅ Paciente P0083 processado com sucesso!\n",
      "✅ Processando paciente 77/817: P0084\n",
      "✅ Paciente P0084 processado com sucesso!\n",
      "✅ Processando paciente 78/817: P0086\n",
      "✅ Paciente P0086 processado com sucesso!\n",
      "✅ Processando paciente 79/817: P0088\n",
      "✅ Paciente P0088 processado com sucesso!\n",
      "✅ Processando paciente 80/817: P0089\n",
      "✅ Paciente P0089 processado com sucesso!\n",
      "✅ Processando paciente 81/817: P0090\n",
      "✅ Paciente P0090 processado com sucesso!\n",
      "✅ Processando paciente 82/817: P0091\n",
      "✅ Paciente P0091 processado com sucesso!\n",
      "✅ Processando paciente 83/817: P0092\n",
      "✅ Paciente P0092 processado com sucesso!\n",
      "✅ Processando paciente 84/817: P0093\n",
      "✅ Paciente P0093 processado com sucesso!\n",
      "✅ Processando paciente 85/817: P0094\n",
      "✅ Paciente P0094 processado com sucesso!\n",
      "✅ Processando paciente 86/817: P0095\n",
      "✅ Paciente P0095 processado com sucesso!\n",
      "✅ Processando paciente 87/817: P0096\n",
      "✅ Paciente P0096 processado com sucesso!\n",
      "✅ Processando paciente 88/817: P0097\n",
      "✅ Paciente P0097 processado com sucesso!\n",
      "✅ Processando paciente 89/817: P0098\n",
      "✅ Paciente P0098 processado com sucesso!\n",
      "✅ Processando paciente 90/817: P0099\n",
      "✅ Paciente P0099 processado com sucesso!\n",
      "✅ Processando paciente 91/817: P0100\n",
      "✅ Paciente P0100 processado com sucesso!\n",
      "✅ Processando paciente 92/817: P0101\n",
      "✅ Paciente P0101 processado com sucesso!\n",
      "✅ Processando paciente 93/817: P0102\n",
      "✅ Paciente P0102 processado com sucesso!\n",
      "✅ Processando paciente 94/817: P0103\n",
      "✅ Paciente P0103 processado com sucesso!\n",
      "✅ Processando paciente 95/817: P0104\n",
      "✅ Paciente P0104 processado com sucesso!\n",
      "✅ Processando paciente 96/817: P0105\n",
      "✅ Paciente P0105 processado com sucesso!\n",
      "✅ Processando paciente 97/817: P0106\n",
      "✅ Paciente P0106 processado com sucesso!\n",
      "✅ Processando paciente 98/817: P0107\n",
      "✅ Paciente P0107 processado com sucesso!\n",
      "✅ Processando paciente 99/817: P0108\n",
      "✅ Paciente P0108 processado com sucesso!\n",
      "✅ Processando paciente 100/817: P0110\n",
      "✅ Paciente P0110 processado com sucesso!\n",
      "✅ Processando paciente 101/817: P0111\n",
      "✅ Paciente P0111 processado com sucesso!\n",
      "✅ Processando paciente 102/817: P0112\n",
      "✅ Paciente P0112 processado com sucesso!\n",
      "✅ Processando paciente 103/817: P0113\n",
      "✅ Paciente P0113 processado com sucesso!\n",
      "✅ Processando paciente 104/817: P0115\n",
      "✅ Paciente P0115 processado com sucesso!\n",
      "✅ Processando paciente 105/817: P0116\n",
      "✅ Paciente P0116 processado com sucesso!\n",
      "✅ Processando paciente 106/817: P0118\n",
      "✅ Paciente P0118 processado com sucesso!\n",
      "✅ Processando paciente 107/817: P0119\n",
      "✅ Paciente P0119 processado com sucesso!\n",
      "✅ Processando paciente 108/817: P0120\n",
      "✅ Paciente P0120 processado com sucesso!\n",
      "✅ Processando paciente 109/817: P0121\n",
      "✅ Paciente P0121 processado com sucesso!\n",
      "✅ Processando paciente 110/817: P0122\n",
      "✅ Paciente P0122 processado com sucesso!\n",
      "✅ Processando paciente 111/817: P0124\n",
      "✅ Paciente P0124 processado com sucesso!\n",
      "✅ Processando paciente 112/817: P0126\n",
      "✅ Paciente P0126 processado com sucesso!\n",
      "✅ Processando paciente 113/817: P0127\n",
      "✅ Paciente P0127 processado com sucesso!\n",
      "✅ Processando paciente 114/817: P0128\n",
      "✅ Paciente P0128 processado com sucesso!\n",
      "✅ Processando paciente 115/817: P0129\n",
      "✅ Paciente P0129 processado com sucesso!\n",
      "✅ Processando paciente 116/817: P0130\n",
      "✅ Paciente P0130 processado com sucesso!\n",
      "✅ Processando paciente 117/817: P0131\n",
      "✅ Paciente P0131 processado com sucesso!\n",
      "✅ Processando paciente 118/817: P0132\n",
      "✅ Paciente P0132 processado com sucesso!\n",
      "✅ Processando paciente 119/817: P0133\n",
      "✅ Paciente P0133 processado com sucesso!\n",
      "✅ Processando paciente 120/817: P0134\n",
      "✅ Paciente P0134 processado com sucesso!\n",
      "✅ Processando paciente 121/817: P0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_fixpeaks.py:307: RuntimeWarning: divide by zero encountered in divide\n",
      "  mrrs /= th2\n",
      "c:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_fixpeaks.py:307: RuntimeWarning: invalid value encountered in divide\n",
      "  mrrs /= th2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Paciente P0135 processado com sucesso!\n",
      "✅ Processando paciente 122/817: P0136\n",
      "✅ Paciente P0136 processado com sucesso!\n",
      "✅ Processando paciente 123/817: P0137\n",
      "✅ Paciente P0137 processado com sucesso!\n",
      "✅ Processando paciente 124/817: P0138\n",
      "✅ Paciente P0138 processado com sucesso!\n",
      "✅ Processando paciente 125/817: P0139\n",
      "✅ Paciente P0139 processado com sucesso!\n",
      "✅ Processando paciente 126/817: P0141\n",
      "✅ Paciente P0141 processado com sucesso!\n",
      "✅ Processando paciente 127/817: P0142\n",
      "✅ Paciente P0142 processado com sucesso!\n",
      "✅ Processando paciente 128/817: P0143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_fixpeaks.py:307: RuntimeWarning: divide by zero encountered in divide\n",
      "  mrrs /= th2\n",
      "c:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_fixpeaks.py:307: RuntimeWarning: invalid value encountered in divide\n",
      "  mrrs /= th2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Paciente P0143 processado com sucesso!\n",
      "✅ Processando paciente 129/817: P0145\n",
      "✅ Paciente P0145 processado com sucesso!\n",
      "✅ Processando paciente 130/817: P0146\n",
      "✅ Paciente P0146 processado com sucesso!\n",
      "✅ Processando paciente 131/817: P0147\n",
      "✅ Paciente P0147 processado com sucesso!\n",
      "✅ Processando paciente 132/817: P0148\n",
      "✅ Paciente P0148 processado com sucesso!\n",
      "✅ Processando paciente 133/817: P0150\n",
      "✅ Paciente P0150 processado com sucesso!\n",
      "✅ Processando paciente 134/817: P0151\n",
      "✅ Paciente P0151 processado com sucesso!\n",
      "✅ Processando paciente 135/817: P0152\n",
      "✅ Paciente P0152 processado com sucesso!\n",
      "✅ Processando paciente 136/817: P0153\n",
      "✅ Paciente P0153 processado com sucesso!\n",
      "✅ Processando paciente 137/817: P0154\n",
      "✅ Paciente P0154 processado com sucesso!\n",
      "✅ Processando paciente 138/817: P0155\n",
      "✅ Paciente P0155 processado com sucesso!\n",
      "✅ Processando paciente 139/817: P0156\n",
      "✅ Paciente P0156 processado com sucesso!\n",
      "✅ Processando paciente 140/817: P0157\n",
      "✅ Paciente P0157 processado com sucesso!\n",
      "✅ Processando paciente 141/817: P0158\n",
      "✅ Paciente P0158 processado com sucesso!\n",
      "✅ Processando paciente 142/817: P0159\n",
      "✅ Paciente P0159 processado com sucesso!\n",
      "✅ Processando paciente 143/817: P0160\n",
      "✅ Paciente P0160 processado com sucesso!\n",
      "✅ Processando paciente 144/817: P0162\n",
      "✅ Paciente P0162 processado com sucesso!\n",
      "✅ Processando paciente 145/817: P0163\n",
      "✅ Paciente P0163 processado com sucesso!\n",
      "✅ Processando paciente 146/817: P0164\n",
      "✅ Paciente P0164 processado com sucesso!\n",
      "✅ Processando paciente 147/817: P0165\n",
      "✅ Paciente P0165 processado com sucesso!\n",
      "✅ Processando paciente 148/817: P0166\n",
      "✅ Paciente P0166 processado com sucesso!\n",
      "✅ Processando paciente 149/817: P0167\n",
      "✅ Paciente P0167 processado com sucesso!\n",
      "✅ Processando paciente 150/817: P0168\n",
      "✅ Paciente P0168 processado com sucesso!\n",
      "✅ Processando paciente 151/817: P0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_fixpeaks.py:307: RuntimeWarning: divide by zero encountered in divide\n",
      "  mrrs /= th2\n",
      "c:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_fixpeaks.py:307: RuntimeWarning: invalid value encountered in divide\n",
      "  mrrs /= th2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Paciente P0169 processado com sucesso!\n",
      "✅ Processando paciente 152/817: P0170\n",
      "✅ Paciente P0170 processado com sucesso!\n",
      "✅ Processando paciente 153/817: P0171\n",
      "✅ Paciente P0171 processado com sucesso!\n",
      "✅ Processando paciente 154/817: P0172\n",
      "✅ Paciente P0172 processado com sucesso!\n",
      "✅ Processando paciente 155/817: P0173\n",
      "✅ Paciente P0173 processado com sucesso!\n",
      "✅ Processando paciente 156/817: P0175\n",
      "✅ Paciente P0175 processado com sucesso!\n",
      "✅ Processando paciente 157/817: P0176\n",
      "✅ Paciente P0176 processado com sucesso!\n",
      "✅ Processando paciente 158/817: P0177\n",
      "✅ Paciente P0177 processado com sucesso!\n",
      "✅ Processando paciente 159/817: P0179\n",
      "✅ Paciente P0179 processado com sucesso!\n",
      "✅ Processando paciente 160/817: P0181\n",
      "✅ Paciente P0181 processado com sucesso!\n",
      "✅ Processando paciente 161/817: P0182\n",
      "✅ Paciente P0182 processado com sucesso!\n",
      "✅ Processando paciente 162/817: P0183\n",
      "✅ Paciente P0183 processado com sucesso!\n",
      "✅ Processando paciente 163/817: P0184\n",
      "✅ Paciente P0184 processado com sucesso!\n",
      "✅ Processando paciente 164/817: P0186\n",
      "✅ Paciente P0186 processado com sucesso!\n",
      "✅ Processando paciente 165/817: P0187\n",
      "✅ Paciente P0187 processado com sucesso!\n",
      "✅ Processando paciente 166/817: P0188\n",
      "✅ Paciente P0188 processado com sucesso!\n",
      "✅ Processando paciente 167/817: P0189\n",
      "✅ Paciente P0189 processado com sucesso!\n",
      "✅ Processando paciente 168/817: P0190\n",
      "✅ Paciente P0190 processado com sucesso!\n",
      "✅ Processando paciente 169/817: P0191\n",
      "✅ Paciente P0191 processado com sucesso!\n",
      "✅ Processando paciente 170/817: P0192\n",
      "✅ Paciente P0192 processado com sucesso!\n",
      "✅ Processando paciente 171/817: P0193\n",
      "✅ Paciente P0193 processado com sucesso!\n",
      "✅ Processando paciente 172/817: P0194\n",
      "✅ Paciente P0194 processado com sucesso!\n",
      "✅ Processando paciente 173/817: P0195\n",
      "✅ Paciente P0195 processado com sucesso!\n",
      "✅ Processando paciente 174/817: P0196\n",
      "✅ Paciente P0196 processado com sucesso!\n",
      "✅ Processando paciente 175/817: P0197\n",
      "✅ Paciente P0197 processado com sucesso!\n",
      "✅ Processando paciente 176/817: P0198\n",
      "✅ Paciente P0198 processado com sucesso!\n",
      "✅ Processando paciente 177/817: P0199\n",
      "✅ Paciente P0199 processado com sucesso!\n",
      "✅ Processando paciente 178/817: P0200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8713e0ac00a43ad8ec7745eb7f71b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Paciente P0200 processado com sucesso!\n",
      "✅ Processando paciente 179/817: P0201\n",
      "✅ Paciente P0201 processado com sucesso!\n",
      "✅ Processando paciente 180/817: P0202\n",
      "✅ Paciente P0202 processado com sucesso!\n",
      "✅ Processando paciente 181/817: P0203\n",
      "✅ Paciente P0203 processado com sucesso!\n",
      "✅ Processando paciente 182/817: P0204\n",
      "✅ Paciente P0204 processado com sucesso!\n",
      "✅ Processando paciente 183/817: P0206\n",
      "✅ Paciente P0206 processado com sucesso!\n",
      "✅ Processando paciente 184/817: P0207\n",
      "✅ Paciente P0207 processado com sucesso!\n",
      "✅ Processando paciente 185/817: P0208\n",
      "✅ Paciente P0208 processado com sucesso!\n",
      "✅ Processando paciente 186/817: P0209\n",
      "✅ Paciente P0209 processado com sucesso!\n",
      "✅ Processando paciente 187/817: P0210\n",
      "✅ Paciente P0210 processado com sucesso!\n",
      "✅ Processando paciente 188/817: P0212\n",
      "✅ Paciente P0212 processado com sucesso!\n",
      "✅ Processando paciente 189/817: P0213\n",
      "✅ Paciente P0213 processado com sucesso!\n",
      "✅ Processando paciente 190/817: P0214\n",
      "✅ Paciente P0214 processado com sucesso!\n",
      "✅ Processando paciente 191/817: P0215\n",
      "✅ Paciente P0215 processado com sucesso!\n",
      "✅ Processando paciente 192/817: P0216\n",
      "✅ Paciente P0216 processado com sucesso!\n",
      "✅ Processando paciente 193/817: P0217\n",
      "✅ Paciente P0217 processado com sucesso!\n",
      "✅ Processando paciente 194/817: P0218\n",
      "✅ Paciente P0218 processado com sucesso!\n",
      "✅ Processando paciente 195/817: P0219\n",
      "✅ Paciente P0219 processado com sucesso!\n",
      "✅ Processando paciente 196/817: P0220\n",
      "✅ Paciente P0220 processado com sucesso!\n",
      "✅ Processando paciente 197/817: P0221\n",
      "✅ Paciente P0221 processado com sucesso!\n",
      "✅ Processando paciente 198/817: P0222\n",
      "✅ Paciente P0222 processado com sucesso!\n",
      "✅ Processando paciente 199/817: P0223\n",
      "✅ Paciente P0223 processado com sucesso!\n",
      "✅ Processando paciente 200/817: P0224\n",
      "✅ Paciente P0224 processado com sucesso!\n",
      "✅ Processando paciente 201/817: P0225\n",
      "✅ Paciente P0225 processado com sucesso!\n",
      "✅ Processando paciente 202/817: P0227\n",
      "✅ Paciente P0227 processado com sucesso!\n",
      "✅ Processando paciente 203/817: P0228\n",
      "✅ Paciente P0228 processado com sucesso!\n",
      "✅ Processando paciente 204/817: P0230\n",
      "✅ Paciente P0230 processado com sucesso!\n",
      "✅ Processando paciente 205/817: P0231\n",
      "✅ Paciente P0231 processado com sucesso!\n",
      "✅ Processando paciente 206/817: P0234\n",
      "✅ Paciente P0234 processado com sucesso!\n",
      "✅ Processando paciente 207/817: P0235\n",
      "✅ Paciente P0235 processado com sucesso!\n",
      "✅ Processando paciente 208/817: P0236\n",
      "✅ Paciente P0236 processado com sucesso!\n",
      "✅ Processando paciente 209/817: P0237\n",
      "✅ Paciente P0237 processado com sucesso!\n",
      "✅ Processando paciente 210/817: P0238\n",
      "✅ Paciente P0238 processado com sucesso!\n",
      "✅ Processando paciente 211/817: P0239\n",
      "✅ Paciente P0239 processado com sucesso!\n",
      "✅ Processando paciente 212/817: P0240\n",
      "✅ Paciente P0240 processado com sucesso!\n",
      "✅ Processando paciente 213/817: P0241\n",
      "✅ Paciente P0241 processado com sucesso!\n",
      "✅ Processando paciente 214/817: P0242\n",
      "✅ Paciente P0242 processado com sucesso!\n",
      "✅ Processando paciente 215/817: P0243\n",
      "✅ Paciente P0243 processado com sucesso!\n",
      "✅ Processando paciente 216/817: P0244\n",
      "✅ Paciente P0244 processado com sucesso!\n",
      "✅ Processando paciente 217/817: P0248\n",
      "✅ Paciente P0248 processado com sucesso!\n",
      "✅ Processando paciente 218/817: P0249\n",
      "✅ Paciente P0249 processado com sucesso!\n",
      "✅ Processando paciente 219/817: P0252\n",
      "✅ Paciente P0252 processado com sucesso!\n",
      "✅ Processando paciente 220/817: P0253\n",
      "✅ Paciente P0253 processado com sucesso!\n",
      "✅ Processando paciente 221/817: P0254\n",
      "✅ Paciente P0254 processado com sucesso!\n",
      "✅ Processando paciente 222/817: P0255\n",
      "✅ Paciente P0255 processado com sucesso!\n",
      "✅ Processando paciente 223/817: P0256\n",
      "✅ Paciente P0256 processado com sucesso!\n",
      "✅ Processando paciente 224/817: P0257\n",
      "✅ Paciente P0257 processado com sucesso!\n",
      "✅ Processando paciente 225/817: P0259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_fixpeaks.py:307: RuntimeWarning: invalid value encountered in divide\n",
      "  mrrs /= th2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Paciente P0259 processado com sucesso!\n",
      "✅ Processando paciente 226/817: P0261\n",
      "✅ Paciente P0261 processado com sucesso!\n",
      "✅ Processando paciente 227/817: P0262\n",
      "✅ Paciente P0262 processado com sucesso!\n",
      "✅ Processando paciente 228/817: P0263\n",
      "✅ Paciente P0263 processado com sucesso!\n",
      "✅ Processando paciente 229/817: P0264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_fixpeaks.py:307: RuntimeWarning: invalid value encountered in divide\n",
      "  mrrs /= th2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Paciente P0264 processado com sucesso!\n",
      "✅ Processando paciente 230/817: P0265\n",
      "✅ Paciente P0265 processado com sucesso!\n",
      "✅ Processando paciente 231/817: P0266\n",
      "✅ Paciente P0266 processado com sucesso!\n",
      "✅ Processando paciente 232/817: P0267\n",
      "✅ Paciente P0267 processado com sucesso!\n",
      "✅ Processando paciente 233/817: P0268\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m sinal_x \u001b[38;5;241m=\u001b[39m df_paciente[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msinal_x\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Processar com Neurokit2\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m ecg_x, _ \u001b[38;5;241m=\u001b[39m \u001b[43mnk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mecg_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43msinal_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneurokit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Criar dicionário para armazenar estatísticas\u001b[39;00m\n\u001b[0;32m     77\u001b[0m estatisticas_paciente \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\ecg\\ecg_process.py:127\u001b[0m, in \u001b[0;36mecg_process\u001b[1;34m(ecg_signal, sampling_rate, method)\u001b[0m\n\u001b[0;32m    117\u001b[0m signals \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m    118\u001b[0m     {\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mECG_Raw\u001b[39m\u001b[38;5;124m\"\u001b[39m: ecg_signal,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m     }\n\u001b[0;32m    124\u001b[0m )\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Delineate QRS complex\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m delineate_signal, delineate_info \u001b[38;5;241m=\u001b[39m \u001b[43mecg_delineate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mecg_cleaned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mecg_cleaned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrpeaks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mECG_R_Peaks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m info\u001b[38;5;241m.\u001b[39mupdate(delineate_info)  \u001b[38;5;66;03m# Merge waves indices dict with info dict\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Determine cardiac phases\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\ecg\\ecg_delineate.py:164\u001b[0m, in \u001b[0;36mecg_delineate\u001b[1;34m(ecg_cleaned, rpeaks, sampling_rate, method, show, show_type, check, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m     waves \u001b[38;5;241m=\u001b[39m _ecg_delineator_cwt(\n\u001b[0;32m    161\u001b[0m         ecg_cleaned, rpeaks\u001b[38;5;241m=\u001b[39mrpeaks, sampling_rate\u001b[38;5;241m=\u001b[39msampling_rate\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdwt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscrete wavelet transform\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 164\u001b[0m     waves \u001b[38;5;241m=\u001b[39m \u001b[43m_dwt_ecg_delineator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mecg_cleaned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrpeaks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeuroKit error: ecg_delineate(): \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeak\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcwt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdwt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\ecg\\ecg_delineate.py:260\u001b[0m, in \u001b[0;36m_dwt_ecg_delineator\u001b[1;34m(ecg, rpeaks, sampling_rate, analysis_sampling_rate)\u001b[0m\n\u001b[0;32m    256\u001b[0m R \u001b[38;5;241m=\u001b[39m heartbeat\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(\n\u001b[0;32m    257\u001b[0m     np\u001b[38;5;241m.\u001b[39mmin(heartbeat\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues[heartbeat\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    258\u001b[0m )\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Q wave\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m Q_index, Q \u001b[38;5;241m=\u001b[39m \u001b[43m_ecg_delineator_peak_Q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrpeak\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheartbeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m qpeaks\u001b[38;5;241m.\u001b[39mappend(Q_index)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# S wave\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\ecg\\ecg_delineate.py:1017\u001b[0m, in \u001b[0;36m_ecg_delineator_peak_Q\u001b[1;34m(rpeak, heartbeat, R)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ecg_delineator_peak_Q\u001b[39m(rpeak, heartbeat, R):\n\u001b[0;32m   1015\u001b[0m     segment \u001b[38;5;241m=\u001b[39m heartbeat\u001b[38;5;241m.\u001b[39mloc[:\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Select left hand side\u001b[39;00m\n\u001b[1;32m-> 1017\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[43msignal_findpeaks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msegment\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSignal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSignal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msegment\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSignal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(Q[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeaks\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_findpeaks.py:133\u001b[0m, in \u001b[0;36msignal_findpeaks\u001b[1;34m(signal, height_min, height_max, relative_height_min, relative_height_max, relative_mean, relative_median, relative_max)\u001b[0m\n\u001b[0;32m    131\u001b[0m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistance\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _signal_findpeaks_distances(info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeaks\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    132\u001b[0m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnsets\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _signal_findpeaks_findbase(info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeaks\u001b[39m\u001b[38;5;124m\"\u001b[39m], signal, what\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 133\u001b[0m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOffsets\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_signal_findpeaks_findbase\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPeaks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moffset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "File \u001b[1;32mc:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\signal\\signal_findpeaks.py:228\u001b[0m, in \u001b[0;36m_signal_findpeaks_findbase\u001b[1;34m(peaks, signal, what)\u001b[0m\n\u001b[0;32m    224\u001b[0m     direction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    226\u001b[0m troughs, _ \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39msignal\u001b[38;5;241m.\u001b[39mfind_peaks(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m signal)\n\u001b[1;32m--> 228\u001b[0m bases \u001b[38;5;241m=\u001b[39m \u001b[43mfind_closest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeaks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtroughs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrictly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m bases \u001b[38;5;241m=\u001b[39m as_vector(bases)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bases\n",
      "File \u001b[1;32mc:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\misc\\find_closest.py:50\u001b[0m, in \u001b[0;36mfind_closest\u001b[1;34m(closest_to, list_to_search_in, direction, strictly, return_index)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"**Find the closest number in the array from a given number x**\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Transform to arrays\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m closest_to \u001b[38;5;241m=\u001b[39m \u001b[43mas_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosest_to\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m list_to_search_in \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(as_vector(list_to_search_in))\n\u001b[0;32m     53\u001b[0m out \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     54\u001b[0m     _find_closest(i, list_to_search_in, direction, strictly, return_index) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m closest_to\n\u001b[0;32m     55\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\neurokit2\\misc\\type_converters.py:31\u001b[0m, in \u001b[0;36mas_vector\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     29\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([x])\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     34\u001b[0m     shape \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import wfdb\n",
    "import neurokit2 as nk\n",
    "\n",
    "# Conectar ao banco de dados DuckDB\n",
    "conn = duckdb.connect(r\"D:\\Projeto_Tese_mestrado\\02_Dataset\\Duckedb\\Holter_ECG\\5minutos\\banco_ecg.duckdb\")\n",
    "\n",
    "# Carregar apenas a lista de pacientes\n",
    "pacientes = conn.execute(\"SELECT DISTINCT id_paciente FROM ecg_pacientes ORDER BY id_paciente\").fetchdf()\n",
    "\n",
    "# Lista de colunas que indicam eventos no ECG\n",
    "colunas = [\n",
    "    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', \n",
    "    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', \n",
    "    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',\n",
    "    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',\n",
    "    'ECG_Phase_Completion_Ventricular'\n",
    "]\n",
    "\n",
    "# Função para calcular estatísticas\n",
    "def calcular_estatisticas(ondas):\n",
    "    if ondas.empty:  \n",
    "        return {stat: 0 for stat in [\"mean\", \"std\", \"max\", \"min\", \"25%\", \"50%\", \"75%\"]}\n",
    "    \n",
    "    # Substituir valores NaN por zero antes de calcular as estatísticas\n",
    "    ondas = ondas.fillna(0)\n",
    "    \n",
    "    estatisticas = {\n",
    "        \"mean\": ondas.mean(),\n",
    "        \"std\": ondas.std(),\n",
    "        \"max\": ondas.max(),\n",
    "        \"min\": ondas.min(),\n",
    "        \"25%\": ondas.quantile(0.25),\n",
    "        \"50%\": ondas.quantile(0.50),\n",
    "        \"75%\": ondas.quantile(0.75)\n",
    "    }\n",
    "    \n",
    "    return estatisticas\n",
    "\n",
    "# Processar paciente por paciente\n",
    "dados_processados = []\n",
    "inicio_geral = time.time()\n",
    "\n",
    "\n",
    "# Filtrar os pacientes que estão no df_filtro\n",
    "pacientes_filtrados = pacientes[pacientes['id_paciente'].isin(df_final['Patient ID'])]\n",
    "\n",
    "# Processar paciente por paciente\n",
    "dados_processados = []\n",
    "inicio_geral = time.time()\n",
    "\n",
    "for idx, paciente in enumerate(pacientes_filtrados[\"id_paciente\"]):\n",
    "    try:\n",
    "        inicio_paciente = time.time()\n",
    "        print(f\"✅ Processando paciente {idx+1}/{len(pacientes_filtrados)}: {paciente}\")\n",
    "\n",
    "        # Carregar os dados do paciente diretamente do banco\n",
    "        df_paciente = conn.execute(f\"\"\"\n",
    "            SELECT sinal_x FROM ecg_pacientes\n",
    "            WHERE id_paciente = '{paciente}'\n",
    "        \"\"\").fetchdf()\n",
    "\n",
    "        # Se não houver dados, pula o paciente\n",
    "        if df_paciente.empty:\n",
    "            print(f\"⚠️ Nenhum dado encontrado para o paciente {paciente}. Pulando...\")\n",
    "            continue\n",
    "\n",
    "        sinal_x = df_paciente[\"sinal_x\"].values\n",
    "\n",
    "        # Processar com Neurokit2\n",
    "        ecg_x, _ = nk.ecg_process(sinal_x, sampling_rate=record.fs, method='neurokit') \n",
    "\n",
    "        # Criar dicionário para armazenar estatísticas\n",
    "        estatisticas_paciente = {}\n",
    "\n",
    "        for coluna in colunas:\n",
    "            if coluna not in ecg_x.columns:\n",
    "                print(f\"⚠️ Coluna {coluna} não encontrada. Pulando...\")\n",
    "                continue \n",
    "\n",
    "            ondas = ecg_x.loc[ecg_x[coluna] == 1, 'ECG_Clean']\n",
    "            estatisticas = calcular_estatisticas(ondas)\n",
    "\n",
    "            for stat, valor in estatisticas.items():\n",
    "                estatisticas_paciente[f\"{coluna}_{stat}\"] = valor\n",
    "\n",
    "        # Adicionando o target do paciente\n",
    "        estatisticas_paciente[\"target\"] = df_final.loc[df_final[\"Patient ID\"] == paciente, \"Cause of death\"].values[0]\n",
    "\n",
    "        # Adicionar ID do paciente\n",
    "        estatisticas_paciente[\"id_paciente\"] = paciente\n",
    "\n",
    "        # Salvar os dados processados\n",
    "        dados_processados.append(estatisticas_paciente)\n",
    "\n",
    "        # Liberar memória\n",
    "        del sinal_x, ecg_x, df_paciente\n",
    "\n",
    "        print(f\"✅ Paciente {paciente} processado com sucesso!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao processar paciente {paciente}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Fechar a conexão com o banco\n",
    "conn.close()\n",
    "\n",
    "# Criar DataFrame com os resultados\n",
    "df_estatisticas_final = pd.DataFrame(dados_processados)\n",
    "df_estatisticas_final.fillna(0, inplace=True)\n",
    "\n",
    "# Salvar os resultados\n",
    "df_estatisticas_final.to_csv(r\"D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ecg_csv_estatistico\\Holter_ECG\\dados_ecg_x.csv\", index=False)\n",
    "\n",
    "# Tempo total de execução\n",
    "fim_geral = time.time()\n",
    "print(f\"🏁 Processamento concluído! Tempo total: {fim_geral - inicio_geral:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Contar o número de pacientes distintos no banco DuckDB\n",
    "# num_pacientes_distintos = conn.execute(\"SELECT COUNT(DISTINCT id_paciente) FROM ecg_pacientes\").fetchone()[0]\n",
    "\n",
    "# # Exibir a quantidade de pacientes distintos\n",
    "# print(f\"Quantidade de pacientes distintos no banco: {num_pacientes_distintos}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando o dataframe\n",
    "#df_ecg_final.to_csv(\"dados_ecg.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_estatisticas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_estatisticas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tratamento_dados(dados, filtro):\n",
    "    \"\"\"\n",
    "    Função para tratar os dados, filtrando colunas específicas e realizando pré-processamento.\n",
    "\n",
    "    Parâmetros:\n",
    "    - dados (DataFrame): O dataset a ser tratado.\n",
    "    - filtro (str): Palavra-chave para selecionar as colunas (default: \"ECG_R_Peaks\").\n",
    "\n",
    "    Retorna:\n",
    "    - X_train_scaled, X_test_scaled, y_train, y_test\n",
    "    \"\"\"\n",
    "\n",
    "    # Filtrar colunas que contêm a palavra-chave no nome\n",
    "    colunas_filtradas = dados.filter(like=filtro).columns\n",
    "    X = dados[colunas_filtradas].values  # Apenas colunas filtradas\n",
    "\n",
    "    # Garantir que o target seja separado corretamente\n",
    "    y = dados['target'].values  \n",
    "\n",
    "    # Transformar os rótulos usando LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Verificar o mapeamento dos rótulos\n",
    "    print(\"Rótulos originais:\", np.unique(y))\n",
    "    print(\"Rótulos transformados:\", np.unique(y_encoded))\n",
    "\n",
    "    # Dividir em treino e teste (80% treino, 20% teste)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n",
    "\n",
    "    # Normalizar os dados\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Converter o target para one-hot encoding se for multiclasse\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentando a Complexidade da Rede (Arquitetura)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentando o Dropout de 0.3 para 0.5\n",
    "\n",
    "Adicionando Regularização L2 (Penalização de Pesos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adotar a função de Ativação Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Reduzindo a Taxa de Aprendizado com ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "def construir_rede_neural(X_train, X_test, y_train, y_test,input_dim):\n",
    "    # Criar modelo poderoso\n",
    "    def criar_rede_neural(input_dim, num_classes):\n",
    "        model = keras.Sequential([\n",
    "        keras.Input(shape=(input_dim,)),  # Corrigindo a entrada\n",
    "        Dense(512),\n",
    "        LeakyReLU(negative_slope=0.1),  # Corrigindo o parâmetro alpha\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(256),\n",
    "        LeakyReLU(negative_slope=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(128),\n",
    "        LeakyReLU(negative_slope=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(64),\n",
    "        LeakyReLU(negative_slope=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "        # Compilar modelo\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),  # Adam já é otimizado para deep learning\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # Criar callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10)\n",
    "\n",
    "\n",
    "    # Criar e treinar o modelo\n",
    "    modelo = criar_rede_neural(input_dim=input_dim, num_classes=4)\n",
    "    history = modelo.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=0  # Define para 0 para ocultar a saída\n",
    "        #verbose=2  # Apenas mostra os valores das métricas\n",
    "    )\n",
    "\n",
    "    # Avaliação\n",
    "    loss, acc = modelo.evaluate(X_test, y_test)\n",
    "    print(f\"\\nAcurácia no conjunto de validação: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link =r\"D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ecg_csv_estatistico\\Holter_ECG/dados_ecg_x.csv\"\n",
    "df_x = pd.read_csv(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "colunas = [\n",
    "    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', \n",
    "    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', \n",
    "    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',\n",
    "    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',\n",
    "    'ECG_Phase_Completion_Ventricular'\n",
    "]\n",
    "\n",
    "for col in colunas:\n",
    "    print(f'Executando {col}')\n",
    "    \n",
    "    start_time = time.time()  # Inicia a contagem do tempo\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = tratamento_dados(df_x, filtro=col)\n",
    "    construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])\n",
    "    \n",
    "    end_time = time.time()  # Finaliza a contagem do tempo\n",
    "    elapsed_time = end_time - start_time  # Calcula o tempo decorrido\n",
    "    \n",
    "    print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')\n",
    "    print('=' * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agora vou testar usando o Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import wfdb\n",
    "import neurokit2 as nk\n",
    "\n",
    "# Conectar ao banco de dados DuckDB\n",
    "conn = duckdb.connect(r\"D:\\Projeto_Tese_mestrado\\02_Dataset\\Duckedb\\Holter_ECG\\5minutos\\banco_ecg.duckdb\")\n",
    "\n",
    "# Carregar apenas a lista de pacientes\n",
    "pacientes = conn.execute(\"SELECT DISTINCT id_paciente FROM ecg_pacientes ORDER BY id_paciente\").fetchdf()\n",
    "\n",
    "# Lista de colunas que indicam eventos no ECG\n",
    "colunas = [\n",
    "    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', \n",
    "    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', \n",
    "    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',\n",
    "    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',\n",
    "    'ECG_Phase_Completion_Ventricular'\n",
    "]\n",
    "\n",
    "# Função para calcular estatísticas\n",
    "def calcular_estatisticas(ondas):\n",
    "    if ondas.empty:  \n",
    "        return {stat: 0 for stat in [\"mean\", \"std\", \"max\", \"min\", \"25%\", \"50%\", \"75%\"]}\n",
    "    \n",
    "    # Substituir valores NaN por zero antes de calcular as estatísticas\n",
    "    ondas = ondas.fillna(0)\n",
    "\n",
    "    # Evitar divisão por zero substituindo valores muito pequenos\n",
    "    ondas[ondas == 0] = 1e-6\n",
    "    \n",
    "    estatisticas = {\n",
    "        \"mean\": ondas.mean(),\n",
    "        \"std\": ondas.std(),\n",
    "        \"max\": ondas.max(),\n",
    "        \"min\": ondas.min(),\n",
    "        \"25%\": ondas.quantile(0.25),\n",
    "        \"50%\": ondas.quantile(0.50),\n",
    "        \"75%\": ondas.quantile(0.75)\n",
    "    }\n",
    "    \n",
    "    return estatisticas\n",
    "\n",
    "# Processar paciente por paciente\n",
    "dados_processados = []\n",
    "inicio_geral = time.time()\n",
    "\n",
    "\n",
    "# Filtrar os pacientes que estão no df_filtro\n",
    "pacientes_filtrados = pacientes[pacientes['id_paciente'].isin(df_final['Patient ID'])]\n",
    "\n",
    "# Processar paciente por paciente\n",
    "dados_processados = []\n",
    "inicio_geral = time.time()\n",
    "\n",
    "for idx, paciente in enumerate(pacientes_filtrados[\"id_paciente\"]):\n",
    "    try:\n",
    "        inicio_paciente = time.time()\n",
    "        print(f\"✅ Processando paciente {idx+1}/{len(pacientes_filtrados)}: {paciente}\")\n",
    "\n",
    "        # Carregar os dados do paciente diretamente do banco\n",
    "        df_paciente = conn.execute(f\"\"\"\n",
    "            SELECT sinal_y FROM ecg_pacientes\n",
    "            WHERE id_paciente = '{paciente}'\n",
    "        \"\"\").fetchdf()\n",
    "\n",
    "        # Se não houver dados, pula o paciente\n",
    "        if df_paciente.empty:\n",
    "            print(f\"⚠️ Nenhum dado encontrado para o paciente {paciente}. Pulando...\")\n",
    "            continue\n",
    "\n",
    "        sinal_y = df_paciente[\"sinal_y\"].values\n",
    "\n",
    "        # Processar com Neurokit2\n",
    "        ecg_y, _ = nk.ecg_process(sinal_y, sampling_rate=record.fs, method='neurokit') \n",
    "\n",
    "        # Criar dicionário para armazenar estatísticas\n",
    "        estatisticas_paciente = {}\n",
    "\n",
    "        for coluna in colunas:\n",
    "            if coluna not in ecg_y.columns:\n",
    "                print(f\"⚠️ Coluna {coluna} não encontrada. Pulando...\")\n",
    "                continue \n",
    "\n",
    "            ondas = ecg_y.loc[ecg_y[coluna] == 1, 'ECG_Clean']\n",
    "            estatisticas = calcular_estatisticas(ondas)\n",
    "\n",
    "            for stat, valor in estatisticas.items():\n",
    "                estatisticas_paciente[f\"{coluna}_{stat}\"] = valor\n",
    "\n",
    "        # Adicionando o target do paciente\n",
    "        estatisticas_paciente[\"target\"] = df_final.loc[df_final[\"Patient ID\"] == paciente, \"Cause of death\"].values[0]\n",
    "\n",
    "        # Adicionar ID do paciente\n",
    "        estatisticas_paciente[\"id_paciente\"] = paciente\n",
    "\n",
    "        # Salvar os dados processados\n",
    "        dados_processados.append(estatisticas_paciente)\n",
    "\n",
    "        # Liberar memória\n",
    "        del sinal_y, ecg_y, df_paciente\n",
    "\n",
    "        print(f\"✅ Paciente {paciente} processado com sucesso!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao processar paciente {paciente}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Fechar a conexão com o banco\n",
    "conn.close()\n",
    "\n",
    "# Criar DataFrame com os resultados\n",
    "df_estatisticas_final = pd.DataFrame(dados_processados)\n",
    "\n",
    "# Substituir valores infinitos por um valor pequeno\n",
    "df_estatisticas_final.replace([np.inf, -np.inf], 1e-6, inplace=True)\n",
    "\n",
    "# Substituir NaN por zero\n",
    "df_estatisticas_final.fillna(0, inplace=True)\n",
    "\n",
    "# Salvar os resultados\n",
    "df_estatisticas_final.to_csv(r\"D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ecg_csv_estatistico\\Holter_ECGdados_ecg_y.csv\", index=False)\n",
    "\n",
    "# Tempo total de execução\n",
    "fim_geral = time.time()\n",
    "print(f\"🏁 Processamento concluído! Tempo total: {fim_geral - inicio_geral:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link =r\"D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ecg_csv_estatistico\\Holter_ECG\\dados_ecg_y.csv\"\n",
    "df_y = pd.read_csv(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "colunas = [\n",
    "    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', \n",
    "    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', \n",
    "    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',\n",
    "    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',\n",
    "    'ECG_Phase_Completion_Ventricular'\n",
    "]\n",
    "\n",
    "for col in colunas:\n",
    "    print(f'Executando {col}')\n",
    "    \n",
    "    start_time = time.time()  # Inicia a contagem do tempo\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = tratamento_dados(df_y, filtro=col)\n",
    "    construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])\n",
    "    \n",
    "    end_time = time.time()  # Finaliza a contagem do tempo\n",
    "    elapsed_time = end_time - start_time  # Calcula o tempo decorrido\n",
    "    \n",
    "    print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')\n",
    "    print('=' * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agora vou testar usando o Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import wfdb\n",
    "import neurokit2 as nk\n",
    "\n",
    "# Conectar ao banco de dados DuckDB\n",
    "conn = duckdb.connect(r\"D:\\Projeto_Tese_mestrado\\02_Dataset\\Duckedb\\Holter_ECG\\5minutos\\banco_ecg.duckdb\")\n",
    "\n",
    "# Carregar apenas a lista de pacientes\n",
    "pacientes = conn.execute(\"SELECT DISTINCT id_paciente FROM ecg_pacientes ORDER BY id_paciente\").fetchdf()\n",
    "\n",
    "# Lista de colunas que indicam eventos no ECG\n",
    "colunas = [\n",
    "    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', \n",
    "    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', \n",
    "    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',\n",
    "    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',\n",
    "    'ECG_Phase_Completion_Ventricular'\n",
    "]\n",
    "\n",
    "# Função para calcular estatísticas\n",
    "def calcular_estatisticas(ondas):\n",
    "    if ondas.empty:  \n",
    "        return {stat: 0 for stat in [\"mean\", \"std\", \"max\", \"min\", \"25%\", \"50%\", \"75%\"]}\n",
    "    \n",
    "    # Substituir valores NaN por zero antes de calcular as estatísticas\n",
    "    ondas = ondas.fillna(0)\n",
    "\n",
    "    # Evitar divisão por zero substituindo valores muito pequenos\n",
    "    ondas[ondas == 0] = 1e-6\n",
    "    \n",
    "    estatisticas = {\n",
    "        \"mean\": ondas.mean(),\n",
    "        \"std\": ondas.std(),\n",
    "        \"max\": ondas.max(),\n",
    "        \"min\": ondas.min(),\n",
    "        \"25%\": ondas.quantile(0.25),\n",
    "        \"50%\": ondas.quantile(0.50),\n",
    "        \"75%\": ondas.quantile(0.75)\n",
    "    }\n",
    "    \n",
    "    return estatisticas\n",
    "\n",
    "# Processar paciente por paciente\n",
    "dados_processados = []\n",
    "inicio_geral = time.time()\n",
    "\n",
    "\n",
    "# Filtrar os pacientes que estão no df_filtro\n",
    "pacientes_filtrados = pacientes[pacientes['id_paciente'].isin(df_final['Patient ID'])]\n",
    "\n",
    "# Processar paciente por paciente\n",
    "dados_processados = []\n",
    "inicio_geral = time.time()\n",
    "\n",
    "for idx, paciente in enumerate(pacientes_filtrados[\"id_paciente\"]):\n",
    "    try:\n",
    "        inicio_paciente = time.time()\n",
    "        print(f\"✅ Processando paciente {idx+1}/{len(pacientes_filtrados)}: {paciente}\")\n",
    "\n",
    "        # Carregar os dados do paciente diretamente do banco\n",
    "        df_paciente = conn.execute(f\"\"\"\n",
    "            SELECT sinal_z FROM ecg_pacientes\n",
    "            WHERE id_paciente = '{paciente}'\n",
    "        \"\"\").fetchdf()\n",
    "\n",
    "        # Se não houver dados, pula o paciente\n",
    "        if df_paciente.empty:\n",
    "            print(f\"⚠️ Nenhum dado encontrado para o paciente {paciente}. Pulando...\")\n",
    "            continue\n",
    "\n",
    "        sinal_z = df_paciente[\"sinal_z\"].values\n",
    "\n",
    "        # Processar com Neurokit2\n",
    "        ecg_z, _ = nk.ecg_process(sinal_z, sampling_rate=record.fs, method='neurokit') \n",
    "\n",
    "        # Criar dicionário para armazenar estatísticas\n",
    "        estatisticas_paciente = {}\n",
    "\n",
    "        for coluna in colunas:\n",
    "            if coluna not in ecg_z.columns:\n",
    "                print(f\"⚠️ Coluna {coluna} não encontrada. Pulando...\")\n",
    "                continue \n",
    "\n",
    "            ondas = ecg_z.loc[ecg_z[coluna] == 1, 'ECG_Clean']\n",
    "            estatisticas = calcular_estatisticas(ondas)\n",
    "\n",
    "            for stat, valor in estatisticas.items():\n",
    "                estatisticas_paciente[f\"{coluna}_{stat}\"] = valor\n",
    "\n",
    "        # Adicionando o target do paciente\n",
    "        estatisticas_paciente[\"target\"] = df_final.loc[df_final[\"Patient ID\"] == paciente, \"Cause of death\"].values[0]\n",
    "\n",
    "        # Adicionar ID do paciente\n",
    "        estatisticas_paciente[\"id_paciente\"] = paciente\n",
    "\n",
    "        # Salvar os dados processados\n",
    "        dados_processados.append(estatisticas_paciente)\n",
    "\n",
    "        # Liberar memória\n",
    "        del sinal_z, ecg_z, df_paciente\n",
    "\n",
    "        print(f\"✅ Paciente {paciente} processado com sucesso!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao processar paciente {paciente}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Fechar a conexão com o banco\n",
    "conn.close()\n",
    "\n",
    "# Criar DataFrame com os resultados\n",
    "df_estatisticas_final = pd.DataFrame(dados_processados)\n",
    "\n",
    "# Substituir valores infinitos por um valor pequeno\n",
    "df_estatisticas_final.replace([np.inf, -np.inf], 1e-6, inplace=True)\n",
    "\n",
    "# Substituir NaN por zero\n",
    "df_estatisticas_final.fillna(0, inplace=True)\n",
    "\n",
    "# Salvar os resultados\n",
    "df_estatisticas_final.to_csv(r\"D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ecg_csv_estatistico\\Holter_ECG\\dados_ecg_z.csv\", index=False)\n",
    "\n",
    "# Tempo total de execução\n",
    "fim_geral = time.time()\n",
    "print(f\"🏁 Processamento concluído! Tempo total: {fim_geral - inicio_geral:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = r\"D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ecg_csv_estatistico\\Holter_ECG\\dados_ecg_z.csv\"\n",
    "df_z = pd.read_csv(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "colunas = [\n",
    "    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', \n",
    "    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', \n",
    "    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',\n",
    "    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',\n",
    "    'ECG_Phase_Completion_Ventricular'\n",
    "]\n",
    "\n",
    "for col in colunas:\n",
    "    print(f'Executando {col}')\n",
    "    \n",
    "    start_time = time.time()  # Inicia a contagem do tempo\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = tratamento_dados(df_z, filtro=col)\n",
    "    construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])\n",
    "    \n",
    "    end_time = time.time()  # Finaliza a contagem do tempo\n",
    "    elapsed_time = end_time - start_time  # Calcula o tempo decorrido\n",
    "    \n",
    "    print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')\n",
    "    print('=' * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agora vou testar usando a Tabela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando somente a tabela dos pacientes sem usar o ECG para comparar com o ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "def tratamento_dados(dados):\n",
    "    \"\"\"\n",
    "    Função para tratar os dados, realizando pré-processamento sem filtro específico.\n",
    "\n",
    "    Parâmetros:\n",
    "    - dados (DataFrame): O dataset a ser tratado.\n",
    "\n",
    "    Retorna:\n",
    "    - X_train_scaled, X_test_scaled, y_train, y_test\n",
    "    \"\"\"\n",
    "\n",
    "    # Separar as features (X) e o target (y)\n",
    "    X = dados.drop(columns=['target']).values  # Remove a coluna target e pega as features\n",
    "    y = dados['target'].values  # Mantém apenas a coluna target\n",
    "\n",
    "    # Transformar os rótulos usando LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Verificar o mapeamento dos rótulos\n",
    "    print(\"Rótulos originais:\", np.unique(y))\n",
    "    print(\"Rótulos transformados:\", np.unique(y_encoded))\n",
    "\n",
    "    # Dividir em treino e teste (80% treino, 20% teste)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n",
    "\n",
    "    # Normalizar os dados\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Converter o target para one-hot encoding se for multiclasse\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_csv = r'D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_csv_info_definitions\\ubject-info_limpo.csv'\n",
    "\n",
    "dados = pd.read_csv(link_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df =  dados.rename(columns={'Cause of death': 'target'})\n",
    "\n",
    "# Eletrocardiogramas e Holter\n",
    "eletrocardiogramas_holter = [\n",
    "    'Hig-resolution ECG available',\n",
    "    'ECG rhythm ',\n",
    "    'Q-waves (necrosis. yes=1)',\n",
    "    'PR interval (ms)',\n",
    "    'QRS duration (ms)',\n",
    "    'QRS > 120 ms ',\n",
    "    'QT interval (ms)',\n",
    "    'QT corrected ',\n",
    "    'Average RR (ms)',\n",
    "    'Left ventricular hypertrophy (yes=1)',\n",
    "    'Intraventricular conduction disorder',\n",
    "    'target'\n",
    "]\n",
    "df = df[eletrocardiogramas_holter]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['target'].isin([1])] # tirando valores que tem 1 pois significa mortes não identificada ou seja ruídos,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(f'Executando eletrocardiogramas_holter')\n",
    "print(f'Resulado da tabela')\n",
    "start_time = time.time()  # Inicia a contagem do tempo\n",
    "\n",
    "X_train, X_test, y_train, y_test = tratamento_dados(df)\n",
    "construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])\n",
    "\n",
    "end_time = time.time()  # Finaliza a contagem do tempo\n",
    "elapsed_time = end_time - start_time  # Calcula o tempo decorrido\n",
    "\n",
    "print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')\n",
    "print('=' * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vou testar a tabela para que o target venha ser binarios 0 para quem não morreu e vou transformar todos outros 1 para quem morreu segundo doenças do coração, Observação vou fazer isso depois de ter excluido os pacientes que são mortes não identificada ou seja ruídos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformando \n",
    "df['target'] = np.where(df['target'] != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repetindo o modeloe o tratamento de dados para ajustar a classe binária sigmod.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def tratamento_dados(dados):\n",
    "    \"\"\"\n",
    "    Função para tratar os dados para classificação binária.\n",
    "\n",
    "    Parâmetros:\n",
    "    - dados (DataFrame): O dataset contendo as features e a variável alvo.\n",
    "\n",
    "    Retorna:\n",
    "    - X_train_scaled, X_test_scaled, y_train, y_test\n",
    "    \"\"\"\n",
    "\n",
    "    # Separar as features (X) e o target (y)\n",
    "    X = dados.drop(columns=['target']).values  # Remove a coluna target\n",
    "    y = dados['target'].values  # Mantém apenas a coluna target\n",
    "\n",
    "    # Verificar os rótulos únicos no conjunto de dados\n",
    "    print(\"Rótulos originais:\", np.unique(y))\n",
    "\n",
    "    # Garantir que os rótulos sejam 0 e 1\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Dividir em treino e teste (80% treino, 20% teste)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    # Normalizar os dados\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Converter para float32 (necessário para o TensorFlow/Keras)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    y_test = y_test.astype(np.float32)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def construir_rede_neural(X_train, X_test, y_train, y_test, input_dim):\n",
    "    def criar_rede_neural(input_dim):\n",
    "        model = keras.Sequential([\n",
    "            keras.Input(shape=(input_dim,)),\n",
    "            Dense(512),\n",
    "            LeakyReLU(negative_slope=0.1),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "\n",
    "            Dense(256),\n",
    "            LeakyReLU(negative_slope=0.1),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "\n",
    "            Dense(128),\n",
    "            LeakyReLU(negative_slope=0.1),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "\n",
    "            Dense(64),\n",
    "            LeakyReLU(negative_slope=0.1),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "\n",
    "            Dense(1, activation='sigmoid')  # Apenas 1 neurônio com ativação sigmoid\n",
    "        ])\n",
    "\n",
    "        # Compilar modelo\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',  # Perda ajustada para binário\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # Criar callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10)\n",
    "\n",
    "    # Criar e treinar o modelo\n",
    "    modelo = criar_rede_neural(input_dim=input_dim)\n",
    "    history = modelo.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Avaliação\n",
    "    loss, acc = modelo.evaluate(X_test, y_test)\n",
    "    print(f\"\\nAcurácia no conjunto de validação: {acc:.4f}\")\n",
    "\n",
    "    return modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(f'Executando eletrocardiogramas_holter')\n",
    "print(f'Resulado da tabela')\n",
    "start_time = time.time()  # Inicia a contagem do tempo\n",
    "\n",
    "X_train, X_test, y_train, y_test = tratamento_dados(df)\n",
    "construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])\n",
    "\n",
    "end_time = time.time()  # Finaliza a contagem do tempo\n",
    "elapsed_time = end_time - start_time  # Calcula o tempo decorrido\n",
    "\n",
    "print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')\n",
    "print('=' * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agora vou testar os dados estastístico por X, Y e Z em suas iguadades exemplo a média do X, Y e Z e treinar e fazer isso para toda ondas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkx = r\"D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ecg_csv_estatistico\\Holter_ECG\\dados_ecg_x.csv\"\n",
    "linky = r\"D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ecg_csv_estatistico\\Holter_ECG\\dados_ecg_y.csv\"\n",
    "linkz = r\"D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ecg_csv_estatistico\\Holter_ECG\\dados_ecg_z.csv\"\n",
    "\n",
    "df_x = pd.read_csv(linkx)\n",
    "df_y = pd.read_csv(linky)\n",
    "df_z = pd.read_csv(linkz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'dados do\\n x {df_x.shape}\\n y {df_y.shape}\\n z {df_z.shape}') # com essa conclusão os ids dos pacientes são iguais em tamanho e dimensão os target tambem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se as colunas \"target\" são iguais nos dois DataFrames\n",
    "iguais_target = df_x[\"target\"].equals(df_y[\"target\"])\n",
    "print(\"As colunas de x e y 'target' são iguais?\", iguais_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iguais_target = df_x[\"target\"].equals(df_z[\"target\"])\n",
    "print(\"As colunas de x e z 'target' são iguais?\", iguais_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diferencas = df_x[[\"target\"]] != df_y[[\"target\"]]\n",
    "print(\"Diferenças encontradas:\\n\", df_x[diferencas.any(axis=1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_x.columns:\n",
    "    iguais = df_x[col].equals(df_y[col])\n",
    "    print(f\"As colunas de {col} de x e y são iguais?\", iguais)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Substitua df pelo seu DataFrame real\n",
    "colunas = df_x.columns  # Obtém todas as colunas do DataFrame\n",
    "\n",
    "for coluna in colunas:\n",
    "    valores_repetidos = df_x[coluna].value_counts()\n",
    "    valores_repetidos = valores_repetidos[valores_repetidos > 1]  # Filtra apenas valores repetidos\n",
    "    \n",
    "    if not valores_repetidos.empty:\n",
    "        print(f\"\\nPosições dos valores repetidos na coluna '{coluna}':\")\n",
    "        for valor in valores_repetidos.index:\n",
    "            posicoes = df_x[df_x[coluna] == valor].index.tolist()  # Obtém os índices das ocorrências\n",
    "            print(f\"Valor: {valor} -> Índices: {posicoes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as possiveis causas do zero seja porque tem valores negativo e por conta disso as divisão ou calculos estastísticos pode resultar em zero. \n",
    "# minha recomendação mudar a limpeza dos dados para outro tipo de padrão para não ficar valores entre um, zero e menor que zero.\n",
    "\n",
    "# recomendação segundo a documentação  o método biosppy porque usa um filtro FIR ([0.67, 45] Hz), evitando distorções de frequência e garantindo que apenas informações relevantes sejam mantidas.\n",
    "#É especialmente útil para sinais fisiológicos porque preserva a forma das ondas.\n",
    "\n",
    "colunas = df_x.columns  # Obtém todas as colunas do DataFrame\n",
    "\n",
    "for coluna in colunas:\n",
    "    zeros = df_x[df_x[coluna] == 0]  # Filtra as linhas onde o valor é zero\n",
    "    \n",
    "    if not zeros.empty:\n",
    "        print(f\"\\nColuna '{coluna}':\")\n",
    "        print(f\"Quantidade de zeros: {zeros.shape[0]}\")  # Conta quantos zeros existem\n",
    "        print(f\"Índices onde há zero: {zeros.index.tolist()}\")  # Lista os índices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amostra = p_signal[0:10000,0]\n",
    "ecg_x, _ = nk.ecg_process(amostra, sampling_rate= record.fs , method='neurokit') # Dividindo a tupla em DataFrame e Dicionário\n",
    "\n",
    "signals = pd.DataFrame({\n",
    "    \"ECG_Raw\" : ecg_x['ECG_Clean'],\n",
    "    \"ECG_NeuroKit\" : nk.ecg_clean(ecg_x['ECG_Clean'], sampling_rate=1000, method=\"neurokit\"),\n",
    "    \"ECG_BioSPPy\" : nk.ecg_clean(ecg_x['ECG_Clean'], sampling_rate=1000, method=\"biosppy\"),\n",
    "    \"ECG_PanTompkins\" : nk.ecg_clean(ecg_x['ECG_Clean'], sampling_rate=1000, method=\"pantompkins1985\"),\n",
    "    \"ECG_Hamilton\" : nk.ecg_clean(ecg_x['ECG_Clean'], sampling_rate=1000, method=\"hamilton2002\"),\n",
    "    \"ECG_Elgendi\" : nk.ecg_clean(ecg_x['ECG_Clean'], sampling_rate=1000, method=\"elgendi2010\"),\n",
    "    \"ECG_EngZeeMod\" : nk.ecg_clean(ecg_x['ECG_Clean'], sampling_rate=1000, method=\"engzeemod2012\"),\n",
    "    \"ECG_VG\" : nk.ecg_clean(ecg_x['ECG_Clean'], sampling_rate=1000, method=\"vg\"),\n",
    "    \"ECG_TC\" : nk.ecg_clean(ecg_x['ECG_Clean'], sampling_rate=1000, method=\"templateconvolution\")\n",
    "})\n",
    "\n",
    "\n",
    "signals.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicadas = df_x.duplicated().sum()\n",
    "print(f\"O DataFrame tem {duplicadas} linhas duplicadas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicadas = df_x[df_x.duplicated()]\n",
    "print(df_duplicadas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "def construir_rede_neural(X_train, X_test, y_train, y_test,input_dim):\n",
    "    # Criar modelo poderoso\n",
    "    def criar_rede_neural(input_dim, num_classes):\n",
    "        model = keras.Sequential([\n",
    "        keras.Input(shape=(input_dim,)),  # Corrigindo a entrada\n",
    "        Dense(512),\n",
    "        LeakyReLU(negative_slope=0.1),  # Corrigindo o parâmetro alpha\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(256),\n",
    "        LeakyReLU(negative_slope=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(128),\n",
    "        LeakyReLU(negative_slope=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(64),\n",
    "        LeakyReLU(negative_slope=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "        # Compilar modelo\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),  # Adam já é otimizado para deep learning\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # Criar callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10)\n",
    "\n",
    "\n",
    "    # Criar e treinar o modelo\n",
    "    modelo = criar_rede_neural(input_dim=input_dim, num_classes=4)\n",
    "    history = modelo.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=0  # Define para 0 para ocultar a saída\n",
    "        #verbose=2  # Apenas mostra os valores das métricas\n",
    "    )\n",
    "\n",
    "    # Avaliação\n",
    "    loss, acc = modelo.evaluate(X_test, y_test)\n",
    "    print(f\"\\nAcurácia no conjunto de validação: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time\n",
    "\n",
    "\n",
    "def tratamento_dados(dados):\n",
    "    \"\"\"\n",
    "    Função para tratar os dados, realizando pré-processamento sem filtro específico.\n",
    "\n",
    "    Parâmetros:\n",
    "    - dados (DataFrame): O dataset a ser tratado.\n",
    "\n",
    "    Retorna:\n",
    "    - X_train_scaled, X_test_scaled, y_train, y_test\n",
    "    \"\"\"\n",
    "\n",
    "    # Separar as features (X) e o target (y)\n",
    "    X = dados.drop(columns=['target_x']).values  # Remove a coluna target e pega as features\n",
    "    y = dados['target_x'].values  # Mantém apenas a coluna target\n",
    "\n",
    "    # Transformar os rótulos usando LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Verificar o mapeamento dos rótulos\n",
    "    print(\"Rótulos originais:\", np.unique(y))\n",
    "    print(\"Rótulos transformados:\", np.unique(y_encoded))\n",
    "\n",
    "    # Dividir em treino e teste (80% treino, 20% teste)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n",
    "\n",
    "    # Normalizar os dados\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Converter o target para one-hot encoding se for multiclasse\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomear colunas (exceto id_paciente) para evitar conflito de nomes\n",
    "df_x = df_x.rename(columns={col: col + \"_x\" for col in df_x.columns if col != \"id_paciente\"})\n",
    "df_y = df_y.rename(columns={col: col + \"_y\" for col in df_y.columns if col != \"id_paciente\"})\n",
    "df_z = df_z.rename(columns={col: col + \"_z\" for col in df_z.columns if col != \"id_paciente\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_z.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# colunas = ['mean', 'std', 'max', 'min', '75', '25', '50'] desativado \n",
    "\n",
    "# Meio simples de fazer mais um pouco errado pois estou fazendo por pocisão de colunas como o df_x df_y df_z tem as colunas iguais em relação a nome vou usar esse método porém o certo é pegar as colunas filtrar ou fazer um merge para concatenar média std ...\n",
    "for col in range(df_x.shape[1]):\n",
    "    #print(f'Executando {col}')\n",
    "    \n",
    "    start_time = time.time()  # Inicia a contagem do tempo\n",
    "\n",
    "\n",
    "    #target = df_x['target']\n",
    "    df_combinado = pd.concat([df_x.iloc[:,col], df_y.iloc[:,col], df_z.iloc[:,col],df_x['target_x']], axis=1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = tratamento_dados(df_combinado)\n",
    "    construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])\n",
    "    \n",
    "    end_time = time.time()  # Finaliza a contagem do tempo\n",
    "    elapsed_time = end_time - start_time  # Calcula o tempo decorrido\n",
    "    \n",
    "    print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')\n",
    "    print('=' * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x['target_x']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMCS713EE+sC1as/alF5ev1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
