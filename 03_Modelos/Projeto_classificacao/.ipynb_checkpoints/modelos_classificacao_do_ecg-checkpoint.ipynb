{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ECG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HclOK4e4Rmij"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "#pio.renderers.default = \"colab\"\n",
    "import neurokit2 as nk\n",
    "#pio.renderers.default = \"browser\"  # Abre o gráfico no navegador\n",
    "# ou\n",
    "#pio.renderers.default = \"notebook\"  # Usa o modo compatível com notebooks\n",
    "# Configurar o tamanho da figura globalmente\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 6)  # Ajuste a largura e altura aqui\n",
    "\n",
    "# Definir o número máximo de linhas a serem exibidas\n",
    "pd.set_option('display.max_rows', None)  # Exibe todas as linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import glob\n",
    "\n",
    "# # Define o padrão para os arquivos CSV\n",
    "\n",
    "# path = r'D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ECG\\High-resolution_ECG\\P0*'\n",
    "# path_holter = r'D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ECG\\Holter_ECG\\P0*'\n",
    "\n",
    "# #path = r'E:/Repositorio_Git/zzz-projeto_final/dados/ca-*.csv'\n",
    "\n",
    "\n",
    "# # Usando glob para pegar todos os arquivos que seguem o padrão\n",
    "# arquivos = glob.glob(path)\n",
    "# arquivos_path_holter = glob.glob(path_holter)\n",
    "# arquivos = [os.path.splitext(arquivo)[0] for arquivo in arquivos] # tirando as a extensões\n",
    "\n",
    "# arquivos_path_holter = [os.path.splitext(arquivos_path_holter)[0] for arquivos_path_holter in arquivos_path_holter] # tirando as a extensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define o padrão para os arquivos CSV\n",
    "\n",
    "path = r'D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ECG\\High-resolution_ECG\\P0*'\n",
    "#path_holter = r'D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ECG\\Holter_ECG\\P0*'\n",
    "\n",
    "#path = r'E:/Repositorio_Git/zzz-projeto_final/dados/ca-*.csv'\n",
    "\n",
    "\n",
    "# Usando glob para pegar todos os arquivos que seguem o padrão\n",
    "arquivos = glob.glob(path)\n",
    "#arquivos_path_holter = glob.glob(path_holter)\n",
    "arquivos = [os.path.splitext(arquivo)[0] for arquivo in arquivos] # tirando as a extensões\n",
    "\n",
    "#arquivos_path_holter = [os.path.splitext(arquivos_path_holter)[0] for arquivos_path_holter in arquivos_path_holter] # tirando as a extensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #arquivos_path_holter\n",
    "\n",
    "# record = wfdb.rdrecord(arquivos_path_holter[0])\n",
    "\n",
    "\n",
    "# # Exibir as informações do arquivo\n",
    "# print(record.__dict__)\n",
    "#arquivos_path_holter\n",
    "\n",
    "lista_sem_duplicatas = pd.Series(arquivos).unique().tolist() # removando duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_sem_duplicatas[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = wfdb.rdrecord(lista_sem_duplicatas[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_signal = record.p_signal\n",
    "p_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinal_x = p_signal[:,0]\n",
    "sinal_y = p_signal[:,1]\n",
    "sinal_z = p_signal[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sinal_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_cleaned = nk.ecg_clean(sinal_x, sampling_rate=record.fs, method=\"neurokit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_cleaned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality = nk.ecg_quality(ecg_cleaned, sampling_rate=record.fs)\n",
    "nk.signal_plot([ecg_cleaned, quality], standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exg_dataframe = pd.DataFrame(ecg_cleaned, columns=['ECG_Clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exg_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#px.line(exg_dataframe[1000:2000], x=exg_dataframe.index[0:1000], y = 'ECG_Clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ecg_x, dict_info_x = nk.ecg_process(sinal_x, sampling_rate= record.fs , method='neurokit') # Dividindo a tupla em DataFrame e Dicionário\n",
    "#ecg_y, dict_info_y = nk.ecg_process(sinal_y, sampling_rate= record.fs , method='neurokit')\n",
    "#ecg_z, dict_info_z = nk.ecg_process(sinal_z, sampling_rate= record.fs , method='neurokit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_x['ECG_Clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somente um paciente contem \n",
    "ecg_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 'ECG_R_Peaks'\n",
    "- Descrição: Refere-se aos picos R do ECG, que são a parte mais alta da onda QRS.\n",
    "Importância: O pico R é um dos eventos mais importantes no ECG, pois indica a despolarização dos ventrículos. É amplamente usado para calcular a frequência cardíaca e identificar batimentos.\n",
    "2. 'ECG_P_Peaks'\n",
    "- Descrição: Refere-se aos picos P, que estão associados à despolarização atrial.\n",
    "Importância: O pico P representa a contração dos átrios do coração. Ele marca o início de cada batimento cardíaco.\n",
    "3. 'ECG_P_Onsets'\n",
    "- Descrição: Marca o início da onda P no ECG, ou seja, o começo da despolarização atrial.\n",
    "Importância: Ajuda a identificar a transição do atividade atrial para o início da contração ventricular.\n",
    "4. 'ECG_P_Offsets'\n",
    "- Descrição: Marca o final da onda P no ECG, ou seja, o término da despolarização atrial.\n",
    "Importância: Indica quando a atividade atrial termina, permitindo a análise do ciclo atrial e ventricular.\n",
    "5. 'ECG_Q_Peaks'\n",
    "- Descrição: Refere-se aos picos Q, que são uma parte da onda QRS.\n",
    "Importância: O pico Q representa a despolarização do septo interventricular. Ele pode ser pequeno ou até ausente em algumas pessoas.\n",
    "6. 'ECG_R_Onsets'\n",
    "- Descrição: Marca o início do pico R, ou seja, o início da despolarização ventricular.\n",
    "Importância: Esse valor ajuda a identificar o momento exato do começo da contração ventricular.\n",
    "7. 'ECG_R_Offsets'\n",
    "- Descrição: Marca o fim do pico R, ou seja, quando a despolarização ventricular está terminando.\n",
    "Importância: Indica o ponto de transição entre a despolarização e a repolarização ventricular.\n",
    "8. 'ECG_S_Peaks'\n",
    "- Descrição: Refere-se aos picos S, que estão associados ao final da onda QRS.\n",
    "Importância: O pico S representa a parte final da despolarização ventricular, quando a células cardíacas estão quase totalmente despolarizadas.\n",
    "9. 'ECG_T_Peaks'\n",
    "- Descrição: Refere-se ao pico T, que está associado à repolarização dos ventrículos.\n",
    "Importância: O pico T indica a recuperação das células ventriculares após a contração, preparando o coração para o próximo batimento.\n",
    "10. 'ECG_T_Onsets'\n",
    "- Descrição: Marca o início da onda T, ou seja, o início da repolarização ventricular.\n",
    "Importância: Esse ponto é importante para calcular a duração da repolarização ventricular, que pode ser usado para avaliar a saúde cardíaca.\n",
    "11. 'ECG_T_Offsets'\n",
    "- Descrição: Marca o final da onda T, indicando quando a repolarização ventricular está concluída.\n",
    "Importância: Esse ponto é útil para analisar a duração da onda T, o que pode ser importante para detectar arritmias.\n",
    "12. 'ECG_Phase_Atrial'\n",
    "- Descrição: Refere-se a uma fase atrial do ECG, provavelmente a fase de despolarização ou repolarização dos átrios.\n",
    "Importância: Pode indicar um marcador da atividade elétrica atrial, o que é útil para análise de arritmias atriais.\n",
    "13. 'ECG_Phase_Completion_Atrial'\n",
    "- Descrição: Marca o término da fase atrial, provavelmente indicando quando a atividade elétrica atrial termina.\n",
    "Importância: Esse evento pode ser útil para identificar a transição entre a atividade atrial e ventricular.\n",
    "14. 'ECG_Phase_Ventricular'\n",
    "- Descrição: Refere-se à fase ventricular do ECG, provavelmente relacionada à despolarização ou repolarização dos ventrículos.\n",
    "Importância: Essencial para a análise da atividade elétrica ventricular e para detectar possíveis problemas cardíacos.\n",
    "15. 'ECG_Phase_Completion_Ventricular'\n",
    "- Descrição: Marca o término da fase ventricular, ou seja, o fim da atividade elétrica ventricular.\n",
    "Importância: Indica a transição entre a repolarização ventricular e o início de um novo ciclo de contração atrial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lista de colunas que indicam eventos no ECG\n",
    "colunas = ['ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', \n",
    "           'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', \n",
    "           'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',\n",
    "           'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',\n",
    "           'ECG_Phase_Completion_Ventricular']\n",
    "\n",
    "\n",
    "# Criar DataFrames separados com o nome da respectiva coluna\n",
    "for coluna in colunas:\n",
    "    globals()[coluna] = pd.DataFrame({coluna: ecg_x.loc[ecg_x[coluna] == 1, 'ECG_Clean'].values})\n",
    "    \n",
    "# criando uma função     \n",
    "def tipo_de_ondas(ecg):\n",
    "    # Criar um dicionário para armazenar os DataFrames\n",
    "    tipos_ondas = {}\n",
    "\n",
    "    for coluna in colunas:\n",
    "        # Filtrar as linhas onde a coluna é igual a 1\n",
    "        tipos_ondas[coluna] = ecg.loc[ecg[coluna] == 1, 'ECG_Clean']\n",
    "\n",
    "    return tipos_ondas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "for coluna in colunas:\n",
    "    df_dict[coluna] = ecg_x.loc[ecg_x[coluna] == 1, 'ECG_Clean'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for x in colunas:\n",
    "    lista = df_dict[x]\n",
    "    \n",
    "    # Criar figura e eixos para os dois gráficos\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 linha, 2 colunas\n",
    "    \n",
    "    # Histograma com KDE\n",
    "    sns.histplot(lista, kde=True, bins=50, ax=axes[0])\n",
    "    axes[0].set_xlabel(x)\n",
    "    axes[0].set_title(f'Histograma e KDE de {x}')\n",
    "    \n",
    "    # Boxplot\n",
    "    sns.boxplot(x=lista, ax=axes[1])\n",
    "    axes[1].set_xlabel(x)\n",
    "    axes[1].set_title(f'Boxplot de {x}')\n",
    "    \n",
    "    # Mostrar gráfico\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECG_R_Peaks[ECG_R_Peaks['ECG_R_Peaks']>0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_x['ECG_Clean'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECG_T_Offsets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.displot(df_dict['ECG_T_Peaks'], kde = True, bins =50)\n",
    "ax.set(xlabel='ECG_T Offsets', ylabel='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x= ECG_Phase_Completion_Ventricular['ECG_Phase_Completion_Ventricular'])\n",
    "ax.set(xlabel='ECG_R_Offsets')\n",
    "ax.set_title('Distribuição de ECG_R_Offsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECG_Phase_Completion_Ventricular['ECG_Phase_Completion_Ventricular']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pequena amostra de 1000\n",
    "# fig = px.line(resultado_x[0:1000], x=resultado_x.index[0:1000], y = 'ECG_Clean')\n",
    "# #fig.show(renderer=\"colab\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering de Séries Temporais (não supervisionado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se o seu conjunto de dados de ECG não tem rótulos e você deseja agrupar os sinais com base em padrões semelhantes, você pode usar clustering, como o K-means ou o DBSCAN. Esses algoritmos podem ser aplicados diretamente em séries temporais, e você poderia extrair características importantes, como frequências, amplitude e padrões temporais para classificar as séries temporais.\n",
    "\n",
    "Exemplo:\n",
    "Transformação das Séries Temporais: Você pode usar métodos como a Transformada Rápida de Fourier (FFT) para extrair características do domínio da frequência, ou Análise de Componentes Principais (PCA) para reduzir a dimensionalidade.\n",
    "Aplicação de K-means ou outro algoritmo de clustering: Após a extração das características, você pode usar K-means para agrupar os sinais de ECG em diferentes clusters, representando padrões semelhantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suponha que X seja sua matriz de características extraídas de séries temporais ECG\n",
    "# Pode ser a Transformada Rápida de Fourier ou outras características extraídas\n",
    "\n",
    "ecg_clean = resultado_x['ECG_Clean']\n",
    "\n",
    "#feactures  = resultado_x.drop(['ECG_Raw','ECG_Clean','ECG_Rate','ECG_Quality','ECG_Phase_Atrial'], axis =1)\n",
    "\n",
    "# Reduzindo a dimensionalidade para visualização (usando PCA)\n",
    "#pca = PCA(n_components=4)\n",
    "#X_reduced = pca.fit_transform(feactures)  # X é um conjunto de dados com as características extraídas\n",
    "\n",
    "ecg_clean_2d = ecg_clean.values.reshape(-1, 1)  # ou ecg_clean.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# Definir a faixa de números de clusters que você deseja testar\n",
    "n_clusters_range = range(1, 11)  # Testando de 1 a 10 clusters\n",
    "inertia = []\n",
    "\n",
    "# Calcular a inércia para cada número de clusters\n",
    "for n_clusters in n_clusters_range:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(ecg_clean_2d)  \n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plotando a curva de inércia\n",
    "plt.plot(n_clusters_range, inertia, marker='o')\n",
    "plt.xlabel('Número de Clusters')\n",
    "plt.ylabel('Inércia')\n",
    "plt.title('Método do Cotovelo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora aplicando o KMeans\n",
    "kmeans = KMeans(n_clusters=7, random_state=42)\n",
    "clusters = kmeans.fit_predict(ecg_clean_2d)\n",
    "\n",
    "\n",
    "plt.scatter(range(len(ecg_clean_2d)), ecg_clean_2d, c=clusters, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vou criar um modelo de classificação que vai descobrir 3 tipos de doença do coração sendo isquemia, infarto ou necrose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A isquemia pode ser reconhecida pela presença de uma onda T apiculada (isquemia subendocárdica) ou invertida (isquemia subepicárdica) e simétrica, visto que a redução do fluxo sanguíneo irá lentificar a repolarização ventricular.\n",
    "\n",
    "- corrente de lesão do infarto pode ser identificada pela presença de um infradesnivelamento (lesão subendocárdica) ou supradesnivelamento (lesão subepicárdica) do segmento ST.\n",
    "\n",
    "- A necrose, por sua vez, por ser um evento mais tardio, é evidenciada pela presença de uma onda Q (do complexo QRS) patológica, que é caracterizada por possuir mais de 1 mm de largura e/ou ⅓ da altura do complexo QRS.\n",
    "\n",
    "-isquemia\n",
    "-corrente\n",
    "-necrose\n",
    "\n",
    "https://blog.jaleko.com.br/infarto-no-ecg-como-reconhecer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# isquemia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alterações na onda T\n",
    "\n",
    "Inversão da onda T em duas derivações contíguas com R proeminente ou R/S > 1 \n",
    "\n",
    "Onda T apiculada (isquemia subendocárdica) ou invertida (isquemia subepicárdica) e simétrica \n",
    "\n",
    "Diminuição da amplitude da onda T (ondas T planas) \n",
    "Aumento acentuado da amplitude da onda T (onda T hiperaguda) \n",
    "\n",
    "Alterações no segmento ST\n",
    "\n",
    "Infradesnivelamento do segmento ST ≥ 0,5 mm (0,05 mV) em duas ou mais derivações contíguas \n",
    "\n",
    "Supradesnivelamento do segmento ST transitório \n",
    "Elevação ou depressão do segmento ST (em relação ao segmento PR) \n",
    "\n",
    "A isquemia afeta a fase de platô e a fase de repolarização rápida, o que causa alterações no segmento ST e na onda T. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ondas_x = tipo_de_ondas(ecg_x)\n",
    "ondas_y = tipo_de_ondas(ecg_y)\n",
    "ondas_z = tipo_de_ondas(ecg_z)\n",
    "\n",
    "\n",
    "ecgx_T_Peaks = ondas_x['ECG_T_Peaks']\n",
    "ecgy_T_Peaks = ondas_y['ECG_T_Peaks']\n",
    "ecgz_T_Peaks = ondas_z['ECG_T_Peaks']\n",
    "\n",
    "df_ecgx_T_Peaks = pd.DataFrame(ecgx_T_Peaks)\n",
    "df_ecgy_T_Peaks = pd.DataFrame(ecgy_T_Peaks)\n",
    "df_ecgz_T_Peaks = pd.DataFrame(ecgz_T_Peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ondas_T_Peaks = pd.DataFrame({\n",
    "    'x_T_Peaks': ecgx_T_Peaks,\n",
    "    'y_T_Peaks': ecgy_T_Peaks,\n",
    "    'z_T_Peaks': ecgz_T_Peaks\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ondas_T_Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecgx_T_Peaks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.ecdf(df_ecgx_T_Peaks, x=\"Picos da onda T do ecg x\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.ecdf(df_ecgy_T_Peaks, x=\"Picos da onda T do ecg y\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.ecdf(df_ecgz_T_Peaks, x=\"Picos da onda T do ecg y\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.boxplot(data=ondas_T_Peaks[['x_T_Peaks', 'y_T_Peaks', 'z_T_Peaks']])\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.title('Boxplot das Ondas T (x, y, z)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=ondas_T_Peaks[['x_T_Peaks', 'z_T_Peaks']])\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.title('Boxplot das Ondas T (x, z)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irei testar Redes neurais com valores binarios do Neurokit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_csv = r'D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_csv_info_definitions\\ubject-info_limpo.csv'\n",
    "\n",
    "dados = pd.read_csv(link_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Follow-up period from enrollment (days)</th>\n",
       "      <th>days_4years</th>\n",
       "      <th>Exit of the study</th>\n",
       "      <th>Cause of death</th>\n",
       "      <th>SCD_4years SinusRhythm</th>\n",
       "      <th>HF_4years SinusRhythm</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender (male=1)</th>\n",
       "      <th>Weight (kg)</th>\n",
       "      <th>...</th>\n",
       "      <th>Angiotensin-II receptor blocker (yes=1)</th>\n",
       "      <th>Anticoagulants/antitrombotics  (yes=1)</th>\n",
       "      <th>Betablockers (yes=1)</th>\n",
       "      <th>Digoxin (yes=1)</th>\n",
       "      <th>Loop diuretics (yes=1)</th>\n",
       "      <th>Spironolactone (yes=1)</th>\n",
       "      <th>Statins (yes=1)</th>\n",
       "      <th>Hidralazina (yes=1)</th>\n",
       "      <th>ACE inhibitor (yes=1)</th>\n",
       "      <th>Nitrovasodilator (yes=1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0001</td>\n",
       "      <td>2065</td>\n",
       "      <td>1460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0002</td>\n",
       "      <td>2045</td>\n",
       "      <td>1460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0003</td>\n",
       "      <td>2044</td>\n",
       "      <td>1460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P0004</td>\n",
       "      <td>2044</td>\n",
       "      <td>1460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P0005</td>\n",
       "      <td>2043</td>\n",
       "      <td>1460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Patient ID  Follow-up period from enrollment (days)  days_4years  \\\n",
       "0      P0001                                     2065         1460   \n",
       "1      P0002                                     2045         1460   \n",
       "2      P0003                                     2044         1460   \n",
       "3      P0004                                     2044         1460   \n",
       "4      P0005                                     2043         1460   \n",
       "\n",
       "   Exit of the study  Cause of death  SCD_4years SinusRhythm  \\\n",
       "0                0.0               0                       0   \n",
       "1                0.0               0                       0   \n",
       "2                0.0               0                       0   \n",
       "3                0.0               0                       0   \n",
       "4                0.0               0                       0   \n",
       "\n",
       "   HF_4years SinusRhythm  Age  Gender (male=1)  Weight (kg)  ...  \\\n",
       "0                      0   58                1           83  ...   \n",
       "1                      0   58                1           74  ...   \n",
       "2                      0   69                1           83  ...   \n",
       "3                      0   56                0           84  ...   \n",
       "4                      0   70                1           97  ...   \n",
       "\n",
       "   Angiotensin-II receptor blocker (yes=1)  \\\n",
       "0                                        0   \n",
       "1                                        1   \n",
       "2                                        1   \n",
       "3                                        1   \n",
       "4                                        0   \n",
       "\n",
       "   Anticoagulants/antitrombotics  (yes=1)  Betablockers (yes=1)  \\\n",
       "0                                       1                     1   \n",
       "1                                       1                     1   \n",
       "2                                       1                     1   \n",
       "3                                       1                     1   \n",
       "4                                       1                     1   \n",
       "\n",
       "   Digoxin (yes=1)  Loop diuretics (yes=1)  Spironolactone (yes=1)  \\\n",
       "0                1                       1                       0   \n",
       "1                0                       0                       0   \n",
       "2                1                       1                       0   \n",
       "3                0                       1                       1   \n",
       "4                0                       1                       0   \n",
       "\n",
       "   Statins (yes=1)  Hidralazina (yes=1)  ACE inhibitor (yes=1)  \\\n",
       "0                0                    0                      1   \n",
       "1                1                    0                      0   \n",
       "2                0                    0                      0   \n",
       "3                0                    0                      0   \n",
       "4                1                    0                      1   \n",
       "\n",
       "   Nitrovasodilator (yes=1)  \n",
       "0                         0  \n",
       "1                         0  \n",
       "2                         0  \n",
       "3                         0  \n",
       "4                         1  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe 0: 726 -> 73.19%\n",
      "Classe 6: 100 -> 10.08%\n",
      "Classe 3: 94 -> 9.48%\n",
      "Classe 1: 61 -> 6.15%\n",
      "Classe 7: 11 -> 1.11%\n"
     ]
    }
   ],
   "source": [
    "# Contagem de classes no conjunto de dados\n",
    "class_counts = dados['Cause of death'].value_counts()\n",
    "\n",
    "\n",
    "# Contagem de classes no conjunto de dados\n",
    "for class_label, count in class_counts.items():\n",
    "    print(f'Classe {class_label}: {count} -> {count/dados.shape[0]*100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de colunas: 105\n",
      "Quantidade de linhas: 992\n"
     ]
    }
   ],
   "source": [
    "print(f'Quantidade de colunas: {dados.shape[1]}')\n",
    "print(f'Quantidade de linhas: {dados.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 3., 1., 2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(dados['Exit of the study'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arquivos_path_holter\n",
    "\n",
    "lista_sem_duplicatas = pd.Series(arquivos).unique().tolist() # removando duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista_sem_duplicatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sem o filtro de tipo de mortes (687, 3)\n",
      "Com o filtro de tipo de mortes (636, 3)\n",
      "Classe 0: 484 -> 76.10%\n",
      "Classe 6: 74 -> 11.64%\n",
      "Classe 3: 70 -> 11.01%\n",
      "Classe 7: 8 -> 1.26%\n"
     ]
    }
   ],
   "source": [
    "# Criando um DataFrame dos endereços\n",
    "df_enderecos = pd.DataFrame({\n",
    "    \"Endereço\": lista_sem_duplicatas\n",
    "})\n",
    "\n",
    "# Extraindo o Patient ID do final do endereço\n",
    "df_enderecos[\"Patient ID\"] = df_enderecos[\"Endereço\"].apply(lambda x: x.split(\"\\\\\")[-1])\n",
    "\n",
    "\n",
    "# Removendo o sufixo '_H' da coluna 'Patient ID'\n",
    "df_enderecos['Patient ID'] = df_enderecos['Patient ID'].str.replace('_H', '', regex=False)\n",
    "\n",
    "# Aqui contem dados de duas colunas a primeira coluna vou usar como chave id para conectar os pacientes de cada scg e a segunda o tipo de morte.\n",
    "novos_dados = dados[['Patient ID','Cause of death']]\n",
    "\n",
    "# Fazendo a fusão (merge) dos DataFrames com base no \"Patient ID\"\n",
    "df_final = df_enderecos.merge(novos_dados, on=\"Patient ID\", how=\"left\")\n",
    "print(f'Sem o filtro de tipo de mortes {df_final.shape}')\n",
    "\n",
    "df_final = df_final[~df_final['Cause of death'].isin([1])] # tirando valores que tem 1 pois significa mortes não identificada ou seja ruídos,\n",
    "\n",
    "print(f'Com o filtro de tipo de mortes {df_final.shape}')\n",
    "\n",
    "# Contagem de classes no conjunto de dados\n",
    "class_counts = df_final['Cause of death'].value_counts()\n",
    "\n",
    "\n",
    "# Contagem de classes no conjunto de dados\n",
    "for class_label, count in class_counts.items():\n",
    "    print(f'Classe {class_label}: {count} -> {count/df_final.shape[0]*100:.2f}%')\n",
    "    \n",
    "    \n",
    "# Resetando o índice para garantir que o loop use os índices corretos\n",
    "df_final.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lista para armazenar informações de cada paciente\n",
    "# dados_pacientes = []\n",
    "\n",
    "# # Loop para processar cada arquivo e pegar o tamanho do sinal\n",
    "# for idx, arquivo in enumerate(arquivos):\n",
    "#     try:\n",
    "#         # Lendo o arquivo do ECG\n",
    "#         record = wfdb.rdrecord(arquivo)\n",
    "#         p_signal = record.p_signal\n",
    "#         sinal_x = p_signal[:, 0]\n",
    "        \n",
    "#         # Armazenando as informações no dicionário\n",
    "#         dados_pacientes.append({\n",
    "#             \"Paciente\": arquivo.split(\"\\\\\")[-1],  # Extrair o nome do paciente\n",
    "#             \"Quantidade de amostras\": len(sinal_x)\n",
    "#         })\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro ao processar {arquivo}: {e}\")\n",
    "\n",
    "# # Convertendo a lista para um DataFrame\n",
    "# df_pacientes = pd.DataFrame(dados_pacientes)\n",
    "\n",
    "# # Exibindo as primeiras linhas do DataFrame\n",
    "# print(df_pacientes)\n",
    "\n",
    "# df_pacientes['Quantidade de amostras'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrv_time = nk.hrv_time(peaks, sampling_rate=100, show=True)\n",
    "# hrv_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculos para diferentes durações de sinal ECG\n",
    "\n",
    "### 1. Para 5 minutos de sinal:\n",
    "1. **Número de segundos em 5 minutos**:  \n",
    "   5 minutos = 5 * 60 = 300 segundos.\n",
    "\n",
    "2. **Número de amostras em 5 minutos**:  \n",
    "   Como há 1.000 amostras por segundo, em 300 segundos teremos:\n",
    "   \\[\n",
    "   \\text{Amostras} = 1.000 \\times 300 = 300.000 \\, \\text{amostras}.\n",
    "   \\]\n",
    "\n",
    "Portanto, **5 minutos de sinal** corresponderiam a **300.000 amostras**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Para 10 minutos de sinal:\n",
    "1. **Número de segundos em 10 minutos**:  \n",
    "   10 minutos = 10 * 60 = 600 segundos.\n",
    "\n",
    "2. **Número de amostras em 10 minutos**:  \n",
    "   Como há 1.000 amostras por segundo, em 600 segundos teremos:\n",
    "   \\[\n",
    "   \\text{Amostras} = 1.000 \\times 600 = 600.000 \\, \\text{amostras}.\n",
    "   \\]\n",
    "\n",
    "Portanto, **10 minutos de sinal** corresponderiam a **600.000 amostras**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Para 15 minutos de sinal:\n",
    "1. **Número de segundos em 15 minutos**:  \n",
    "   15 minutos = 15 * 60 = 900 segundos.\n",
    "\n",
    "2. **Número de amostras em 15 minutos**:  \n",
    "   Como há 1.000 amostras por segundo, em 900 segundos teremos:\n",
    "   \\[\n",
    "   \\text{Amostras} = 1.000 \\times 900 = 900.000 \\, \\text{amostras}.\n",
    "   \\]\n",
    "\n",
    "Portanto, **15 minutos de sinal** corresponderiam a **900.000 amostras**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Para 20 minutos de sinal:\n",
    "1. **Número de segundos em 20 minutos**:  \n",
    "   20 minutos = 20 * 60 = 1.200 segundos.\n",
    "\n",
    "2. **Número de amostras em 20 minutos**:  \n",
    "   Como há 1.000 amostras por segundo, em 1.200 segundos teremos:\n",
    "   \\[\n",
    "   \\text{Amostras} = 1.000 \\times 1.200 = 1.200.000 \\, \\text{amostras}.\n",
    "   \\]\n",
    "\n",
    "Portanto, **20 minutos de sinal** corresponderiam a **1.200.000 amostras**.\n",
    "\n",
    "---\n",
    "Vou desconsiderar por enquanto primeiro vou usar o ECG com resolução otima, ela tem 20 minutos no total\n",
    "### 5. Para 25 minutos de sinal:\n",
    "1. **Número de segundos em 25 minutos**:  \n",
    "   25 minutos = 25 * 60 = 1.500 segundos.\n",
    "\n",
    "2. **Número de amostras em 25 minutos**:  \n",
    "   Como há 1.000 amostras por segundo, em 1.500 segundos teremos:\n",
    "   \\[\n",
    "   \\text{Amostras} = 1.000 \\times 1.500 = 1.500.000 \\, \\text{amostras}.\n",
    "   \\]\n",
    "\n",
    "Portanto, **25 minutos de sinal** corresponderiam a **1.500.000 amostras**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import wfdb\n",
    "# import neurokit2 as nk\n",
    "\n",
    "# inicio_geral = time.time()\n",
    "\n",
    "# # Resetando o índice para garantir que o loop use os índices corretos\n",
    "# df_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# inicio_amostra = 100  # Começar a partir da amostra 100 (reduzir ruídos iniciais)\n",
    "# duracao_em_amostras = 300000  # 5 minutos em amostras\n",
    "\n",
    "# # Lista de colunas que indicam eventos no ECG\n",
    "# colunas = [\n",
    "#     'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', \n",
    "#     'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', \n",
    "#     'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',\n",
    "#     'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',\n",
    "#     'ECG_Phase_Completion_Ventricular'\n",
    "# ]\n",
    "\n",
    "# # Função para calcular estatísticas\n",
    "# def calcular_estatisticas(ondas):\n",
    "#     if ondas.empty:  # Se não houver valores, retorna estatísticas zeradas\n",
    "#         return {stat: 0 for stat in [\"mean\", \"std\", \"max\", \"min\", \"25%\", \"50%\", \"75%\"]}\n",
    "    \n",
    "#     estatisticas = {\n",
    "#         \"mean\": ondas.mean(),\n",
    "#         \"std\": ondas.std(),\n",
    "#         \"max\": ondas.max(),\n",
    "#         \"min\": ondas.min(),\n",
    "#         \"25%\": ondas.quantile(0.25),\n",
    "#         \"50%\": ondas.quantile(0.50),\n",
    "#         \"75%\": ondas.quantile(0.75)\n",
    "#     }\n",
    "    \n",
    "#     return estatisticas\n",
    "\n",
    "# # Função para processar os dados\n",
    "# def processar_ecg_com_estatisticas(df_final):\n",
    "#     dados_processados = []\n",
    "\n",
    "#     # Iniciando a contagem de tempo\n",
    "#     inicio = time.time()\n",
    "\n",
    "#     for idx, endereco in enumerate(df_final[\"Endereço\"]):\n",
    "#         try:\n",
    "#             inicio_paciente = time.time()\n",
    "#             print(f\"✅ Iniciando processamento do paciente {idx+1}/{len(df_final)}...\")\n",
    "\n",
    "#             # Lendo o sinal do ECG\n",
    "#             print(f\"   Lendo arquivo {endereco}...\")\n",
    "#             record = wfdb.rdrecord(endereco)\n",
    "#             sinal_x = record.p_signal[:, 0]\n",
    "\n",
    "#             # Filtrando o sinal ECG para considerar apenas os dados relevantes\n",
    "#             sinal_filtrado = sinal_x[inicio_amostra: inicio_amostra + duracao_em_amostras]\n",
    "\n",
    "#             # Processando o ECG com o Neurokit (pegando só as colunas necessárias)\n",
    "#             print(f\"   Processando ECG com Neurokit...\")\n",
    "#             ecg_x, _ = nk.ecg_process(sinal_filtrado, sampling_rate=record.fs, method='neurokit')\n",
    "\n",
    "#             # Criando um dicionário para armazenar as estatísticas das ondas\n",
    "#             estatisticas_paciente = {}\n",
    "\n",
    "#             for coluna in colunas:\n",
    "#                 if coluna not in ecg_x.columns:\n",
    "#                     print(f\"⚠️ Coluna {coluna} não encontrada. Pulando...\")\n",
    "#                     continue  # Pula essa etapa se a coluna não existir\n",
    "\n",
    "#                 print(f\"   Processando coluna: {coluna}...\")\n",
    "#                 ondas = ecg_x.loc[ecg_x[coluna] == 1, 'ECG_Clean']\n",
    "\n",
    "#                 # Calculando as estatísticas para essa onda\n",
    "#                 estatisticas = calcular_estatisticas(ondas)\n",
    "#                 for stat, valor in estatisticas.items():\n",
    "#                     estatisticas_paciente[f\"{coluna}_{stat}\"] = valor\n",
    "\n",
    "#             # Adicionando o target do paciente\n",
    "#             estatisticas_paciente[\"target\"] = df_final.loc[idx, \"Cause of death\"]\n",
    "\n",
    "#             # Adicionando os resultados à lista\n",
    "#             dados_processados.append(estatisticas_paciente)\n",
    "\n",
    "#             # Liberando a memória de ecg_x após o uso\n",
    "#             del ecg_x\n",
    "\n",
    "#             print(f\"✅ Paciente {idx+1}/{len(df_final)} processado: {endereco}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Erro ao processar {endereco}: {e}\")\n",
    "#             continue  # Continua com o próximo paciente, sem interromper o código\n",
    "\n",
    "#     # Convertendo a lista de dicionários em um DataFrame final\n",
    "#     df_estatisticas_final = pd.DataFrame(dados_processados)\n",
    "\n",
    "#     # Substituindo todos os NaN por zero (caso ainda haja algum)\n",
    "#     df_estatisticas_final.fillna(0, inplace=True)\n",
    "\n",
    "#     # Finalizando a contagem de tempo\n",
    "#     fim = time.time()\n",
    "#     tempo_execucao = fim - inicio\n",
    "\n",
    "#     # Exibindo o tempo total de execução\n",
    "#     print(f\"Tempo total de execução: {tempo_execucao:.2f} segundos\")\n",
    "\n",
    "#     return df_estatisticas_final\n",
    "\n",
    "# # Exemplo de uso\n",
    "# df_estatisticas = processar_ecg_com_estatisticas(df_final)\n",
    "\n",
    "# # Exibindo as primeiras linhas do DataFrame final\n",
    "# #print(df_estatisticas.head())\n",
    "\n",
    "\n",
    "# # salvando o dataframe\n",
    "# df_estatisticas.to_csv(\"D:/Projeto_Tese_mestrado/02_Dataset/dados_ECG/dados_ecg.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando o dataframe\n",
    "#df_ecg_final.to_csv(\"dados_ecg.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_estatisticas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_estatisticas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tratamento_dados(dados, filtro):\n",
    "    \"\"\"\n",
    "    Função para tratar os dados, filtrando colunas específicas e realizando pré-processamento.\n",
    "\n",
    "    Parâmetros:\n",
    "    - dados (DataFrame): O dataset a ser tratado.\n",
    "    - filtro (str): Palavra-chave para selecionar as colunas (default: \"ECG_R_Peaks\").\n",
    "\n",
    "    Retorna:\n",
    "    - X_train_scaled, X_test_scaled, y_train, y_test\n",
    "    \"\"\"\n",
    "\n",
    "    # Filtrar colunas que contêm a palavra-chave no nome\n",
    "    colunas_filtradas = dados.filter(like=filtro).columns\n",
    "    X = dados[colunas_filtradas].values  # Apenas colunas filtradas\n",
    "\n",
    "    # Garantir que o target seja separado corretamente\n",
    "    y = dados['target'].values  \n",
    "\n",
    "    # Transformar os rótulos usando LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Verificar o mapeamento dos rótulos\n",
    "    print(\"Rótulos originais:\", np.unique(y))\n",
    "    print(\"Rótulos transformados:\", np.unique(y_encoded))\n",
    "\n",
    "    # Dividir em treino e teste (80% treino, 20% teste)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n",
    "\n",
    "    # Normalizar os dados\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Converter o target para one-hot encoding se for multiclasse\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentando a Complexidade da Rede (Arquitetura)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentando o Dropout de 0.3 para 0.5\n",
    "\n",
    "Adicionando Regularização L2 (Penalização de Pesos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adotar a função de Ativação Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Reduzindo a Taxa de Aprendizado com ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "def construir_rede_neural(X_train, X_test, y_train, y_test,input_dim):\n",
    "    # Criar modelo poderoso\n",
    "    def criar_rede_neural(input_dim, num_classes):\n",
    "        model = keras.Sequential([\n",
    "        keras.Input(shape=(input_dim,)),  # Corrigindo a entrada\n",
    "        Dense(512),\n",
    "        LeakyReLU(negative_slope=0.1),  # Corrigindo o parâmetro alpha\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(256),\n",
    "        LeakyReLU(negative_slope=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(128),\n",
    "        LeakyReLU(negative_slope=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(64),\n",
    "        LeakyReLU(negative_slope=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "        # Compilar modelo\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),  # Adam já é otimizado para deep learning\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # Criar callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10)\n",
    "\n",
    "\n",
    "    # Criar e treinar o modelo\n",
    "    modelo = criar_rede_neural(input_dim=input_dim, num_classes=4)\n",
    "    history = modelo.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=0  # Define para 0 para ocultar a saída\n",
    "        #verbose=2  # Apenas mostra os valores das métricas\n",
    "    )\n",
    "\n",
    "    # Avaliação\n",
    "    loss, acc = modelo.evaluate(X_test, y_test)\n",
    "    print(f\"\\nAcurácia no conjunto de validação: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "link =r'D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ECG\\dados_ecg_x.csv'\n",
    "df_x = pd.read_csv(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ECG_R_Peaks_mean</th>\n",
       "      <th>ECG_R_Peaks_std</th>\n",
       "      <th>ECG_R_Peaks_max</th>\n",
       "      <th>ECG_R_Peaks_min</th>\n",
       "      <th>ECG_R_Peaks_25%</th>\n",
       "      <th>ECG_R_Peaks_50%</th>\n",
       "      <th>ECG_R_Peaks_75%</th>\n",
       "      <th>ECG_P_Peaks_mean</th>\n",
       "      <th>ECG_P_Peaks_std</th>\n",
       "      <th>ECG_P_Peaks_max</th>\n",
       "      <th>...</th>\n",
       "      <th>ECG_Phase_Ventricular_50%</th>\n",
       "      <th>ECG_Phase_Ventricular_75%</th>\n",
       "      <th>ECG_Phase_Completion_Ventricular_mean</th>\n",
       "      <th>ECG_Phase_Completion_Ventricular_std</th>\n",
       "      <th>ECG_Phase_Completion_Ventricular_max</th>\n",
       "      <th>ECG_Phase_Completion_Ventricular_min</th>\n",
       "      <th>ECG_Phase_Completion_Ventricular_25%</th>\n",
       "      <th>ECG_Phase_Completion_Ventricular_50%</th>\n",
       "      <th>ECG_Phase_Completion_Ventricular_75%</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.241630</td>\n",
       "      <td>0.082193</td>\n",
       "      <td>0.322544</td>\n",
       "      <td>-0.091673</td>\n",
       "      <td>0.250071</td>\n",
       "      <td>0.267336</td>\n",
       "      <td>0.278057</td>\n",
       "      <td>0.004551</td>\n",
       "      <td>0.048138</td>\n",
       "      <td>0.600674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019374</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.103692</td>\n",
       "      <td>0.149076</td>\n",
       "      <td>0.322389</td>\n",
       "      <td>-0.090233</td>\n",
       "      <td>-0.033648</td>\n",
       "      <td>-0.003640</td>\n",
       "      <td>0.267132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.887003</td>\n",
       "      <td>0.047842</td>\n",
       "      <td>1.012329</td>\n",
       "      <td>0.581168</td>\n",
       "      <td>0.855531</td>\n",
       "      <td>0.878867</td>\n",
       "      <td>0.919580</td>\n",
       "      <td>0.018456</td>\n",
       "      <td>0.007530</td>\n",
       "      <td>0.038518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014906</td>\n",
       "      <td>0.126137</td>\n",
       "      <td>0.430159</td>\n",
       "      <td>0.454926</td>\n",
       "      <td>1.009548</td>\n",
       "      <td>-0.047024</td>\n",
       "      <td>-0.022637</td>\n",
       "      <td>0.109724</td>\n",
       "      <td>0.875895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.934128</td>\n",
       "      <td>0.200314</td>\n",
       "      <td>1.315409</td>\n",
       "      <td>-0.121479</td>\n",
       "      <td>0.903386</td>\n",
       "      <td>0.964950</td>\n",
       "      <td>1.032918</td>\n",
       "      <td>-0.020112</td>\n",
       "      <td>0.105865</td>\n",
       "      <td>1.166618</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042933</td>\n",
       "      <td>0.030048</td>\n",
       "      <td>0.445295</td>\n",
       "      <td>0.506371</td>\n",
       "      <td>1.307368</td>\n",
       "      <td>-0.133698</td>\n",
       "      <td>-0.040524</td>\n",
       "      <td>0.020390</td>\n",
       "      <td>0.961410</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.353488</td>\n",
       "      <td>0.055687</td>\n",
       "      <td>0.534015</td>\n",
       "      <td>0.165861</td>\n",
       "      <td>0.312032</td>\n",
       "      <td>0.351159</td>\n",
       "      <td>0.392537</td>\n",
       "      <td>0.021540</td>\n",
       "      <td>0.007793</td>\n",
       "      <td>0.064705</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039988</td>\n",
       "      <td>0.043105</td>\n",
       "      <td>0.162217</td>\n",
       "      <td>0.194831</td>\n",
       "      <td>0.533228</td>\n",
       "      <td>-0.087780</td>\n",
       "      <td>-0.021943</td>\n",
       "      <td>0.018429</td>\n",
       "      <td>0.350440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.500880</td>\n",
       "      <td>0.118902</td>\n",
       "      <td>1.624561</td>\n",
       "      <td>0.017130</td>\n",
       "      <td>1.478393</td>\n",
       "      <td>1.507362</td>\n",
       "      <td>1.548657</td>\n",
       "      <td>0.055187</td>\n",
       "      <td>0.009338</td>\n",
       "      <td>0.080235</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048259</td>\n",
       "      <td>-0.019217</td>\n",
       "      <td>0.750655</td>\n",
       "      <td>0.794030</td>\n",
       "      <td>1.624331</td>\n",
       "      <td>-0.075072</td>\n",
       "      <td>-0.040743</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>1.550994</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ECG_R_Peaks_mean  ECG_R_Peaks_std  ECG_R_Peaks_max  ECG_R_Peaks_min  \\\n",
       "0          0.241630         0.082193         0.322544        -0.091673   \n",
       "1          0.887003         0.047842         1.012329         0.581168   \n",
       "2          0.934128         0.200314         1.315409        -0.121479   \n",
       "3          0.353488         0.055687         0.534015         0.165861   \n",
       "4          1.500880         0.118902         1.624561         0.017130   \n",
       "\n",
       "   ECG_R_Peaks_25%  ECG_R_Peaks_50%  ECG_R_Peaks_75%  ECG_P_Peaks_mean  \\\n",
       "0         0.250071         0.267336         0.278057          0.004551   \n",
       "1         0.855531         0.878867         0.919580          0.018456   \n",
       "2         0.903386         0.964950         1.032918         -0.020112   \n",
       "3         0.312032         0.351159         0.392537          0.021540   \n",
       "4         1.478393         1.507362         1.548657          0.055187   \n",
       "\n",
       "   ECG_P_Peaks_std  ECG_P_Peaks_max  ...  ECG_Phase_Ventricular_50%  \\\n",
       "0         0.048138         0.600674  ...                  -0.019374   \n",
       "1         0.007530         0.038518  ...                   0.014906   \n",
       "2         0.105865         1.166618  ...                  -0.042933   \n",
       "3         0.007793         0.064705  ...                  -0.039988   \n",
       "4         0.009338         0.080235  ...                  -0.048259   \n",
       "\n",
       "   ECG_Phase_Ventricular_75%  ECG_Phase_Completion_Ventricular_mean  \\\n",
       "0                   0.002296                               0.103692   \n",
       "1                   0.126137                               0.430159   \n",
       "2                   0.030048                               0.445295   \n",
       "3                   0.043105                               0.162217   \n",
       "4                  -0.019217                               0.750655   \n",
       "\n",
       "   ECG_Phase_Completion_Ventricular_std  ECG_Phase_Completion_Ventricular_max  \\\n",
       "0                              0.149076                              0.322389   \n",
       "1                              0.454926                              1.009548   \n",
       "2                              0.506371                              1.307368   \n",
       "3                              0.194831                              0.533228   \n",
       "4                              0.794030                              1.624331   \n",
       "\n",
       "   ECG_Phase_Completion_Ventricular_min  ECG_Phase_Completion_Ventricular_25%  \\\n",
       "0                             -0.090233                             -0.033648   \n",
       "1                             -0.047024                             -0.022637   \n",
       "2                             -0.133698                             -0.040524   \n",
       "3                             -0.087780                             -0.021943   \n",
       "4                             -0.075072                             -0.040743   \n",
       "\n",
       "   ECG_Phase_Completion_Ventricular_50%  ECG_Phase_Completion_Ventricular_75%  \\\n",
       "0                             -0.003640                              0.267132   \n",
       "1                              0.109724                              0.875895   \n",
       "2                              0.020390                              0.961410   \n",
       "3                              0.018429                              0.350440   \n",
       "4                              0.732800                              1.550994   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando ECG_R_Peaks\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7534 - loss: 0.8363\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7638\n",
      "Finalizando o processo\n",
      "====================================================================================================\n",
      "Executando ECG_P_Peaks\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7534 - loss: 0.7803\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7638\n",
      "Finalizando o processo\n",
      "====================================================================================================\n",
      "Executando ECG_P_Onsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7534 - loss: 0.7541\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7638\n",
      "Finalizando o processo\n",
      "====================================================================================================\n",
      "Executando ECG_P_Offsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7534 - loss: 0.7558\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7638\n",
      "Finalizando o processo\n",
      "====================================================================================================\n",
      "Executando ECG_Q_Peaks\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7482 - loss: 0.7491 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7559\n",
      "Finalizando o processo\n",
      "====================================================================================================\n",
      "Executando ECG_R_Onsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7534 - loss: 0.7671\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7638\n",
      "Finalizando o processo\n",
      "====================================================================================================\n",
      "Executando ECG_R_Offsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7534 - loss: 0.7876\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7638\n",
      "Finalizando o processo\n",
      "====================================================================================================\n",
      "Executando ECG_S_Peaks\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7534 - loss: 0.7756\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7638\n",
      "Finalizando o processo\n",
      "====================================================================================================\n",
      "Executando ECG_T_Peaks\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7534 - loss: 0.7939 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7638\n",
      "Finalizando o processo\n",
      "====================================================================================================\n",
      "Executando ECG_T_Onsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7534 - loss: 0.8297\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7638\n",
      "Finalizando o processo\n",
      "====================================================================================================\n",
      "Executando ECG_T_Offsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7534 - loss: 0.7755\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7638\n",
      "Finalizando o processo\n",
      "====================================================================================================\n",
      "Executando ECG_Phase_Atrial\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7534 - loss: 0.7905\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7638\n",
      "Finalizando o processo\n",
      "====================================================================================================\n",
      "Executando ECG_Phase_Completion_Atrial\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7587 - loss: 0.8216 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7717\n",
      "Finalizando o processo\n",
      "====================================================================================================\n",
      "Executando ECG_Phase_Ventricular\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7534 - loss: 0.8021\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7638\n",
      "Finalizando o processo\n",
      "====================================================================================================\n",
      "Executando ECG_Phase_Completion_Ventricular\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7534 - loss: 0.7687\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7638\n",
      "Finalizando o processo\n",
      "====================================================================================================\n",
      "CPU times: total: 4min 47s\n",
      "Wall time: 3min 40s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "colunas = [\n",
    "    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', \n",
    "    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', \n",
    "    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',\n",
    "    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',\n",
    "    'ECG_Phase_Completion_Ventricular'\n",
    "]\n",
    "\n",
    "for col in colunas:\n",
    "    print(f'Executando {col}')\n",
    "    \n",
    "    start_time = time.time()  # Inicia a contagem do tempo\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = tratamento_dados(df_x, filtro=col)\n",
    "    construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])\n",
    "    \n",
    "    end_time = time.time()  # Finaliza a contagem do tempo\n",
    "    elapsed_time = end_time - start_time  # Calcula o tempo decorrido\n",
    "    \n",
    "    print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')\n",
    "    print('=' * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vou testar usando o y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import duckdb\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import wfdb\n",
    "# import neurokit2 as nk\n",
    "\n",
    "# # Conectar ao banco de dados DuckDB\n",
    "# conn = duckdb.connect(\"D:/Projeto_Tese_mestrado/02_Dataset/Duckedb/banco_ecg.duckdb\")\n",
    "\n",
    "# # Carregar apenas a lista de pacientes\n",
    "# pacientes = conn.execute(\"SELECT DISTINCT id_paciente FROM ecg_pacientes ORDER BY id_paciente\").fetchdf()\n",
    "\n",
    "# # Lista de colunas que indicam eventos no ECG\n",
    "# colunas = [\n",
    "#     'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', \n",
    "#     'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', \n",
    "#     'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',\n",
    "#     'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',\n",
    "#     'ECG_Phase_Completion_Ventricular'\n",
    "# ]\n",
    "\n",
    "# # Função para calcular estatísticas\n",
    "# def calcular_estatisticas(ondas):\n",
    "#     if ondas.empty:  \n",
    "#         return {stat: 0 for stat in [\"mean\", \"std\", \"max\", \"min\", \"25%\", \"50%\", \"75%\"]}\n",
    "    \n",
    "#     # Substituir valores NaN por zero antes de calcular as estatísticas\n",
    "#     ondas = ondas.fillna(0)\n",
    "    \n",
    "#     estatisticas = {\n",
    "#         \"mean\": ondas.mean(),\n",
    "#         \"std\": ondas.std(),\n",
    "#         \"max\": ondas.max(),\n",
    "#         \"min\": ondas.min(),\n",
    "#         \"25%\": ondas.quantile(0.25),\n",
    "#         \"50%\": ondas.quantile(0.50),\n",
    "#         \"75%\": ondas.quantile(0.75)\n",
    "#     }\n",
    "    \n",
    "#     return estatisticas\n",
    "\n",
    "# # Processar paciente por paciente\n",
    "# dados_processados = []\n",
    "# inicio_geral = time.time()\n",
    "\n",
    "# for idx, paciente in enumerate(pacientes[\"id_paciente\"]):\n",
    "#     try:\n",
    "#         inicio_paciente = time.time()\n",
    "#         print(f\"✅ Processando paciente {idx+1}/{len(pacientes)}: {paciente}\")\n",
    "\n",
    "#         # Carregar os dados do paciente diretamente do banco\n",
    "#         df_paciente = conn.execute(f\"\"\"\n",
    "#             SELECT sinal_y FROM ecg_pacientes\n",
    "#             WHERE id_paciente = '{paciente}'\n",
    "#         \"\"\").fetchdf()\n",
    "\n",
    "#         # Se não houver dados, pula o paciente\n",
    "#         if df_paciente.empty:\n",
    "#             print(f\"⚠️ Nenhum dado encontrado para o paciente {paciente}. Pulando...\")\n",
    "#             continue\n",
    "\n",
    "\n",
    "#         sinal_y = df_paciente[\"sinal_y\"].values\n",
    "\n",
    "\n",
    "#         # Processar com Neurokit2\n",
    "#         ecg_y, _ = nk.ecg_process(sinal_y, sampling_rate=1000, method='neurokit')  # Ajuste a taxa de amostragem se necessário\n",
    "\n",
    "#         # Criar dicionário para armazenar estatísticas\n",
    "#         estatisticas_paciente = {}\n",
    "\n",
    "#         for coluna in colunas:\n",
    "#             if coluna not in ecg_y.columns:\n",
    "#                 print(f\"⚠️ Coluna {coluna} não encontrada. Pulando...\")\n",
    "#                 continue \n",
    "\n",
    "#             ondas = ecg_y.loc[ecg_y[coluna] == 1, 'ECG_Clean']\n",
    "#             estatisticas = calcular_estatisticas(ondas)\n",
    "\n",
    "#             for stat, valor in estatisticas.items():\n",
    "#                 estatisticas_paciente[f\"{coluna}_{stat}\"] = valor\n",
    "\n",
    "#         # Adicionando o target do paciente\n",
    "#         estatisticas_paciente[\"target\"] = df_final.loc[idx, \"Cause of death\"]\n",
    "\n",
    "#         # Adicionar ID do paciente\n",
    "#         estatisticas_paciente[\"id_paciente\"] = paciente\n",
    "\n",
    "#         # Salvar os dados processados\n",
    "#         dados_processados.append(estatisticas_paciente)\n",
    "        \n",
    "\n",
    "\n",
    "#         # Liberar memória\n",
    "#         del sinal_y, ecg_y, df_paciente\n",
    "\n",
    "#         print(f\"✅ Paciente {paciente} processado com sucesso!\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Erro ao processar paciente {paciente}: {e}\")\n",
    "#         continue\n",
    "\n",
    "# # Fechar a conexão com o banco\n",
    "# conn.close()\n",
    "\n",
    "# # Criar DataFrame com os resultados\n",
    "# df_estatisticas_final = pd.DataFrame(dados_processados)\n",
    "# df_estatisticas_final.fillna(0, inplace=True)\n",
    "\n",
    "# # Salvar os resultados\n",
    "# df_estatisticas_final.to_csv(\"D:/Projeto_Tese_mestrado/02_Dataset/dados_ECG/dados_ecg_y.csv\", index=False)\n",
    "\n",
    "# # Tempo total de execução\n",
    "# fim_geral = time.time()\n",
    "# print(f\"🏁 Processamento concluído! Tempo total: {fim_geral - inicio_geral:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "link =r'D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ECG\\dados_ecg_y.csv'\n",
    "df_y = pd.read_csv(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando ECG_R_Peaks\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7902 - loss: 0.7131 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 11.21 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_P_Peaks\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7902 - loss: 0.7042 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 11.51 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_P_Onsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7902 - loss: 0.6666 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 14.17 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_P_Offsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7902 - loss: 0.7023\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 13.02 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_Q_Peaks\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7902 - loss: 0.6830\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 14.73 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_R_Onsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7902 - loss: 0.6850 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 14.07 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_R_Offsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7902 - loss: 0.7282 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 11.17 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_S_Peaks\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7902 - loss: 0.7083 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 12.15 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_T_Peaks\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7902 - loss: 0.7434 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 11.98 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_T_Onsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7902 - loss: 0.7164\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 12.52 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_T_Offsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7902 - loss: 0.7507\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 11.92 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_Phase_Atrial\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7902 - loss: 0.6900\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 19.77 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_Phase_Completion_Atrial\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7902 - loss: 0.6964 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 17.14 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_Phase_Ventricular\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7902 - loss: 0.7157\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 14.48 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_Phase_Completion_Ventricular\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7902 - loss: 0.7236 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 16.36 segundos\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "colunas = [\n",
    "    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', \n",
    "    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', \n",
    "    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',\n",
    "    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',\n",
    "    'ECG_Phase_Completion_Ventricular'\n",
    "]\n",
    "\n",
    "for col in colunas:\n",
    "    print(f'Executando {col}')\n",
    "    \n",
    "    start_time = time.time()  # Inicia a contagem do tempo\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = tratamento_dados(df_y, filtro=col)\n",
    "    construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])\n",
    "    \n",
    "    end_time = time.time()  # Finaliza a contagem do tempo\n",
    "    elapsed_time = end_time - start_time  # Calcula o tempo decorrido\n",
    "    \n",
    "    print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')\n",
    "    print('=' * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import duckdb\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import wfdb\n",
    "# import neurokit2 as nk\n",
    "\n",
    "# # Conectar ao banco de dados DuckDB\n",
    "# conn = duckdb.connect(\"D:/Projeto_Tese_mestrado/02_Dataset/Duckedb/banco_ecg.duckdb\")\n",
    "\n",
    "# # Carregar apenas a lista de pacientes\n",
    "# pacientes = conn.execute(\"SELECT DISTINCT id_paciente FROM ecg_pacientes ORDER BY id_paciente\").fetchdf()\n",
    "\n",
    "# # Lista de colunas que indicam eventos no ECG\n",
    "# colunas = [\n",
    "#     'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', \n",
    "#     'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', \n",
    "#     'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',\n",
    "#     'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',\n",
    "#     'ECG_Phase_Completion_Ventricular'\n",
    "# ]\n",
    "\n",
    "# # Função para calcular estatísticas\n",
    "# def calcular_estatisticas(ondas):\n",
    "#     if ondas.empty:  \n",
    "#         return {stat: 0 for stat in [\"mean\", \"std\", \"max\", \"min\", \"25%\", \"50%\", \"75%\"]}\n",
    "    \n",
    "#     # Substituir valores NaN por zero antes de calcular as estatísticas\n",
    "#     ondas = ondas.fillna(0)\n",
    "    \n",
    "#     estatisticas = {\n",
    "#         \"mean\": ondas.mean(),\n",
    "#         \"std\": ondas.std(),\n",
    "#         \"max\": ondas.max(),\n",
    "#         \"min\": ondas.min(),\n",
    "#         \"25%\": ondas.quantile(0.25),\n",
    "#         \"50%\": ondas.quantile(0.50),\n",
    "#         \"75%\": ondas.quantile(0.75)\n",
    "#     }\n",
    "    \n",
    "#     return estatisticas\n",
    "\n",
    "# # Processar paciente por paciente\n",
    "# dados_processados = []\n",
    "# inicio_geral = time.time()\n",
    "\n",
    "# for idx, paciente in enumerate(pacientes[\"id_paciente\"]):\n",
    "#     try:\n",
    "#         inicio_paciente = time.time()\n",
    "#         print(f\"✅ Processando paciente {idx+1}/{len(pacientes)}: {paciente}\")\n",
    "\n",
    "#         # Carregar os dados do paciente diretamente do banco\n",
    "#         df_paciente = conn.execute(f\"\"\"\n",
    "#             SELECT sinal_z FROM ecg_pacientes\n",
    "#             WHERE id_paciente = '{paciente}'\n",
    "#         \"\"\").fetchdf()\n",
    "\n",
    "#         # Se não houver dados, pula o paciente\n",
    "#         if df_paciente.empty:\n",
    "#             print(f\"⚠️ Nenhum dado encontrado para o paciente {paciente}. Pulando...\")\n",
    "#             continue\n",
    "\n",
    "\n",
    "#         sinal_z = df_paciente[\"sinal_z\"].values\n",
    "\n",
    "\n",
    "#         # Processar com Neurokit2\n",
    "#         ecg_z, _ = nk.ecg_process(sinal_z, sampling_rate=1000, method='neurokit')  # Ajuste a taxa de amostragem se necessário\n",
    "\n",
    "#         # Criar dicionário para armazenar estatísticas\n",
    "#         estatisticas_paciente = {}\n",
    "\n",
    "#         for coluna in colunas:\n",
    "#             if coluna not in ecg_z.columns:\n",
    "#                 print(f\"⚠️ Coluna {coluna} não encontrada. Pulando...\")\n",
    "#                 continue \n",
    "\n",
    "#             ondas = ecg_z.loc[ecg_z[coluna] == 1, 'ECG_Clean']\n",
    "#             estatisticas = calcular_estatisticas(ondas)\n",
    "\n",
    "#             for stat, valor in estatisticas.items():\n",
    "#                 estatisticas_paciente[f\"{coluna}_{stat}\"] = valor\n",
    "                \n",
    "                \n",
    "#         # Adicionando o target do paciente\n",
    "#         estatisticas_paciente[\"target\"] = df_final.loc[idx, \"Cause of death\"]\n",
    "\n",
    "#         # Adicionar ID do paciente\n",
    "#         estatisticas_paciente[\"id_paciente\"] = paciente\n",
    "\n",
    "#         # Salvar os dados processados\n",
    "#         dados_processados.append(estatisticas_paciente)\n",
    "\n",
    "#         # Liberar memória\n",
    "#         del sinal_z, ecg_z, df_paciente\n",
    "\n",
    "#         print(f\"✅ Paciente {paciente} processado com sucesso!\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Erro ao processar paciente {paciente}: {e}\")\n",
    "#         continue\n",
    "\n",
    "# # Fechar a conexão com o banco\n",
    "# conn.close()\n",
    "\n",
    "# # Criar DataFrame com os resultados\n",
    "# df_estatisticas_final = pd.DataFrame(dados_processados)\n",
    "# df_estatisticas_final.fillna(0, inplace=True)\n",
    "\n",
    "# # Salvar os resultados\n",
    "# df_estatisticas_final.to_csv(\"D:/Projeto_Tese_mestrado/02_Dataset/dados_ECG/dados_ecg_z.csv\", index=False)\n",
    "\n",
    "# # Tempo total de execução\n",
    "# fim_geral = time.time()\n",
    "# print(f\"🏁 Processamento concluído! Tempo total: {fim_geral - inicio_geral:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "link =r'D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_ECG\\dados_ecg_z.csv'\n",
    "df_z = pd.read_csv(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando ECG_R_Peaks\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7902 - loss: 0.7303 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 14.91 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_P_Peaks\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7902 - loss: 0.6988 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 15.35 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_P_Onsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7902 - loss: 0.7273\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 14.93 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_P_Offsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7902 - loss: 0.6928\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 14.98 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_Q_Peaks\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7849 - loss: 0.7358 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7540\n",
      "Finalizando o processo - Tempo de execução: 17.29 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_R_Onsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7902 - loss: 0.6927 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 13.75 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_R_Offsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7954 - loss: 0.7058 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7698\n",
      "Finalizando o processo - Tempo de execução: 14.31 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_S_Peaks\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7902 - loss: 0.7094 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 12.19 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_T_Peaks\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7902 - loss: 0.7353\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 12.86 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_T_Onsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7902 - loss: 0.7197\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 13.45 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_T_Offsets\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7902 - loss: 0.7326\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 14.81 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_Phase_Atrial\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7902 - loss: 0.7085 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 16.20 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_Phase_Completion_Atrial\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7902 - loss: 0.6927\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 16.17 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_Phase_Ventricular\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7902 - loss: 0.7601\n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 13.32 segundos\n",
      "====================================================================================================\n",
      "Executando ECG_Phase_Completion_Ventricular\n",
      "Rótulos originais: [0 3 6 7]\n",
      "Rótulos transformados: [0 1 2 3]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7902 - loss: 0.7497 \n",
      "\n",
      "Acurácia no conjunto de validação: 0.7619\n",
      "Finalizando o processo - Tempo de execução: 12.88 segundos\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "colunas = [\n",
    "    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', \n",
    "    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', \n",
    "    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',\n",
    "    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',\n",
    "    'ECG_Phase_Completion_Ventricular'\n",
    "]\n",
    "\n",
    "for col in colunas:\n",
    "    print(f'Executando {col}')\n",
    "    \n",
    "    start_time = time.time()  # Inicia a contagem do tempo\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = tratamento_dados(df_z, filtro=col)\n",
    "    construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])\n",
    "    \n",
    "    end_time = time.time()  # Finaliza a contagem do tempo\n",
    "    elapsed_time = end_time - start_time  # Calcula o tempo decorrido\n",
    "    \n",
    "    print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')\n",
    "    print('=' * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando somente a tabela dos pacientes sem usar o ECG para comparar com o ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "def tratamento_dados(dados):\n",
    "    \"\"\"\n",
    "    Função para tratar os dados, realizando pré-processamento sem filtro específico.\n",
    "\n",
    "    Parâmetros:\n",
    "    - dados (DataFrame): O dataset a ser tratado.\n",
    "\n",
    "    Retorna:\n",
    "    - X_train_scaled, X_test_scaled, y_train, y_test\n",
    "    \"\"\"\n",
    "\n",
    "    # Separar as features (X) e o target (y)\n",
    "    X = dados.drop(columns=['target']).values  # Remove a coluna target e pega as features\n",
    "    y = dados['target'].values  # Mantém apenas a coluna target\n",
    "\n",
    "    # Transformar os rótulos usando LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Verificar o mapeamento dos rótulos\n",
    "    print(\"Rótulos originais:\", np.unique(y))\n",
    "    print(\"Rótulos transformados:\", np.unique(y_encoded))\n",
    "\n",
    "    # Dividir em treino e teste (80% treino, 20% teste)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n",
    "\n",
    "    # Normalizar os dados\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Converter o target para one-hot encoding se for multiclasse\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_csv = r'D:\\Projeto_Tese_mestrado\\02_Dataset\\dados_csv_info_definitions\\ubject-info_limpo.csv'\n",
    "\n",
    "dados = pd.read_csv(link_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df =  dados.rename(columns={'Cause of death': 'target'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "colunas = [\n",
    "    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', \n",
    "    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', \n",
    "    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',\n",
    "    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',\n",
    "    'ECG_Phase_Completion_Ventricular'\n",
    "]\n",
    "\n",
    "for col in colunas:\n",
    "    print(f'Executando {col}')\n",
    "    \n",
    "    start_time = time.time()  # Inicia a contagem do tempo\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = tratamento_dados(df_z, filtro=col)\n",
    "    construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])\n",
    "    \n",
    "    end_time = time.time()  # Finaliza a contagem do tempo\n",
    "    elapsed_time = end_time - start_time  # Calcula o tempo decorrido\n",
    "    \n",
    "    print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')\n",
    "    print('=' * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando com colunas: mean\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['target'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m df_final \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_x_filtrado, df_y_filtrado, df_z_filtrado], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Dividir em treino e teste\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtratamento_dados\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_final\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Construir e treinar a rede neural\u001b[39;00m\n\u001b[0;32m     41\u001b[0m construir_rede_neural(X_train, X_test, y_train, y_test, input_dim\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[55], line 18\u001b[0m, in \u001b[0;36mtratamento_dados\u001b[1;34m(dados)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mFunção para tratar os dados, realizando pré-processamento sem filtro específico.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m- X_train_scaled, X_test_scaled, y_train, y_test\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Separar as features (X) e o target (y)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdados\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Remove a coluna target e pega as features\u001b[39;00m\n\u001b[0;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m dados[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Mantém apenas a coluna target\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Transformar os rótulos usando LabelEncoder\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\NOTEBOOK\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['target'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# import time\n",
    "\n",
    "# # Dicionário para armazenar diferentes categorias de colunas\n",
    "# estatisticas = {\n",
    "#     \"mean\": [],\n",
    "#     \"max\": [],\n",
    "#     \"std\": [],\n",
    "#     \"min\": [],\n",
    "#     \"25%\": [],\n",
    "#     \"50%\": [],\n",
    "#     \"75%\": []\n",
    "# }\n",
    "\n",
    "# # Pegando todas as colunas e classificando por estatística\n",
    "# for col in df_x.columns:\n",
    "#     for estatistica in estatisticas.keys():\n",
    "#         if estatistica in col:\n",
    "#             estatisticas[estatistica].append(col)\n",
    "\n",
    "# # Criando lista de DataFrames para treinar\n",
    "# dataframes = [df_x, df_y, df_z]\n",
    "\n",
    "# # Loop para treinar com cada estatística\n",
    "# for estatistica, colunas in estatisticas.items():\n",
    "#     print(f'Treinando com colunas: {estatistica}')\n",
    "    \n",
    "#     start_time = time.time()  # Início do tempo\n",
    "\n",
    "#     # Filtrar apenas as colunas da estatística atual para cada DataFrame\n",
    "#     df_x_filtrado = df_x[colunas]\n",
    "#     df_y_filtrado = df_y[colunas]\n",
    "#     df_z_filtrado = df_z[colunas]\n",
    "    \n",
    "#     # Concatenar os DataFrames (se necessário)\n",
    "#     df_final = pd.concat([df_x_filtrado, df_y_filtrado, df_z_filtrado], axis=0)\n",
    "\n",
    "#     # Dividir em treino e teste\n",
    "#     X_train, X_test, y_train, y_test = tratamento_dados(df_final)\n",
    "\n",
    "#     # Construir e treinar a rede neural\n",
    "#     construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])\n",
    "\n",
    "#     end_time = time.time()  # Fim do tempo\n",
    "#     elapsed_time = end_time - start_time  # Tempo decorrido\n",
    "\n",
    "#     print(f'Finalizando treinamento para {estatistica} - Tempo: {elapsed_time:.2f} segundos')\n",
    "#     print('=' * 100)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMCS713EE+sC1as/alF5ev1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
