


import pandas as pd

import numpy as np
import wfdb
import os
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
import plotly.io as pio
#pio.renderers.default = "colab"
import neurokit2 as nk
#pio.renderers.default = "browser"  # Abre o gráfico no navegador
# ou
#pio.renderers.default = "notebook"  # Usa o modo compatível com notebooks
# Configurar o tamanho da figura globalmente
plt.rcParams["figure.figsize"] = (16, 6)  # Ajuste a largura e altura aqui

# Definir o número máximo de linhas a serem exibidas
pd.set_option('display.max_rows', None)  # Exibe todas as linhas


# import pandas as pd
# import glob

# # Define o padrão para os arquivos CSV

# path = r'D:\Projeto_Tese_mestrado\02_Dataset\dados_ECG\High-resolution_ECG\P0*'
# path_holter = r'D:\Projeto_Tese_mestrado\02_Dataset\dados_ECG\Holter_ECG\P0*'

# #path = r'E:/Repositorio_Git/zzz-projeto_final/dados/ca-*.csv'


# # Usando glob para pegar todos os arquivos que seguem o padrão
# arquivos = glob.glob(path)
# arquivos_path_holter = glob.glob(path_holter)
# arquivos = [os.path.splitext(arquivo)[0] for arquivo in arquivos] # tirando as a extensões

# arquivos_path_holter = [os.path.splitext(arquivos_path_holter)[0] for arquivos_path_holter in arquivos_path_holter] # tirando as a extensões


import pandas as pd
import glob

# Define o padrão para os arquivos CSV

#path = r'D:\Projeto_Tese_mestrado\02_Dataset\dados_ECG\High-resolution_ECG\P0*'
path = r'D:\Projeto_Tese_mestrado\02_Dataset\dados_ECG\Holter_ECG\P0*'

#path = r'E:/Repositorio_Git/zzz-projeto_final/dados/ca-*.csv'


# Usando glob para pegar todos os arquivos que seguem o padrão
arquivos = glob.glob(path)
#arquivos_path_holter = glob.glob(path_holter)
arquivos = [os.path.splitext(arquivo)[0] for arquivo in arquivos] # tirando as a extensões

#arquivos_path_holter = [os.path.splitext(arquivos_path_holter)[0] for arquivos_path_holter in arquivos_path_holter] # tirando as a extensões


# #arquivos_path_holter

# record = wfdb.rdrecord(arquivos_path_holter[0])


# # Exibir as informações do arquivo
# print(record.__dict__)
#arquivos_path_holter

lista_sem_duplicatas = pd.Series(arquivos).unique().tolist() # removando duplicadas


lista_sem_duplicatas[4]


record = wfdb.rdrecord(lista_sem_duplicatas[0])





p_signal = record.p_signal
p_signal


sinal_x = p_signal[:,0]
sinal_y = p_signal[:,1]
sinal_z = p_signal[:,2]


len(sinal_x)


#ecg_cleaned = nk.ecg_clean(sinal_x, sampling_rate=record.fs, method="neurokit")


ecg_cleaned 


quality = nk.ecg_quality(ecg_cleaned, sampling_rate=record.fs)
nk.signal_plot([ecg_cleaned, quality], standardize=True)


exg_dataframe = pd.DataFrame(ecg_cleaned, columns=['ECG_Clean'])


exg_dataframe


#px.line(exg_dataframe[1000:2000], x=exg_dataframe.index[0:1000], y = 'ECG_Clean')


%%time

ecg_x, dict_info_x = nk.ecg_process(sinal_x, sampling_rate= record.fs , method='neurokit') # Dividindo a tupla em DataFrame e Dicionário
#ecg_y, dict_info_y = nk.ecg_process(sinal_y, sampling_rate= record.fs , method='neurokit')
#ecg_z, dict_info_z = nk.ecg_process(sinal_z, sampling_rate= record.fs , method='neurokit')





ecg_x['ECG_Clean'].head()


teste = ecg_x['ECG_Clean']


teste_resultado, _ = nk.ecg_process(teste, sampling_rate= record.fs , method='neurokit') 


teste_resultado['ECG_Clean'].head()


# somente um paciente contem 
ecg_x.shape


ecg_x.columns


ecg_x.head()








link_csv = r'D:\Projeto_Tese_mestrado\02_Dataset\dados_csv_info_definitions\ubject-info_limpo.csv'

dados = pd.read_csv(link_csv)


dados.head()


# Contagem de classes no conjunto de dados
class_counts = dados['Cause of death'].value_counts()


# Contagem de classes no conjunto de dados
for class_label, count in class_counts.items():
    print(f'Classe {class_label}: {count} -> {count/dados.shape[0]*100:.2f}%')




print(f'Quantidade de colunas: {dados.shape[1]}')
print(f'Quantidade de linhas: {dados.shape[0]}')


pd.unique(dados['Exit of the study'])


#arquivos_path_holter

lista_sem_duplicatas = pd.Series(arquivos).unique().tolist() # removando duplicadas


len(lista_sem_duplicatas)


# Criando um DataFrame dos endereços
df_enderecos = pd.DataFrame({
    "Endereço": lista_sem_duplicatas
})

# Extraindo o Patient ID do final do endereço
df_enderecos["Patient ID"] = df_enderecos["Endereço"].apply(lambda x: x.split("\\")[-1])


# Removendo o sufixo '_H' da coluna 'Patient ID'
df_enderecos['Patient ID'] = df_enderecos['Patient ID'].str.replace('_H', '', regex=False)

# Aqui contem dados de duas colunas a primeira coluna vou usar como chave id para conectar os pacientes de cada scg e a segunda o tipo de morte.
novos_dados = dados[['Patient ID','Cause of death']]

# Fazendo a fusão (merge) dos DataFrames com base no "Patient ID"
df_final = df_enderecos.merge(novos_dados, on="Patient ID", how="left")
print(f'Sem o filtro de tipo de mortes {df_final.shape}')

df_final = df_final[~df_final['Cause of death'].isin([1])] # tirando valores que tem 1 pois significa mortes não identificada ou seja ruídos,

print(f'Com o filtro de tipo de mortes {df_final.shape}')

# Contagem de classes no conjunto de dados
class_counts = df_final['Cause of death'].value_counts()


# Contagem de classes no conjunto de dados
for class_label, count in class_counts.items():
    print(f'Classe {class_label}: {count} -> {count/df_final.shape[0]*100:.2f}%')
    
    
# Resetando o índice para garantir que o loop use os índices corretos
df_final.reset_index(drop=True, inplace=True)


# # Lista para armazenar informações de cada paciente
# dados_pacientes = []

# # Loop para processar cada arquivo e pegar o tamanho do sinal
# for idx, arquivo in enumerate(arquivos):
#     try:
#         # Lendo o arquivo do ECG
#         record = wfdb.rdrecord(arquivo)
#         p_signal = record.p_signal
#         sinal_x = p_signal[:, 0]
        
#         # Armazenando as informações no dicionário
#         dados_pacientes.append({
#             "Paciente": arquivo.split("\\")[-1],  # Extrair o nome do paciente
#             "Quantidade de amostras": len(sinal_x)
#         })

#     except Exception as e:
#         print(f"Erro ao processar {arquivo}: {e}")

# # Convertendo a lista para um DataFrame
# df_pacientes = pd.DataFrame(dados_pacientes)

# # Exibindo as primeiras linhas do DataFrame
# print(df_pacientes)

# df_pacientes['Quantidade de amostras'].unique()


# hrv_time = nk.hrv_time(peaks, sampling_rate=100, show=True)
# hrv_time








import duckdb
import pandas as pd
import numpy as np
import time
import wfdb
import neurokit2 as nk

# Conectar ao banco de dados DuckDB
conn = duckdb.connect(r"D:\Projeto_Tese_mestrado\02_Dataset\Duckedb\Holter_ECG\5minutos\banco_ecg.duckdb")

# Carregar apenas a lista de pacientes
pacientes = conn.execute("SELECT DISTINCT id_paciente FROM ecg_pacientes ORDER BY id_paciente").fetchdf()

# Lista de colunas que indicam eventos no ECG
colunas = [
    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', 
    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', 
    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',
    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',
    'ECG_Phase_Completion_Ventricular'
]

# Função para calcular estatísticas
def calcular_estatisticas(ondas):
    if ondas.empty:  
        return {stat: 0 for stat in ["mean", "std", "max", "min", "25%", "50%", "75%"]}
    
    # Substituir valores NaN por zero antes de calcular as estatísticas
    ondas = ondas.fillna(0)
    
    estatisticas = {
        "mean": ondas.mean(),
        "std": ondas.std(),
        "max": ondas.max(),
        "min": ondas.min(),
        "25%": ondas.quantile(0.25),
        "50%": ondas.quantile(0.50),
        "75%": ondas.quantile(0.75)
    }
    
    return estatisticas

# Processar paciente por paciente
dados_processados = []
inicio_geral = time.time()


# Filtrar os pacientes que estão no df_filtro
pacientes_filtrados = pacientes[pacientes['id_paciente'].isin(df_final['Patient ID'])]

# Processar paciente por paciente
dados_processados = []
inicio_geral = time.time()

for idx, paciente in enumerate(pacientes_filtrados["id_paciente"]):
    try:
        inicio_paciente = time.time()
        print(f"✅ Processando paciente {idx+1}/{len(pacientes_filtrados)}: {paciente}")

        # Carregar os dados do paciente diretamente do banco
        df_paciente = conn.execute(f"""
            SELECT sinal_x FROM ecg_pacientes
            WHERE id_paciente = '{paciente}'
        """).fetchdf()

        # Se não houver dados, pula o paciente
        if df_paciente.empty:
            print(f"⚠️ Nenhum dado encontrado para o paciente {paciente}. Pulando...")
            continue

        sinal_x = df_paciente["sinal_x"].values

        # Processar com Neurokit2
        ecg_x, _ = nk.ecg_process(sinal_x, sampling_rate=1000, method='neurokit') 

        # Criar dicionário para armazenar estatísticas
        estatisticas_paciente = {}

        for coluna in colunas:
            if coluna not in ecg_x.columns:
                print(f"⚠️ Coluna {coluna} não encontrada. Pulando...")
                continue 

            ondas = ecg_x.loc[ecg_x[coluna] == 1, 'ECG_Clean']
            estatisticas = calcular_estatisticas(ondas)

            for stat, valor in estatisticas.items():
                estatisticas_paciente[f"{coluna}_{stat}"] = valor

        # Adicionando o target do paciente
        estatisticas_paciente["target"] = df_final.loc[df_final["Patient ID"] == paciente, "Cause of death"].values[0]

        # Adicionar ID do paciente
        estatisticas_paciente["id_paciente"] = paciente

        # Salvar os dados processados
        dados_processados.append(estatisticas_paciente)

        # Liberar memória
        del sinal_x, ecg_x, df_paciente

        print(f"✅ Paciente {paciente} processado com sucesso!")

    except Exception as e:
        print(f"❌ Erro ao processar paciente {paciente}: {e}")
        continue

# Fechar a conexão com o banco
conn.close()

# Criar DataFrame com os resultados
df_estatisticas_final = pd.DataFrame(dados_processados)
df_estatisticas_final.fillna(0, inplace=True)

# Salvar os resultados
df_estatisticas_final.to_csv(r"D:\Projeto_Tese_mestrado\02_Dataset\dados_ecg_csv_estatistico\Holter_ECG\dados_ecg_x.csv", index=False)

# Tempo total de execução
fim_geral = time.time()
print(f"🏁 Processamento concluído! Tempo total: {fim_geral - inicio_geral:.2f} segundos")



# # Contar o número de pacientes distintos no banco DuckDB
# num_pacientes_distintos = conn.execute("SELECT COUNT(DISTINCT id_paciente) FROM ecg_pacientes").fetchone()[0]

# # Exibir a quantidade de pacientes distintos
# print(f"Quantidade de pacientes distintos no banco: {num_pacientes_distintos}")



# salvando o dataframe
#df_ecg_final.to_csv("dados_ecg.csv", index=False)



#df_estatisticas.shape


#df_estatisticas.head()


from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np


def tratamento_dados(dados, filtro):
    """
    Função para tratar os dados, filtrando colunas específicas e realizando pré-processamento.

    Parâmetros:
    - dados (DataFrame): O dataset a ser tratado.
    - filtro (str): Palavra-chave para selecionar as colunas (default: "ECG_R_Peaks").

    Retorna:
    - X_train_scaled, X_test_scaled, y_train, y_test
    """

    # Filtrar colunas que contêm a palavra-chave no nome
    colunas_filtradas = dados.filter(like=filtro).columns
    X = dados[colunas_filtradas].values  # Apenas colunas filtradas

    # Garantir que o target seja separado corretamente
    y = dados['target'].values  

    # Transformar os rótulos usando LabelEncoder
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)

    # Verificar o mapeamento dos rótulos
    print("Rótulos originais:", np.unique(y))
    print("Rótulos transformados:", np.unique(y_encoded))

    # Dividir em treino e teste (80% treino, 20% teste)
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)

    # Normalizar os dados
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Converter o target para one-hot encoding se for multiclasse
    num_classes = len(np.unique(y_encoded))
    y_train = to_categorical(y_train, num_classes=num_classes)
    y_test = to_categorical(y_test, num_classes=num_classes)
    
    return X_train_scaled, X_test_scaled, y_train, y_test
















import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import LeakyReLU

def construir_rede_neural(X_train, X_test, y_train, y_test,input_dim):
    # Criar modelo poderoso
    def criar_rede_neural(input_dim, num_classes):
        model = keras.Sequential([
        keras.Input(shape=(input_dim,)),  # Corrigindo a entrada
        Dense(512),
        LeakyReLU(negative_slope=0.1),  # Corrigindo o parâmetro alpha
        BatchNormalization(),
        Dropout(0.3),

        Dense(256),
        LeakyReLU(negative_slope=0.1),
        BatchNormalization(),
        Dropout(0.3),

        Dense(128),
        LeakyReLU(negative_slope=0.1),
        BatchNormalization(),
        Dropout(0.3),

        Dense(64),
        LeakyReLU(negative_slope=0.1),
        BatchNormalization(),
        Dropout(0.3),

        Dense(num_classes, activation='softmax')
    ])



        # Compilar modelo
        model.compile(
            optimizer=Adam(learning_rate=0.001),  # Adam já é otimizado para deep learning
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model

    # Criar callbacks
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10)


    # Criar e treinar o modelo
    modelo = criar_rede_neural(input_dim=input_dim, num_classes=4)
    history = modelo.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=100,
        batch_size=32,
        callbacks=[early_stopping, reduce_lr],
        verbose=0  # Define para 0 para ocultar a saída
        #verbose=2  # Apenas mostra os valores das métricas
    )

    # Avaliação
    loss, acc = modelo.evaluate(X_test, y_test)
    print(f"\nAcurácia no conjunto de validação: {acc:.4f}")



link =r"D:\Projeto_Tese_mestrado\02_Dataset\dados_ecg_csv_estatistico\Holter_ECG/dados_ecg_x.csv"
df_x = pd.read_csv(link)


df_x.head()


import time

colunas = [
    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', 
    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', 
    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',
    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',
    'ECG_Phase_Completion_Ventricular'
]

for col in colunas:
    print(f'Executando {col}')
    
    start_time = time.time()  # Inicia a contagem do tempo
    
    X_train, X_test, y_train, y_test = tratamento_dados(df_x, filtro=col)
    construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])
    
    end_time = time.time()  # Finaliza a contagem do tempo
    elapsed_time = end_time - start_time  # Calcula o tempo decorrido
    
    print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')
    print('=' * 100)






import duckdb
import pandas as pd
import numpy as np
import time
import wfdb
import neurokit2 as nk

# Conectar ao banco de dados DuckDB
conn = duckdb.connect(r"D:\Projeto_Tese_mestrado\02_Dataset\Duckedb\Holter_ECG\5minutos\banco_ecg.duckdb")

# Carregar apenas a lista de pacientes
pacientes = conn.execute("SELECT DISTINCT id_paciente FROM ecg_pacientes ORDER BY id_paciente").fetchdf()

# Lista de colunas que indicam eventos no ECG
colunas = [
    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', 
    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', 
    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',
    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',
    'ECG_Phase_Completion_Ventricular'
]

# Função para calcular estatísticas
def calcular_estatisticas(ondas):
    if ondas.empty:  
        return {stat: 0 for stat in ["mean", "std", "max", "min", "25%", "50%", "75%"]}
    
    # Substituir valores NaN por zero antes de calcular as estatísticas
    ondas = ondas.fillna(0)

    # Evitar divisão por zero substituindo valores muito pequenos
    ondas[ondas == 0] = 1e-6
    
    estatisticas = {
        "mean": ondas.mean(),
        "std": ondas.std(),
        "max": ondas.max(),
        "min": ondas.min(),
        "25%": ondas.quantile(0.25),
        "50%": ondas.quantile(0.50),
        "75%": ondas.quantile(0.75)
    }
    
    return estatisticas

# Processar paciente por paciente
dados_processados = []
inicio_geral = time.time()


# Filtrar os pacientes que estão no df_filtro
pacientes_filtrados = pacientes[pacientes['id_paciente'].isin(df_final['Patient ID'])]

# Processar paciente por paciente
dados_processados = []
inicio_geral = time.time()

for idx, paciente in enumerate(pacientes_filtrados["id_paciente"]):
    try:
        inicio_paciente = time.time()
        print(f"✅ Processando paciente {idx+1}/{len(pacientes_filtrados)}: {paciente}")

        # Carregar os dados do paciente diretamente do banco
        df_paciente = conn.execute(f"""
            SELECT sinal_y FROM ecg_pacientes
            WHERE id_paciente = '{paciente}'
        """).fetchdf()

        # Se não houver dados, pula o paciente
        if df_paciente.empty:
            print(f"⚠️ Nenhum dado encontrado para o paciente {paciente}. Pulando...")
            continue

        sinal_y = df_paciente["sinal_y"].values

        # Processar com Neurokit2
        ecg_y, _ = nk.ecg_process(sinal_y, sampling_rate=1000, method='neurokit') 

        # Criar dicionário para armazenar estatísticas
        estatisticas_paciente = {}

        for coluna in colunas:
            if coluna not in ecg_y.columns:
                print(f"⚠️ Coluna {coluna} não encontrada. Pulando...")
                continue 

            ondas = ecg_y.loc[ecg_y[coluna] == 1, 'ECG_Clean']
            estatisticas = calcular_estatisticas(ondas)

            for stat, valor in estatisticas.items():
                estatisticas_paciente[f"{coluna}_{stat}"] = valor

        # Adicionando o target do paciente
        estatisticas_paciente["target"] = df_final.loc[df_final["Patient ID"] == paciente, "Cause of death"].values[0]

        # Adicionar ID do paciente
        estatisticas_paciente["id_paciente"] = paciente

        # Salvar os dados processados
        dados_processados.append(estatisticas_paciente)

        # Liberar memória
        del sinal_y, ecg_y, df_paciente

        print(f"✅ Paciente {paciente} processado com sucesso!")

    except Exception as e:
        print(f"❌ Erro ao processar paciente {paciente}: {e}")
        continue

# Fechar a conexão com o banco
conn.close()

# Criar DataFrame com os resultados
df_estatisticas_final = pd.DataFrame(dados_processados)

# Substituir valores infinitos por um valor pequeno
df_estatisticas_final.replace([np.inf, -np.inf], 1e-6, inplace=True)

# Substituir NaN por zero
df_estatisticas_final.fillna(0, inplace=True)

# Salvar os resultados
df_estatisticas_final.to_csv(r"D:\Projeto_Tese_mestrado\02_Dataset\dados_ecg_csv_estatistico\Holter_ECGdados_ecg_y.csv", index=False)

# Tempo total de execução
fim_geral = time.time()
print(f"🏁 Processamento concluído! Tempo total: {fim_geral - inicio_geral:.2f} segundos")



link =r"D:\Projeto_Tese_mestrado\02_Dataset\dados_ecg_csv_estatistico\Holter_ECG\dados_ecg_y.csv"
df_y = pd.read_csv(link)


df_y.head()


import time

colunas = [
    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', 
    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', 
    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',
    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',
    'ECG_Phase_Completion_Ventricular'
]

for col in colunas:
    print(f'Executando {col}')
    
    start_time = time.time()  # Inicia a contagem do tempo
    
    X_train, X_test, y_train, y_test = tratamento_dados(df_y, filtro=col)
    construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])
    
    end_time = time.time()  # Finaliza a contagem do tempo
    elapsed_time = end_time - start_time  # Calcula o tempo decorrido
    
    print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')
    print('=' * 100)






import duckdb
import pandas as pd
import numpy as np
import time
import wfdb
import neurokit2 as nk

# Conectar ao banco de dados DuckDB
conn = duckdb.connect(r"D:\Projeto_Tese_mestrado\02_Dataset\Duckedb\Holter_ECG\5minutos\banco_ecg.duckdb")

# Carregar apenas a lista de pacientes
pacientes = conn.execute("SELECT DISTINCT id_paciente FROM ecg_pacientes ORDER BY id_paciente").fetchdf()

# Lista de colunas que indicam eventos no ECG
colunas = [
    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', 
    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', 
    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',
    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',
    'ECG_Phase_Completion_Ventricular'
]

# Função para calcular estatísticas
def calcular_estatisticas(ondas):
    if ondas.empty:  
        return {stat: 0 for stat in ["mean", "std", "max", "min", "25%", "50%", "75%"]}
    
    # Substituir valores NaN por zero antes de calcular as estatísticas
    ondas = ondas.fillna(0)

    # Evitar divisão por zero substituindo valores muito pequenos
    ondas[ondas == 0] = 1e-6
    
    estatisticas = {
        "mean": ondas.mean(),
        "std": ondas.std(),
        "max": ondas.max(),
        "min": ondas.min(),
        "25%": ondas.quantile(0.25),
        "50%": ondas.quantile(0.50),
        "75%": ondas.quantile(0.75)
    }
    
    return estatisticas

# Processar paciente por paciente
dados_processados = []
inicio_geral = time.time()


# Filtrar os pacientes que estão no df_filtro
pacientes_filtrados = pacientes[pacientes['id_paciente'].isin(df_final['Patient ID'])]

# Processar paciente por paciente
dados_processados = []
inicio_geral = time.time()

for idx, paciente in enumerate(pacientes_filtrados["id_paciente"]):
    try:
        inicio_paciente = time.time()
        print(f"✅ Processando paciente {idx+1}/{len(pacientes_filtrados)}: {paciente}")

        # Carregar os dados do paciente diretamente do banco
        df_paciente = conn.execute(f"""
            SELECT sinal_z FROM ecg_pacientes
            WHERE id_paciente = '{paciente}'
        """).fetchdf()

        # Se não houver dados, pula o paciente
        if df_paciente.empty:
            print(f"⚠️ Nenhum dado encontrado para o paciente {paciente}. Pulando...")
            continue

        sinal_z = df_paciente["sinal_z"].values

        # Processar com Neurokit2
        ecg_z, _ = nk.ecg_process(sinal_z, sampling_rate=1000, method='neurokit') 

        # Criar dicionário para armazenar estatísticas
        estatisticas_paciente = {}

        for coluna in colunas:
            if coluna not in ecg_z.columns:
                print(f"⚠️ Coluna {coluna} não encontrada. Pulando...")
                continue 

            ondas = ecg_z.loc[ecg_z[coluna] == 1, 'ECG_Clean']
            estatisticas = calcular_estatisticas(ondas)

            for stat, valor in estatisticas.items():
                estatisticas_paciente[f"{coluna}_{stat}"] = valor

        # Adicionando o target do paciente
        estatisticas_paciente["target"] = df_final.loc[df_final["Patient ID"] == paciente, "Cause of death"].values[0]

        # Adicionar ID do paciente
        estatisticas_paciente["id_paciente"] = paciente

        # Salvar os dados processados
        dados_processados.append(estatisticas_paciente)

        # Liberar memória
        del sinal_z, ecg_z, df_paciente

        print(f"✅ Paciente {paciente} processado com sucesso!")

    except Exception as e:
        print(f"❌ Erro ao processar paciente {paciente}: {e}")
        continue

# Fechar a conexão com o banco
conn.close()

# Criar DataFrame com os resultados
df_estatisticas_final = pd.DataFrame(dados_processados)

# Substituir valores infinitos por um valor pequeno
df_estatisticas_final.replace([np.inf, -np.inf], 1e-6, inplace=True)

# Substituir NaN por zero
df_estatisticas_final.fillna(0, inplace=True)

# Salvar os resultados
df_estatisticas_final.to_csv(r"D:\Projeto_Tese_mestrado\02_Dataset\dados_ecg_csv_estatistico\Holter_ECG\dados_ecg_z.csv", index=False)

# Tempo total de execução
fim_geral = time.time()
print(f"🏁 Processamento concluído! Tempo total: {fim_geral - inicio_geral:.2f} segundos")



link = r"D:\Projeto_Tese_mestrado\02_Dataset\dados_ecg_csv_estatistico\Holter_ECG\dados_ecg_z.csv"
df_z = pd.read_csv(link)


import time

colunas = [
    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', 
    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', 
    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',
    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',
    'ECG_Phase_Completion_Ventricular'
]

for col in colunas:
    print(f'Executando {col}')
    
    start_time = time.time()  # Inicia a contagem do tempo
    
    X_train, X_test, y_train, y_test = tratamento_dados(df_z, filtro=col)
    construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])
    
    end_time = time.time()  # Finaliza a contagem do tempo
    elapsed_time = end_time - start_time  # Calcula o tempo decorrido
    
    print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')
    print('=' * 100)









from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import numpy as np

def tratamento_dados(dados):
    """
    Função para tratar os dados, realizando pré-processamento sem filtro específico.

    Parâmetros:
    - dados (DataFrame): O dataset a ser tratado.

    Retorna:
    - X_train_scaled, X_test_scaled, y_train, y_test
    """

    # Separar as features (X) e o target (y)
    X = dados.drop(columns=['target']).values  # Remove a coluna target e pega as features
    y = dados['target'].values  # Mantém apenas a coluna target

    # Transformar os rótulos usando LabelEncoder
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)

    # Verificar o mapeamento dos rótulos
    print("Rótulos originais:", np.unique(y))
    print("Rótulos transformados:", np.unique(y_encoded))

    # Dividir em treino e teste (80% treino, 20% teste)
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)

    # Normalizar os dados
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Converter o target para one-hot encoding se for multiclasse
    num_classes = len(np.unique(y_encoded))
    y_train = to_categorical(y_train, num_classes=num_classes)
    y_test = to_categorical(y_test, num_classes=num_classes)
    
    return X_train_scaled, X_test_scaled, y_train, y_test



link_csv = r'D:\Projeto_Tese_mestrado\02_Dataset\dados_csv_info_definitions\ubject-info_limpo.csv'

dados = pd.read_csv(link_csv)



df =  dados.rename(columns={'Cause of death': 'target'})

# Eletrocardiogramas e Holter
eletrocardiogramas_holter = [
    'Hig-resolution ECG available',
    'ECG rhythm ',
    'Q-waves (necrosis. yes=1)',
    'PR interval (ms)',
    'QRS duration (ms)',
    'QRS > 120 ms ',
    'QT interval (ms)',
    'QT corrected ',
    'Average RR (ms)',
    'Left ventricular hypertrophy (yes=1)',
    'Intraventricular conduction disorder',
    'target'
]
df = df[eletrocardiogramas_holter]




df.shape


df = df[~df['target'].isin([1])] # tirando valores que tem 1 pois significa mortes não identificada ou seja ruídos,


df.shape


import time

print(f'Executando eletrocardiogramas_holter')
print(f'Resulado da tabela')
start_time = time.time()  # Inicia a contagem do tempo

X_train, X_test, y_train, y_test = tratamento_dados(df)
construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])

end_time = time.time()  # Finaliza a contagem do tempo
elapsed_time = end_time - start_time  # Calcula o tempo decorrido

print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')
print('=' * 100)



# import time

# # Dicionário para armazenar diferentes categorias de colunas
# estatisticas = {
#     "mean": [],
#     "max": [],
#     "std": [],
#     "min": [],
#     "25%": [],
#     "50%": [],
#     "75%": []
# }

# # Pegando todas as colunas e classificando por estatística
# for col in df_x.columns:
#     for estatistica in estatisticas.keys():
#         if estatistica in col:
#             estatisticas[estatistica].append(col)

# # Criando lista de DataFrames para treinar
# dataframes = [df_x, df_y, df_z]

# # Loop para treinar com cada estatística
# for estatistica, colunas in estatisticas.items():
#     print(f'Treinando com colunas: {estatistica}')
    
#     start_time = time.time()  # Início do tempo

#     # Filtrar apenas as colunas da estatística atual para cada DataFrame
#     df_x_filtrado = df_x[colunas]
#     df_y_filtrado = df_y[colunas]
#     df_z_filtrado = df_z[colunas]
    
#     # Concatenar os DataFrames (se necessário)
#     df_final = pd.concat([df_x_filtrado, df_y_filtrado, df_z_filtrado], axis=0)

#     # Dividir em treino e teste
#     X_train, X_test, y_train, y_test = tratamento_dados(df_final)

#     # Construir e treinar a rede neural
#     construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])

#     end_time = time.time()  # Fim do tempo
#     elapsed_time = end_time - start_time  # Tempo decorrido

#     print(f'Finalizando treinamento para {estatistica} - Tempo: {elapsed_time:.2f} segundos')
#     print('=' * 100)

