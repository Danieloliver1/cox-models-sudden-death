import numpy as np
import wfdb
import os
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
import plotly.io as pio
#pio.renderers.default = "colab"
import pandas as pd
import neurokit2 as nk
#pio.renderers.default = "browser"  # Abre o gráfico no navegador
# ou
#pio.renderers.default = "notebook"  # Usa o modo compatível com notebooks
# Configurar o tamanho da figura globalmente
plt.rcParams["figure.figsize"] = (16, 6)  # Ajuste a largura e altura aqui

# Definir o número máximo de linhas a serem exibidas
pd.set_option('display.max_rows', None)  # Exibe todas as linhas


# import pandas as pd
# import glob

# # Define o padrão para os arquivos CSV

# path = r'D:\Projeto_Tese_mestrado\02_Dataset\dados_ECG\High-resolution_ECG\P0*'
# path_holter = r'D:\Projeto_Tese_mestrado\02_Dataset\dados_ECG\Holter_ECG\P0*'

# #path = r'E:/Repositorio_Git/zzz-projeto_final/dados/ca-*.csv'


# # Usando glob para pegar todos os arquivos que seguem o padrão
# arquivos = glob.glob(path)
# arquivos_path_holter = glob.glob(path_holter)
# arquivos = [os.path.splitext(arquivo)[0] for arquivo in arquivos] # tirando as a extensões

# arquivos_path_holter = [os.path.splitext(arquivos_path_holter)[0] for arquivos_path_holter in arquivos_path_holter] # tirando as a extensões


import pandas as pd
import glob

# Define o padrão para os arquivos CSV

path = r'D:\Projeto_Tese_mestrado\02_Dataset\dados_ECG\High-resolution_ECG\P0*'
#path_holter = r'D:\Projeto_Tese_mestrado\02_Dataset\dados_ECG\Holter_ECG\P0*'

#path = r'E:/Repositorio_Git/zzz-projeto_final/dados/ca-*.csv'


# Usando glob para pegar todos os arquivos que seguem o padrão
arquivos = glob.glob(path)
#arquivos_path_holter = glob.glob(path_holter)
arquivos = [os.path.splitext(arquivo)[0] for arquivo in arquivos] # tirando as a extensões

#arquivos_path_holter = [os.path.splitext(arquivos_path_holter)[0] for arquivos_path_holter in arquivos_path_holter] # tirando as a extensões


# #arquivos_path_holter

# record = wfdb.rdrecord(arquivos_path_holter[0])


# # Exibir as informações do arquivo
# print(record.__dict__)
#arquivos_path_holter

lista_sem_duplicatas = pd.Series(arquivos).unique().tolist() # removando duplicadas


lista_sem_duplicatas[4]


record = wfdb.rdrecord(lista_sem_duplicatas[0])





p_signal = record.p_signal
p_signal


sinal_x = p_signal[:,0]
sinal_y = p_signal[:,1]
sinal_z = p_signal[:,2]


len(sinal_x)


ecg_cleaned = nk.ecg_clean(sinal_x, sampling_rate=record.fs, method="neurokit")


ecg_cleaned 


quality = nk.ecg_quality(ecg_cleaned, sampling_rate=record.fs)
nk.signal_plot([ecg_cleaned, quality], standardize=True)


exg_dataframe = pd.DataFrame(ecg_cleaned, columns=['ECG_Clean'])


exg_dataframe


#px.line(exg_dataframe[1000:2000], x=exg_dataframe.index[0:1000], y = 'ECG_Clean')


%%time

ecg_x, dict_info_x = nk.ecg_process(sinal_x, sampling_rate= record.fs , method='neurokit') # Dividindo a tupla em DataFrame e Dicionário
#ecg_y, dict_info_y = nk.ecg_process(sinal_y, sampling_rate= record.fs , method='neurokit')
#ecg_z, dict_info_z = nk.ecg_process(sinal_z, sampling_rate= record.fs , method='neurokit')





ecg_x['ECG_Clean']


# somente um paciente contem 
ecg_x.shape


ecg_x.columns


ecg_x.head()





import pandas as pd

# Lista de colunas que indicam eventos no ECG
colunas = ['ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', 
           'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', 
           'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',
           'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',
           'ECG_Phase_Completion_Ventricular']


# Criar DataFrames separados com o nome da respectiva coluna
for coluna in colunas:
    globals()[coluna] = pd.DataFrame({coluna: ecg_x.loc[ecg_x[coluna] == 1, 'ECG_Clean'].values})
    
# criando uma função     
def tipo_de_ondas(ecg):
    # Criar um dicionário para armazenar os DataFrames
    tipos_ondas = {}

    for coluna in colunas:
        # Filtrar as linhas onde a coluna é igual a 1
        tipos_ondas[coluna] = ecg.loc[ecg[coluna] == 1, 'ECG_Clean']

    return tipos_ondas



df_dict = {}
for coluna in colunas:
    df_dict[coluna] = ecg_x.loc[ecg_x[coluna] == 1, 'ECG_Clean'].values




import matplotlib.pyplot as plt
import seaborn as sns

for x in colunas:
    lista = df_dict[x]
    
    # Criar figura e eixos para os dois gráficos
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 linha, 2 colunas
    
    # Histograma com KDE
    sns.histplot(lista, kde=True, bins=50, ax=axes[0])
    axes[0].set_xlabel(x)
    axes[0].set_title(f'Histograma e KDE de {x}')
    
    # Boxplot
    sns.boxplot(x=lista, ax=axes[1])
    axes[1].set_xlabel(x)
    axes[1].set_title(f'Boxplot de {x}')
    
    # Mostrar gráfico
    plt.tight_layout()
    plt.show()



ECG_R_Peaks[ECG_R_Peaks['ECG_R_Peaks']>0.7]


ecg_x['ECG_Clean'].max()


ECG_T_Offsets.describe()


ax = sns.displot(df_dict['ECG_T_Peaks'], kde = True, bins =50)
ax.set(xlabel='ECG_T Offsets', ylabel='')


ax = sns.boxplot(x= ECG_Phase_Completion_Ventricular['ECG_Phase_Completion_Ventricular'])
ax.set(xlabel='ECG_R_Offsets')
ax.set_title('Distribuição de ECG_R_Offsets')


ECG_Phase_Completion_Ventricular['ECG_Phase_Completion_Ventricular']


# pequena amostra de 1000
# fig = px.line(resultado_x[0:1000], x=resultado_x.index[0:1000], y = 'ECG_Clean')
# #fig.show(renderer="colab")
# fig.show()








from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt

# Suponha que X seja sua matriz de características extraídas de séries temporais ECG
# Pode ser a Transformada Rápida de Fourier ou outras características extraídas

ecg_clean = resultado_x['ECG_Clean']

#feactures  = resultado_x.drop(['ECG_Raw','ECG_Clean','ECG_Rate','ECG_Quality','ECG_Phase_Atrial'], axis =1)

# Reduzindo a dimensionalidade para visualização (usando PCA)
#pca = PCA(n_components=4)
#X_reduced = pca.fit_transform(feactures)  # X é um conjunto de dados com as características extraídas

ecg_clean_2d = ecg_clean.values.reshape(-1, 1)  # ou ecg_clean.to_numpy().reshape(-1, 1)

# Definir a faixa de números de clusters que você deseja testar
n_clusters_range = range(1, 11)  # Testando de 1 a 10 clusters
inertia = []

# Calcular a inércia para cada número de clusters
for n_clusters in n_clusters_range:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(ecg_clean_2d)  
    inertia.append(kmeans.inertia_)

# Plotando a curva de inércia
plt.plot(n_clusters_range, inertia, marker='o')
plt.xlabel('Número de Clusters')
plt.ylabel('Inércia')
plt.title('Método do Cotovelo')
plt.show()


# Agora aplicando o KMeans
kmeans = KMeans(n_clusters=7, random_state=42)
clusters = kmeans.fit_predict(ecg_clean_2d)


plt.scatter(range(len(ecg_clean_2d)), ecg_clean_2d, c=clusters, cmap='viridis')
plt.show()














colunas


ondas_x = tipo_de_ondas(ecg_x)
ondas_y = tipo_de_ondas(ecg_y)
ondas_z = tipo_de_ondas(ecg_z)


ecgx_T_Peaks = ondas_x['ECG_T_Peaks']
ecgy_T_Peaks = ondas_y['ECG_T_Peaks']
ecgz_T_Peaks = ondas_z['ECG_T_Peaks']

df_ecgx_T_Peaks = pd.DataFrame(ecgx_T_Peaks)
df_ecgy_T_Peaks = pd.DataFrame(ecgy_T_Peaks)
df_ecgz_T_Peaks = pd.DataFrame(ecgz_T_Peaks)


ondas_T_Peaks = pd.DataFrame({
    'x_T_Peaks': ecgx_T_Peaks,
    'y_T_Peaks': ecgy_T_Peaks,
    'z_T_Peaks': ecgz_T_Peaks
})


ondas_T_Peaks


df_ecgx_T_Peaks.columns


fig = px.ecdf(df_ecgx_T_Peaks, x="Picos da onda T do ecg x")
fig.show()


fig = px.ecdf(df_ecgy_T_Peaks, x="Picos da onda T do ecg y")
fig.show()


fig = px.ecdf(df_ecgz_T_Peaks, x="Picos da onda T do ecg y")
fig.show()



sns.boxplot(data=ondas_T_Peaks[['x_T_Peaks', 'y_T_Peaks', 'z_T_Peaks']])

# Exibir o gráfico
plt.title('Boxplot das Ondas T (x, y, z)')
plt.show()



sns.boxplot(data=ondas_T_Peaks[['x_T_Peaks', 'z_T_Peaks']])

# Exibir o gráfico
plt.title('Boxplot das Ondas T (x, z)')
plt.show()





link_csv = r'D:\Projeto_Tese_mestrado\02_Dataset\dados_csv_info_definitions\ubject-info_limpo.csv'

dados = pd.read_csv(link_csv)


dados.head()


# Contagem de classes no conjunto de dados
class_counts = dados['Cause of death'].value_counts()


# Contagem de classes no conjunto de dados
for class_label, count in class_counts.items():
    print(f'Classe {class_label}: {count} -> {count/dados.shape[0]*100:.2f}%')




print(f'Quantidade de colunas: {dados.shape[1]}')
print(f'Quantidade de linhas: {dados.shape[0]}')


pd.unique(dados['Exit of the study'])


#arquivos_path_holter

lista_sem_duplicatas = pd.Series(arquivos).unique().tolist() # removando duplicadas


len(lista_sem_duplicatas)


# Criando um DataFrame dos endereços
df_enderecos = pd.DataFrame({
    "Endereço": lista_sem_duplicatas
})

# Extraindo o Patient ID do final do endereço
df_enderecos["Patient ID"] = df_enderecos["Endereço"].apply(lambda x: x.split("\\")[-1])


# Removendo o sufixo '_H' da coluna 'Patient ID'
df_enderecos['Patient ID'] = df_enderecos['Patient ID'].str.replace('_H', '', regex=False)

# Aqui contem dados de duas colunas a primeira coluna vou usar como chave id para conectar os pacientes de cada scg e a segunda o tipo de morte.
novos_dados = dados[['Patient ID','Cause of death']]

# Fazendo a fusão (merge) dos DataFrames com base no "Patient ID"
df_final = df_enderecos.merge(novos_dados, on="Patient ID", how="left")
print(f'Sem o filtro de tipo de mortes {df_final.shape}')

df_final = df_final[~df_final['Cause of death'].isin([1])] # tirando valores que tem 1 pois significa mortes não identificada ou seja ruídos,

print(f'Com o filtro de tipo de mortes {df_final.shape}')

# Contagem de classes no conjunto de dados
class_counts = df_final['Cause of death'].value_counts()


# Contagem de classes no conjunto de dados
for class_label, count in class_counts.items():
    print(f'Classe {class_label}: {count} -> {count/df_final.shape[0]*100:.2f}%')
    
    
# Resetando o índice para garantir que o loop use os índices corretos
df_final.reset_index(drop=True, inplace=True)


# # Lista para armazenar informações de cada paciente
# dados_pacientes = []

# # Loop para processar cada arquivo e pegar o tamanho do sinal
# for idx, arquivo in enumerate(arquivos):
#     try:
#         # Lendo o arquivo do ECG
#         record = wfdb.rdrecord(arquivo)
#         p_signal = record.p_signal
#         sinal_x = p_signal[:, 0]
        
#         # Armazenando as informações no dicionário
#         dados_pacientes.append({
#             "Paciente": arquivo.split("\\")[-1],  # Extrair o nome do paciente
#             "Quantidade de amostras": len(sinal_x)
#         })

#     except Exception as e:
#         print(f"Erro ao processar {arquivo}: {e}")

# # Convertendo a lista para um DataFrame
# df_pacientes = pd.DataFrame(dados_pacientes)

# # Exibindo as primeiras linhas do DataFrame
# print(df_pacientes)

# df_pacientes['Quantidade de amostras'].unique()


# hrv_time = nk.hrv_time(peaks, sampling_rate=100, show=True)
# hrv_time





# import pandas as pd
# import numpy as np
# import time
# import wfdb
# import neurokit2 as nk

# inicio_geral = time.time()

# # Resetando o índice para garantir que o loop use os índices corretos
# df_final.reset_index(drop=True, inplace=True)

# inicio_amostra = 100  # Começar a partir da amostra 100 (reduzir ruídos iniciais)
# duracao_em_amostras = 300000  # 5 minutos em amostras

# # Lista de colunas que indicam eventos no ECG
# colunas = [
#     'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', 
#     'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', 
#     'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',
#     'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',
#     'ECG_Phase_Completion_Ventricular'
# ]

# # Função para calcular estatísticas
# def calcular_estatisticas(ondas):
#     if ondas.empty:  # Se não houver valores, retorna estatísticas zeradas
#         return {stat: 0 for stat in ["mean", "std", "max", "min", "25%", "50%", "75%"]}
    
#     estatisticas = {
#         "mean": ondas.mean(),
#         "std": ondas.std(),
#         "max": ondas.max(),
#         "min": ondas.min(),
#         "25%": ondas.quantile(0.25),
#         "50%": ondas.quantile(0.50),
#         "75%": ondas.quantile(0.75)
#     }
    
#     return estatisticas

# # Função para processar os dados
# def processar_ecg_com_estatisticas(df_final):
#     dados_processados = []

#     # Iniciando a contagem de tempo
#     inicio = time.time()

#     for idx, endereco in enumerate(df_final["Endereço"]):
#         try:
#             inicio_paciente = time.time()
#             print(f"✅ Iniciando processamento do paciente {idx+1}/{len(df_final)}...")

#             # Lendo o sinal do ECG
#             print(f"   Lendo arquivo {endereco}...")
#             record = wfdb.rdrecord(endereco)
#             sinal_x = record.p_signal[:, 0]

#             # Filtrando o sinal ECG para considerar apenas os dados relevantes
#             sinal_filtrado = sinal_x[inicio_amostra: inicio_amostra + duracao_em_amostras]

#             # Processando o ECG com o Neurokit (pegando só as colunas necessárias)
#             print(f"   Processando ECG com Neurokit...")
#             ecg_x, _ = nk.ecg_process(sinal_filtrado, sampling_rate=record.fs, method='neurokit')

#             # Criando um dicionário para armazenar as estatísticas das ondas
#             estatisticas_paciente = {}

#             for coluna in colunas:
#                 if coluna not in ecg_x.columns:
#                     print(f"⚠️ Coluna {coluna} não encontrada. Pulando...")
#                     continue  # Pula essa etapa se a coluna não existir

#                 print(f"   Processando coluna: {coluna}...")
#                 ondas = ecg_x.loc[ecg_x[coluna] == 1, 'ECG_Clean']

#                 # Calculando as estatísticas para essa onda
#                 estatisticas = calcular_estatisticas(ondas)
#                 for stat, valor in estatisticas.items():
#                     estatisticas_paciente[f"{coluna}_{stat}"] = valor

#             # Adicionando o target do paciente
#             estatisticas_paciente["target"] = df_final.loc[idx, "Cause of death"]

#             # Adicionando os resultados à lista
#             dados_processados.append(estatisticas_paciente)

#             # Liberando a memória de ecg_x após o uso
#             del ecg_x

#             print(f"✅ Paciente {idx+1}/{len(df_final)} processado: {endereco}")

#         except Exception as e:
#             print(f"❌ Erro ao processar {endereco}: {e}")
#             continue  # Continua com o próximo paciente, sem interromper o código

#     # Convertendo a lista de dicionários em um DataFrame final
#     df_estatisticas_final = pd.DataFrame(dados_processados)

#     # Substituindo todos os NaN por zero (caso ainda haja algum)
#     df_estatisticas_final.fillna(0, inplace=True)

#     # Finalizando a contagem de tempo
#     fim = time.time()
#     tempo_execucao = fim - inicio

#     # Exibindo o tempo total de execução
#     print(f"Tempo total de execução: {tempo_execucao:.2f} segundos")

#     return df_estatisticas_final

# # Exemplo de uso
# df_estatisticas = processar_ecg_com_estatisticas(df_final)

# # Exibindo as primeiras linhas do DataFrame final
# #print(df_estatisticas.head())


# # salvando o dataframe
# df_estatisticas.to_csv("D:/Projeto_Tese_mestrado/02_Dataset/dados_ECG/dados_ecg.csv", index=False)



# salvando o dataframe
#df_ecg_final.to_csv("dados_ecg.csv", index=False)



#df_estatisticas.shape


#df_estatisticas.head()


from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np


def tratamento_dados(dados, filtro):
    """
    Função para tratar os dados, filtrando colunas específicas e realizando pré-processamento.

    Parâmetros:
    - dados (DataFrame): O dataset a ser tratado.
    - filtro (str): Palavra-chave para selecionar as colunas (default: "ECG_R_Peaks").

    Retorna:
    - X_train_scaled, X_test_scaled, y_train, y_test
    """

    # Filtrar colunas que contêm a palavra-chave no nome
    colunas_filtradas = dados.filter(like=filtro).columns
    X = dados[colunas_filtradas].values  # Apenas colunas filtradas

    # Garantir que o target seja separado corretamente
    y = dados['target'].values  

    # Transformar os rótulos usando LabelEncoder
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)

    # Verificar o mapeamento dos rótulos
    print("Rótulos originais:", np.unique(y))
    print("Rótulos transformados:", np.unique(y_encoded))

    # Dividir em treino e teste (80% treino, 20% teste)
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)

    # Normalizar os dados
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Converter o target para one-hot encoding se for multiclasse
    num_classes = len(np.unique(y_encoded))
    y_train = to_categorical(y_train, num_classes=num_classes)
    y_test = to_categorical(y_test, num_classes=num_classes)
    
    return X_train_scaled, X_test_scaled, y_train, y_test
















import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import LeakyReLU

def construir_rede_neural(X_train, X_test, y_train, y_test,input_dim):
    # Criar modelo poderoso
    def criar_rede_neural(input_dim, num_classes):
        model = keras.Sequential([
        keras.Input(shape=(input_dim,)),  # Corrigindo a entrada
        Dense(512),
        LeakyReLU(negative_slope=0.1),  # Corrigindo o parâmetro alpha
        BatchNormalization(),
        Dropout(0.3),

        Dense(256),
        LeakyReLU(negative_slope=0.1),
        BatchNormalization(),
        Dropout(0.3),

        Dense(128),
        LeakyReLU(negative_slope=0.1),
        BatchNormalization(),
        Dropout(0.3),

        Dense(64),
        LeakyReLU(negative_slope=0.1),
        BatchNormalization(),
        Dropout(0.3),

        Dense(num_classes, activation='softmax')
    ])



        # Compilar modelo
        model.compile(
            optimizer=Adam(learning_rate=0.001),  # Adam já é otimizado para deep learning
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model

    # Criar callbacks
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10)


    # Criar e treinar o modelo
    modelo = criar_rede_neural(input_dim=input_dim, num_classes=4)
    history = modelo.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=100,
        batch_size=32,
        callbacks=[early_stopping, reduce_lr],
        verbose=0  # Define para 0 para ocultar a saída
        #verbose=2  # Apenas mostra os valores das métricas
    )

    # Avaliação
    loss, acc = modelo.evaluate(X_test, y_test)
    print(f"\nAcurácia no conjunto de validação: {acc:.4f}")



link =r'D:\Projeto_Tese_mestrado\02_Dataset\dados_ECG\dados_ecg_x.csv'
df_x = pd.read_csv(link)


df_x.head()


import time

colunas = [
    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', 
    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', 
    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',
    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',
    'ECG_Phase_Completion_Ventricular'
]

for col in colunas:
    print(f'Executando {col}')
    
    start_time = time.time()  # Inicia a contagem do tempo
    
    X_train, X_test, y_train, y_test = tratamento_dados(df_x, filtro=col)
    construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])
    
    end_time = time.time()  # Finaliza a contagem do tempo
    elapsed_time = end_time - start_time  # Calcula o tempo decorrido
    
    print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')
    print('=' * 100)






# import duckdb
# import pandas as pd
# import numpy as np
# import time
# import wfdb
# import neurokit2 as nk

# # Conectar ao banco de dados DuckDB
# conn = duckdb.connect("D:/Projeto_Tese_mestrado/02_Dataset/Duckedb/banco_ecg.duckdb")

# # Carregar apenas a lista de pacientes
# pacientes = conn.execute("SELECT DISTINCT id_paciente FROM ecg_pacientes ORDER BY id_paciente").fetchdf()

# # Lista de colunas que indicam eventos no ECG
# colunas = [
#     'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', 
#     'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', 
#     'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',
#     'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',
#     'ECG_Phase_Completion_Ventricular'
# ]

# # Função para calcular estatísticas
# def calcular_estatisticas(ondas):
#     if ondas.empty:  
#         return {stat: 0 for stat in ["mean", "std", "max", "min", "25%", "50%", "75%"]}
    
#     # Substituir valores NaN por zero antes de calcular as estatísticas
#     ondas = ondas.fillna(0)
    
#     estatisticas = {
#         "mean": ondas.mean(),
#         "std": ondas.std(),
#         "max": ondas.max(),
#         "min": ondas.min(),
#         "25%": ondas.quantile(0.25),
#         "50%": ondas.quantile(0.50),
#         "75%": ondas.quantile(0.75)
#     }
    
#     return estatisticas

# # Processar paciente por paciente
# dados_processados = []
# inicio_geral = time.time()

# for idx, paciente in enumerate(pacientes["id_paciente"]):
#     try:
#         inicio_paciente = time.time()
#         print(f"✅ Processando paciente {idx+1}/{len(pacientes)}: {paciente}")

#         # Carregar os dados do paciente diretamente do banco
#         df_paciente = conn.execute(f"""
#             SELECT sinal_y FROM ecg_pacientes
#             WHERE id_paciente = '{paciente}'
#         """).fetchdf()

#         # Se não houver dados, pula o paciente
#         if df_paciente.empty:
#             print(f"⚠️ Nenhum dado encontrado para o paciente {paciente}. Pulando...")
#             continue


#         sinal_y = df_paciente["sinal_y"].values


#         # Processar com Neurokit2
#         ecg_y, _ = nk.ecg_process(sinal_y, sampling_rate=1000, method='neurokit')  # Ajuste a taxa de amostragem se necessário

#         # Criar dicionário para armazenar estatísticas
#         estatisticas_paciente = {}

#         for coluna in colunas:
#             if coluna not in ecg_y.columns:
#                 print(f"⚠️ Coluna {coluna} não encontrada. Pulando...")
#                 continue 

#             ondas = ecg_y.loc[ecg_y[coluna] == 1, 'ECG_Clean']
#             estatisticas = calcular_estatisticas(ondas)

#             for stat, valor in estatisticas.items():
#                 estatisticas_paciente[f"{coluna}_{stat}"] = valor

#         # Adicionando o target do paciente
#         estatisticas_paciente["target"] = df_final.loc[idx, "Cause of death"]

#         # Adicionar ID do paciente
#         estatisticas_paciente["id_paciente"] = paciente

#         # Salvar os dados processados
#         dados_processados.append(estatisticas_paciente)
        


#         # Liberar memória
#         del sinal_y, ecg_y, df_paciente

#         print(f"✅ Paciente {paciente} processado com sucesso!")

#     except Exception as e:
#         print(f"❌ Erro ao processar paciente {paciente}: {e}")
#         continue

# # Fechar a conexão com o banco
# conn.close()

# # Criar DataFrame com os resultados
# df_estatisticas_final = pd.DataFrame(dados_processados)
# df_estatisticas_final.fillna(0, inplace=True)

# # Salvar os resultados
# df_estatisticas_final.to_csv("D:/Projeto_Tese_mestrado/02_Dataset/dados_ECG/dados_ecg_y.csv", index=False)

# # Tempo total de execução
# fim_geral = time.time()
# print(f"🏁 Processamento concluído! Tempo total: {fim_geral - inicio_geral:.2f} segundos")



link =r'D:\Projeto_Tese_mestrado\02_Dataset\dados_ECG\dados_ecg_y.csv'
df_y = pd.read_csv(link)


import time

colunas = [
    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', 
    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', 
    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',
    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',
    'ECG_Phase_Completion_Ventricular'
]

for col in colunas:
    print(f'Executando {col}')
    
    start_time = time.time()  # Inicia a contagem do tempo
    
    X_train, X_test, y_train, y_test = tratamento_dados(df_y, filtro=col)
    construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])
    
    end_time = time.time()  # Finaliza a contagem do tempo
    elapsed_time = end_time - start_time  # Calcula o tempo decorrido
    
    print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')
    print('=' * 100)



# import duckdb
# import pandas as pd
# import numpy as np
# import time
# import wfdb
# import neurokit2 as nk

# # Conectar ao banco de dados DuckDB
# conn = duckdb.connect("D:/Projeto_Tese_mestrado/02_Dataset/Duckedb/banco_ecg.duckdb")

# # Carregar apenas a lista de pacientes
# pacientes = conn.execute("SELECT DISTINCT id_paciente FROM ecg_pacientes ORDER BY id_paciente").fetchdf()

# # Lista de colunas que indicam eventos no ECG
# colunas = [
#     'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', 
#     'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', 
#     'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',
#     'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',
#     'ECG_Phase_Completion_Ventricular'
# ]

# # Função para calcular estatísticas
# def calcular_estatisticas(ondas):
#     if ondas.empty:  
#         return {stat: 0 for stat in ["mean", "std", "max", "min", "25%", "50%", "75%"]}
    
#     # Substituir valores NaN por zero antes de calcular as estatísticas
#     ondas = ondas.fillna(0)
    
#     estatisticas = {
#         "mean": ondas.mean(),
#         "std": ondas.std(),
#         "max": ondas.max(),
#         "min": ondas.min(),
#         "25%": ondas.quantile(0.25),
#         "50%": ondas.quantile(0.50),
#         "75%": ondas.quantile(0.75)
#     }
    
#     return estatisticas

# # Processar paciente por paciente
# dados_processados = []
# inicio_geral = time.time()

# for idx, paciente in enumerate(pacientes["id_paciente"]):
#     try:
#         inicio_paciente = time.time()
#         print(f"✅ Processando paciente {idx+1}/{len(pacientes)}: {paciente}")

#         # Carregar os dados do paciente diretamente do banco
#         df_paciente = conn.execute(f"""
#             SELECT sinal_z FROM ecg_pacientes
#             WHERE id_paciente = '{paciente}'
#         """).fetchdf()

#         # Se não houver dados, pula o paciente
#         if df_paciente.empty:
#             print(f"⚠️ Nenhum dado encontrado para o paciente {paciente}. Pulando...")
#             continue


#         sinal_z = df_paciente["sinal_z"].values


#         # Processar com Neurokit2
#         ecg_z, _ = nk.ecg_process(sinal_z, sampling_rate=1000, method='neurokit')  # Ajuste a taxa de amostragem se necessário

#         # Criar dicionário para armazenar estatísticas
#         estatisticas_paciente = {}

#         for coluna in colunas:
#             if coluna not in ecg_z.columns:
#                 print(f"⚠️ Coluna {coluna} não encontrada. Pulando...")
#                 continue 

#             ondas = ecg_z.loc[ecg_z[coluna] == 1, 'ECG_Clean']
#             estatisticas = calcular_estatisticas(ondas)

#             for stat, valor in estatisticas.items():
#                 estatisticas_paciente[f"{coluna}_{stat}"] = valor
                
                
#         # Adicionando o target do paciente
#         estatisticas_paciente["target"] = df_final.loc[idx, "Cause of death"]

#         # Adicionar ID do paciente
#         estatisticas_paciente["id_paciente"] = paciente

#         # Salvar os dados processados
#         dados_processados.append(estatisticas_paciente)

#         # Liberar memória
#         del sinal_z, ecg_z, df_paciente

#         print(f"✅ Paciente {paciente} processado com sucesso!")

#     except Exception as e:
#         print(f"❌ Erro ao processar paciente {paciente}: {e}")
#         continue

# # Fechar a conexão com o banco
# conn.close()

# # Criar DataFrame com os resultados
# df_estatisticas_final = pd.DataFrame(dados_processados)
# df_estatisticas_final.fillna(0, inplace=True)

# # Salvar os resultados
# df_estatisticas_final.to_csv("D:/Projeto_Tese_mestrado/02_Dataset/dados_ECG/dados_ecg_z.csv", index=False)

# # Tempo total de execução
# fim_geral = time.time()
# print(f"🏁 Processamento concluído! Tempo total: {fim_geral - inicio_geral:.2f} segundos")



link =r'D:\Projeto_Tese_mestrado\02_Dataset\dados_ECG\dados_ecg_z.csv'
df_z = pd.read_csv(link)


import time

colunas = [
    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', 
    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', 
    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',
    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',
    'ECG_Phase_Completion_Ventricular'
]

for col in colunas:
    print(f'Executando {col}')
    
    start_time = time.time()  # Inicia a contagem do tempo
    
    X_train, X_test, y_train, y_test = tratamento_dados(df_z, filtro=col)
    construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])
    
    end_time = time.time()  # Finaliza a contagem do tempo
    elapsed_time = end_time - start_time  # Calcula o tempo decorrido
    
    print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')
    print('=' * 100)






from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import numpy as np

def tratamento_dados(dados):
    """
    Função para tratar os dados, realizando pré-processamento sem filtro específico.

    Parâmetros:
    - dados (DataFrame): O dataset a ser tratado.

    Retorna:
    - X_train_scaled, X_test_scaled, y_train, y_test
    """

    # Separar as features (X) e o target (y)
    X = dados.drop(columns=['target']).values  # Remove a coluna target e pega as features
    y = dados['target'].values  # Mantém apenas a coluna target

    # Transformar os rótulos usando LabelEncoder
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)

    # Verificar o mapeamento dos rótulos
    print("Rótulos originais:", np.unique(y))
    print("Rótulos transformados:", np.unique(y_encoded))

    # Dividir em treino e teste (80% treino, 20% teste)
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)

    # Normalizar os dados
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Converter o target para one-hot encoding se for multiclasse
    num_classes = len(np.unique(y_encoded))
    y_train = to_categorical(y_train, num_classes=num_classes)
    y_test = to_categorical(y_test, num_classes=num_classes)
    
    return X_train_scaled, X_test_scaled, y_train, y_test



link_csv = r'D:\Projeto_Tese_mestrado\02_Dataset\dados_csv_info_definitions\ubject-info_limpo.csv'

dados = pd.read_csv(link_csv)



df =  dados.rename(columns={'Cause of death': 'target'})



import time

colunas = [
    'ECG_R_Peaks', 'ECG_P_Peaks', 'ECG_P_Onsets', 'ECG_P_Offsets', 
    'ECG_Q_Peaks', 'ECG_R_Onsets', 'ECG_R_Offsets', 'ECG_S_Peaks', 
    'ECG_T_Peaks', 'ECG_T_Onsets', 'ECG_T_Offsets', 'ECG_Phase_Atrial',
    'ECG_Phase_Completion_Atrial', 'ECG_Phase_Ventricular',
    'ECG_Phase_Completion_Ventricular'
]

for col in colunas:
    print(f'Executando {col}')
    
    start_time = time.time()  # Inicia a contagem do tempo
    
    X_train, X_test, y_train, y_test = tratamento_dados(df_z, filtro=col)
    construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])
    
    end_time = time.time()  # Finaliza a contagem do tempo
    elapsed_time = end_time - start_time  # Calcula o tempo decorrido
    
    print(f'Finalizando o processo - Tempo de execução: {elapsed_time:.2f} segundos')
    print('=' * 100)



# import time

# # Dicionário para armazenar diferentes categorias de colunas
# estatisticas = {
#     "mean": [],
#     "max": [],
#     "std": [],
#     "min": [],
#     "25%": [],
#     "50%": [],
#     "75%": []
# }

# # Pegando todas as colunas e classificando por estatística
# for col in df_x.columns:
#     for estatistica in estatisticas.keys():
#         if estatistica in col:
#             estatisticas[estatistica].append(col)

# # Criando lista de DataFrames para treinar
# dataframes = [df_x, df_y, df_z]

# # Loop para treinar com cada estatística
# for estatistica, colunas in estatisticas.items():
#     print(f'Treinando com colunas: {estatistica}')
    
#     start_time = time.time()  # Início do tempo

#     # Filtrar apenas as colunas da estatística atual para cada DataFrame
#     df_x_filtrado = df_x[colunas]
#     df_y_filtrado = df_y[colunas]
#     df_z_filtrado = df_z[colunas]
    
#     # Concatenar os DataFrames (se necessário)
#     df_final = pd.concat([df_x_filtrado, df_y_filtrado, df_z_filtrado], axis=0)

#     # Dividir em treino e teste
#     X_train, X_test, y_train, y_test = tratamento_dados(df_final)

#     # Construir e treinar a rede neural
#     construir_rede_neural(X_train, X_test, y_train, y_test, input_dim=X_train.shape[1])

#     end_time = time.time()  # Fim do tempo
#     elapsed_time = end_time - start_time  # Tempo decorrido

#     print(f'Finalizando treinamento para {estatistica} - Tempo: {elapsed_time:.2f} segundos')
#     print('=' * 100)

